{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arrayfire as af\n",
    "import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toArrayFire(x):\n",
    "    x_np = x.cpu().detach().contiguous().numpy()\n",
    "    shape = 1\n",
    "    if len(x_np.shape) == 0:\n",
    "        shape = (1,)\n",
    "    else:\n",
    "        shape = x_np.shape[::-1]\n",
    "    afArray = af.Array(x_np.ctypes.data, shape, x_np.dtype.char)\n",
    "    return afArray\n",
    "\n",
    "def saveStateDict(model, filepath):\n",
    "    params = {}\n",
    "    i = 0\n",
    "    for (name, param) in model.state_dict().items():\n",
    "        if 'running' in name:\n",
    "            continue\n",
    "        if 'in_proj' in name and 'in_proj.weight' != name and 'in_proj.bias' != name:\n",
    "            q, k, v = param.chunk(3, dim=0)\n",
    "            hack = '0'\n",
    "            if 'in_proj_bias' in name: hack = '1'\n",
    "            params['0q_' + hack + name] = q\n",
    "            params['1k_' + hack + name] = k\n",
    "            params['2v_' + hack + name] = v\n",
    "            if 'in_proj_bias' in name:\n",
    "                for key in sorted(params.keys()):\n",
    "                    af_array = toArrayFire(params[key])\n",
    "                    if 'weight' in key:\n",
    "                        af_array = af.array.transpose(af_array)\n",
    "                    #print(key, i, params[key].shape)\n",
    "                    af.array.save_array(key, af_array, filepath, True)\n",
    "                    #print(key, af.array.save_array(key, af_array, filepath, True))\n",
    "                    i = i + 1\n",
    "                params = {}\n",
    "            continue\n",
    "        elif len(param.size()) > 0:\n",
    "            if 'input_proj.bias' in name:\n",
    "                param = param.reshape((1, 1, 256))\n",
    "            af_array = toArrayFire(param)\n",
    "            if 'fc' in name and 'weight' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            elif 'weight' in name and 'proj' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            elif 'weight' in name and 'linear' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            elif 'query_embed' in name:\n",
    "                af_array = af_array\n",
    "            elif 'weight' in name and 'embed' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "\n",
    "            #print(name, i, param.shape)\n",
    "            af.array.save_array(name, af_array, filepath, True)\n",
    "            #print(name, af.array.save_array(name, af_array, filepath, True))\n",
    "            i = i + 1\n",
    "    for name in model.state_dict():\n",
    "        if 'running' in name:\n",
    "            #print(name)\n",
    "            af_array = toArrayFire(model.state_dict()[name])\n",
    "            #af.array.save_array(name, af_array, filepath + 'running', True)\n",
    "            #print(name, af.array.save_array(name, af_array, filepath + 'running', True))\n",
    "            #print(name, model.state_dict()[name].shape,)\n",
    "    \n",
    "def create_parser():\n",
    "    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n",
    "    parser.add_argument('--lr', default=1e-4, type=float)\n",
    "    parser.add_argument('--lr_backbone', default=1e-5, type=float)\n",
    "    parser.add_argument('--batch_size', default=2, type=int)\n",
    "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
    "    parser.add_argument('--epochs', default=300, type=int)\n",
    "    parser.add_argument('--lr_drop', default=200, type=int)\n",
    "    parser.add_argument('--optimizer', default=\"adam\", type=str)\n",
    "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
    "                        help='gradient clipping max norm')\n",
    "    parser.add_argument('--eval_skip', default=1, type=int,\n",
    "                        help='do evaluation every \"eval_skip\" frames')\n",
    "    parser.add_argument('--schedule', default='step', type=str,\n",
    "                        choices=('step', 'multistep'))\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--frozen_weights', type=str, default=None,\n",
    "                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n",
    "    # * Backbone\n",
    "    parser.add_argument('--backbone', default='resnet50', type=str,\n",
    "                        help=\"Name of the convolutional backbone to use\")\n",
    "    parser.add_argument('--dilation', action='store_true',\n",
    "                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
    "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n",
    "                        help=\"Type of positional embedding to use on top of the image features\")\n",
    "\n",
    "    # * Transformer\n",
    "    parser.add_argument('--enc_layers', default=6, type=int,\n",
    "                        help=\"Number of encoding layers in the transformer\")\n",
    "    parser.add_argument('--dec_layers', default=6, type=int,\n",
    "                        help=\"Number of decoding layers in the transformer\")\n",
    "    parser.add_argument('--dim_feedforward', default=2048, type=int,\n",
    "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
    "    parser.add_argument('--hidden_dim', default=256, type=int,\n",
    "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
    "    parser.add_argument('--dropout', default=0.1, type=float,\n",
    "                        help=\"Dropout applied in the transformer\")\n",
    "    parser.add_argument('--nheads', default=8, type=int,\n",
    "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
    "    parser.add_argument('--num_queries', default=100, type=int,\n",
    "                        help=\"Number of query slots\")\n",
    "    parser.add_argument('--pre_norm', action='store_true')\n",
    "    parser.add_argument('--no_pass_pos_and_query', dest='pass_pos_and_query', action='store_false',\n",
    "                        help=\"Disables passing the positional encodings to each attention layers\")\n",
    "\n",
    "    # * Segmentation\n",
    "    parser.add_argument('--mask_model', default='none', type=str, choices=(\"none\", \"smallconv\", \"v2\"),\n",
    "                        help=\"Segmentation head to be used (if None, segmentation will not be trained)\")\n",
    "\n",
    "    # Loss\n",
    "    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',\n",
    "                        help=\"Disables auxiliary decoding losses (loss at each layer)\")\n",
    "    parser.add_argument('--set_loss', default='hungarian', type=str,\n",
    "                        choices=('sequential', 'hungarian', 'lexicographical'),\n",
    "                        help=\"Type of matching to perform in the loss\")\n",
    "    parser.add_argument('--bcl', dest='use_bcl', action='store_true',\n",
    "                        help=\"Use balanced classification loss\")\n",
    "    # * Matcher\n",
    "    parser.add_argument('--set_cost_class', default=1, type=float,\n",
    "                        help=\"Class coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_bbox', default=5, type=float,\n",
    "                        help=\"L1 box coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_giou', default=2, type=float,\n",
    "                        help=\"giou box coefficient in the matching cost\")\n",
    "    # * Loss coefficients\n",
    "    parser.add_argument('--mask_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--dice_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--bbox_loss_coef', default=5, type=float)\n",
    "    parser.add_argument('--giou_loss_coef', default=2, type=float)\n",
    "    parser.add_argument('--eos_coef', default=0.1, type=float,\n",
    "                        help=\"Relative classification weight of the no-object class\")\n",
    "\n",
    "    # dataset parameters\n",
    "    parser.add_argument('--dataset_file', default='coco')\n",
    "    parser.add_argument('--coco_path', type=str, default='/datasets01/COCO/022719')\n",
    "    parser.add_argument('--coco_panoptic_path', type=str, default='/datasets01/COCO/060419')\n",
    "    parser.add_argument('--remove_difficult', action='store_true')\n",
    "    parser.add_argument('--masks', action='store_true')\n",
    "\n",
    "    parser.add_argument('--output-dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=42, type=int)\n",
    "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "    parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--num_workers', default=2, type=int)\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world-size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--dist-url', default='env://', help='url used to set up distributed training')\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=19.25s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import models.detr \n",
    "import datasets.coco\n",
    "parser = create_parser()\n",
    "pretrained_path = '/private/home/padentomasello/scratch/pytorch_testing/detr-r50-e632da11.pth'\n",
    "args = parser.parse_args([\"--dropout=0.0\", \"--eos_coef=0.1\", \"--resume=/private/home/padentomasello/scratch/pytorch_testing/detr-r50-e632da11.pth\"])\n",
    "#args = parser.parse_args([])\n",
    "model, criterion, post = models.detr.build(args)   \n",
    "dataset = datasets.coco.build('train', args)\n",
    "if args.resume:\n",
    "    if args.resume.startswith('https'):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            args.resume, map_location='cpu', check_hash=True)\n",
    "    else:\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from torch.utils.data import DataLoader, DistributedSampler\n",
    "    import util.misc as utils\n",
    "    dataset_train = dataset\n",
    "    sampler_train = torch.utils.data.SequentialSampler(dataset)\n",
    "    #batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "        #    sampler_train, args.batch_size, drop_last=True)\n",
    "    batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "            sampler_train, 1, drop_last=True)\n",
    "    data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                                collate_fn=utils.collate_fn, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(args.device)\n",
    "for (sample_tmp, target_tmp) in data_loader_train:\n",
    "    samples = sample_tmp.to(device)\n",
    "    targets  = [{k: v.to(device) for k, v in t.items()} for t in target_tmp]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "429"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.transformer import *\n",
    "\n",
    "\n",
    "from models.backbone import *\n",
    "filepath = '/private/home/padentomasello/scratch/pytorch_testing/detr.array'\n",
    "\n",
    "N = 2\n",
    "C = 3\n",
    "H = 224\n",
    "W = 224\n",
    "\n",
    "\n",
    "image = samples.tensors\n",
    "image.requires_grad = True\n",
    "mask = samples.mask\n",
    "\n",
    "af.array.save_array('image', toArrayFire(image), filepath, False)\n",
    "af.array.save_array('mask', toArrayFire(mask.float()), filepath, True)\n",
    "target_boxes = targets[0]['boxes']\n",
    "target_classes = targets[0]['labels']\n",
    "af.array.save_array('target_boxes', toArrayFire(target_boxes), filepath, True)\n",
    "af.array.save_array('target_labels', toArrayFire(target_classes.float()), filepath, True)\n",
    "#af.array.save_array('pos', toArrayFire(pos), filepath, True)\n",
    "       \n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "output = model(samples)\n",
    "saveStateDict(model, filepath)\n",
    "af.array.save_array('pred_logits', toArrayFire(output['pred_logits']), filepath, True)\n",
    "af.array.save_array('pred_boxes', toArrayFire(output['pred_boxes']), filepath, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-15.0318,  -0.7601,  -7.4993,  ..., -16.1768,  -4.3605,   6.9969],\n",
       "         [-14.2471,  -4.3277,  -3.1712,  ..., -13.3617,  -6.5534,   7.4247],\n",
       "         [-14.6081,  -3.6830,  -6.3749,  ..., -21.5078,  -7.0447,   6.6202],\n",
       "         ...,\n",
       "         [-15.2303,  -0.2827,  -4.8999,  ..., -11.4098,  -6.8363,   6.1961],\n",
       "         [-14.9263,  -1.7318,  -8.4528,  ..., -11.2523,  -9.2181,   7.2515],\n",
       "         [-15.4960,  -3.6935,  -3.4774,  ..., -12.8652,  -8.1547,   7.7371]]],\n",
       "       device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"pred_logits\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "criteiron(output, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses = criterion(output, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses['loss_giou'].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-15.0318,  -0.7601,  -7.4993,  ..., -16.1768,  -4.3605,   6.9969],\n",
       "         [-14.2471,  -4.3277,  -3.1712,  ..., -13.3617,  -6.5534,   7.4247],\n",
       "         [-14.6081,  -3.6830,  -6.3749,  ..., -21.5078,  -7.0447,   6.6202],\n",
       "         ...,\n",
       "         [-15.2303,  -0.2827,  -4.8999,  ..., -11.4098,  -6.8363,   6.1961],\n",
       "         [-14.9263,  -1.7318,  -8.4528,  ..., -11.2523,  -9.2181,   7.2515],\n",
       "         [-15.4960,  -3.6935,  -3.4774,  ..., -12.8652,  -8.1547,   7.7371]]],\n",
       "       device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['pred_logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_ce': tensor(0.2454, device='cuda:1', grad_fn=<NllLoss2DBackward>),\n",
       " 'class_error': tensor(12.5000, device='cuda:1'),\n",
       " 'loss_bbox': tensor(0.0566, device='cuda:1', grad_fn=<DivBackward0>),\n",
       " 'loss_giou': tensor(0.1525, device='cuda:1', grad_fn=<DivBackward0>),\n",
       " 'cardinality_error': tensor(9., device='cuda:1'),\n",
       " 'loss_ce_0': tensor(0.3793, device='cuda:1', grad_fn=<NllLoss2DBackward>),\n",
       " 'loss_bbox_0': tensor(0.0651, device='cuda:1', grad_fn=<DivBackward0>),\n",
       " 'loss_giou_0': tensor(0.1401, device='cuda:1', grad_fn=<DivBackward0>),\n",
       " 'cardinality_error_0': tensor(14., device='cuda:1'),\n",
       " 'loss_ce_1': tensor(0.4734, device='cuda:1', grad_fn=<NllLoss2DBackward>),\n",
       " 'loss_bbox_1': tensor(0.0488, device='cuda:1', grad_fn=<DivBackward0>),\n",
       " 'loss_giou_1': tensor(0.1170, device='cuda:1', grad_fn=<DivBackward0>),\n",
       " 'cardinality_error_1': tensor(11., device='cuda:1'),\n",
       " 'loss_ce_2': tensor(0.2963, device='cuda:1', grad_fn=<NllLoss2DBackward>),\n",
       " 'loss_bbox_2': tensor(0.0528, device='cuda:1', grad_fn=<DivBackward0>),\n",
       " 'loss_giou_2': tensor(0.1580, device='cuda:1', grad_fn=<DivBackward0>),\n",
       " 'cardinality_error_2': tensor(12., device='cuda:1'),\n",
       " 'loss_ce_3': tensor(0.2454, device='cuda:1', grad_fn=<NllLoss2DBackward>),\n",
       " 'loss_bbox_3': tensor(0.0551, device='cuda:1', grad_fn=<DivBackward0>),\n",
       " 'loss_giou_3': tensor(0.1545, device='cuda:1', grad_fn=<DivBackward0>),\n",
       " 'cardinality_error_3': tensor(11., device='cuda:1'),\n",
       " 'loss_ce_4': tensor(0.2508, device='cuda:1', grad_fn=<NllLoss2DBackward>),\n",
       " 'loss_bbox_4': tensor(0.0556, device='cuda:1', grad_fn=<DivBackward0>),\n",
       " 'loss_giou_4': tensor(0.1512, device='cuda:1', grad_fn=<DivBackward0>),\n",
       " 'cardinality_error_4': tensor(10., device='cuda:1')}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
