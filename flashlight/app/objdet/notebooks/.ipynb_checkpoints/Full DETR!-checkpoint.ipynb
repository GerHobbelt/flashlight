{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arrayfire as af\n",
    "import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toArrayFire(x):\n",
    "    x_np = x.cpu().detach().contiguous().numpy()\n",
    "    shape = 1\n",
    "    if len(x_np.shape) == 0:\n",
    "        shape = (1,)\n",
    "    else:\n",
    "        shape = x_np.shape[::-1]\n",
    "    afArray = af.Array(x_np.ctypes.data, shape, x_np.dtype.char)\n",
    "    return afArray\n",
    "\n",
    "def saveStateDict(model, filepath):\n",
    "    params = {}\n",
    "    i = 0\n",
    "    for (name, param) in model.state_dict().items():\n",
    "        if 'running' in name:\n",
    "            continue\n",
    "        if 'in_proj' in name and 'in_proj.weight' != name and 'in_proj.bias' != name:\n",
    "            q, k, v = param.chunk(3, dim=0)\n",
    "            hack = '0'\n",
    "            if 'in_proj_bias' in name: hack = '1'\n",
    "            params['0q_' + hack + name] = q\n",
    "            params['1k_' + hack + name] = k\n",
    "            params['2v_' + hack + name] = v\n",
    "            if 'in_proj_bias' in name:\n",
    "                for key in sorted(params.keys()):\n",
    "                    af_array = toArrayFire(params[key])\n",
    "                    if 'weight' in key:\n",
    "                        af_array = af.array.transpose(af_array)\n",
    "                    #print(key, i, params[key].shape)\n",
    "                    af.array.save_array(key, af_array, filepath, True)\n",
    "                    #print(key, af.array.save_array(key, af_array, filepath, True))\n",
    "                    i = i + 1\n",
    "                params = {}\n",
    "            continue\n",
    "        elif len(param.size()) > 0:\n",
    "            if 'input_proj.bias' in name:\n",
    "                param = param.reshape((1, 1, 256))\n",
    "            af_array = toArrayFire(param)\n",
    "            if 'fc' in name and 'weight' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            elif 'weight' in name and 'proj' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            elif 'weight' in name and 'linear' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            elif 'query_embed' in name:\n",
    "                af_array = af_array\n",
    "            elif 'weight' in name and 'embed' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "\n",
    "            #print(name, i, param.shape)\n",
    "            af.array.save_array(name, af_array, filepath, True)\n",
    "            #print(name, af.array.save_array(name, af_array, filepath, True))\n",
    "            i = i + 1\n",
    "    for name in model.state_dict():\n",
    "        if 'running' in name:\n",
    "            #print(name)\n",
    "            af_array = toArrayFire(model.state_dict()[name])\n",
    "            #af.array.save_array(name, af_array, filepath + 'running', True)\n",
    "            #print(name, af.array.save_array(name, af_array, filepath + 'running', True))\n",
    "            #print(name, model.state_dict()[name].shape,)\n",
    "    \n",
    "def create_parser():\n",
    "    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n",
    "    parser.add_argument('--lr', default=1e-4, type=float)\n",
    "    parser.add_argument('--lr_backbone', default=1e-5, type=float)\n",
    "    parser.add_argument('--batch_size', default=2, type=int)\n",
    "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
    "    parser.add_argument('--epochs', default=300, type=int)\n",
    "    parser.add_argument('--lr_drop', default=200, type=int)\n",
    "    parser.add_argument('--optimizer', default=\"adam\", type=str)\n",
    "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
    "                        help='gradient clipping max norm')\n",
    "    parser.add_argument('--eval_skip', default=1, type=int,\n",
    "                        help='do evaluation every \"eval_skip\" frames')\n",
    "    parser.add_argument('--schedule', default='step', type=str,\n",
    "                        choices=('step', 'multistep'))\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--frozen_weights', type=str, default=None,\n",
    "                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n",
    "    # * Backbone\n",
    "    parser.add_argument('--backbone', default='resnet50', type=str,\n",
    "                        help=\"Name of the convolutional backbone to use\")\n",
    "    parser.add_argument('--dilation', action='store_true',\n",
    "                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
    "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n",
    "                        help=\"Type of positional embedding to use on top of the image features\")\n",
    "\n",
    "    # * Transformer\n",
    "    parser.add_argument('--enc_layers', default=6, type=int,\n",
    "                        help=\"Number of encoding layers in the transformer\")\n",
    "    parser.add_argument('--dec_layers', default=6, type=int,\n",
    "                        help=\"Number of decoding layers in the transformer\")\n",
    "    parser.add_argument('--dim_feedforward', default=2048, type=int,\n",
    "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
    "    parser.add_argument('--hidden_dim', default=256, type=int,\n",
    "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
    "    parser.add_argument('--dropout', default=0.1, type=float,\n",
    "                        help=\"Dropout applied in the transformer\")\n",
    "    parser.add_argument('--nheads', default=8, type=int,\n",
    "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
    "    parser.add_argument('--num_queries', default=100, type=int,\n",
    "                        help=\"Number of query slots\")\n",
    "    parser.add_argument('--pre_norm', action='store_true')\n",
    "    parser.add_argument('--no_pass_pos_and_query', dest='pass_pos_and_query', action='store_false',\n",
    "                        help=\"Disables passing the positional encodings to each attention layers\")\n",
    "\n",
    "    # * Segmentation\n",
    "    parser.add_argument('--mask_model', default='none', type=str, choices=(\"none\", \"smallconv\", \"v2\"),\n",
    "                        help=\"Segmentation head to be used (if None, segmentation will not be trained)\")\n",
    "\n",
    "    # Loss\n",
    "    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',\n",
    "                        help=\"Disables auxiliary decoding losses (loss at each layer)\")\n",
    "    parser.add_argument('--set_loss', default='hungarian', type=str,\n",
    "                        choices=('sequential', 'hungarian', 'lexicographical'),\n",
    "                        help=\"Type of matching to perform in the loss\")\n",
    "    parser.add_argument('--bcl', dest='use_bcl', action='store_true',\n",
    "                        help=\"Use balanced classification loss\")\n",
    "    # * Matcher\n",
    "    parser.add_argument('--set_cost_class', default=1, type=float,\n",
    "                        help=\"Class coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_bbox', default=5, type=float,\n",
    "                        help=\"L1 box coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_giou', default=2, type=float,\n",
    "                        help=\"giou box coefficient in the matching cost\")\n",
    "    # * Loss coefficients\n",
    "    parser.add_argument('--mask_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--dice_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--bbox_loss_coef', default=5, type=float)\n",
    "    parser.add_argument('--giou_loss_coef', default=2, type=float)\n",
    "    parser.add_argument('--eos_coef', default=0.1, type=float,\n",
    "                        help=\"Relative classification weight of the no-object class\")\n",
    "\n",
    "    # dataset parameters\n",
    "    parser.add_argument('--dataset_file', default='coco')\n",
    "    parser.add_argument('--coco_path', type=str, default='/datasets01/COCO/022719')\n",
    "    parser.add_argument('--coco_panoptic_path', type=str, default='/datasets01/COCO/060419')\n",
    "    parser.add_argument('--remove_difficult', action='store_true')\n",
    "    parser.add_argument('--masks', action='store_true')\n",
    "\n",
    "    parser.add_argument('--output-dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=42, type=int)\n",
    "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "    parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--num_workers', default=2, type=int)\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world-size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--dist-url', default='env://', help='url used to set up distributed training')\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=18.69s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import models.detr \n",
    "import datasets.coco\n",
    "parser = create_parser()\n",
    "pretrained_path = '/private/home/padentomasello/scratch/pytorch_testing/detr-r50-e632da11.pth'\n",
    "args = parser.parse_args([\"--eos_coef=0.5\", \"--resume=/private/home/padentomasello/scratch/pytorch_testing/detr-r50-e632da11.pth\"])\n",
    "args = parser.parse_args([])\n",
    "model, criterion, post = models.detr.build(args)   \n",
    "dataset = datasets.coco.build('train', args)\n",
    "if args.resume:\n",
    "    if args.resume.startswith('https'):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            args.resume, map_location='cpu', check_hash=True)\n",
    "    else:\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from torch.utils.data import DataLoader, DistributedSampler\n",
    "    import util.misc as utils\n",
    "    dataset_train = dataset\n",
    "    sampler_train = torch.utils.data.SequentialSampler(dataset)\n",
    "    #batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "        #    sampler_train, args.batch_size, drop_last=True)\n",
    "    batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "            sampler_train, 1, drop_last=True)\n",
    "    data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                                collate_fn=utils.collate_fn, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(args.device)\n",
    "for (sample_tmp, target_tmp) in data_loader_train:\n",
    "    samples = sample_tmp.to(device)\n",
    "    targets  = [{k: v.to(device) for k, v in t.items()} for t in target_tmp]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "429"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.transformer import *\n",
    "\n",
    "\n",
    "from models.backbone import *\n",
    "filepath = '/private/home/padentomasello/scratch/pytorch_testing/detr.array'\n",
    "\n",
    "N = 2\n",
    "C = 3\n",
    "H = 224\n",
    "W = 224\n",
    "\n",
    "\n",
    "image = samples.tensors\n",
    "image.requires_grad = True\n",
    "mask = samples.mask\n",
    "\n",
    "af.array.save_array('image', toArrayFire(image), filepath, False)\n",
    "af.array.save_array('mask', toArrayFire(mask.float()), filepath, True)\n",
    "target_boxes = targets[0]['boxes']\n",
    "target_classes = targets[0]['labels']\n",
    "af.array.save_array('target_boxes', toArrayFire(target_boxes), filepath, True)\n",
    "af.array.save_array('target_labels', toArrayFire(target_classes.float()), filepath, True)\n",
    "#af.array.save_array('pos', toArrayFire(pos), filepath, True)\n",
    "       \n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "output = model(samples)\n",
    "saveStateDict(model, filepath)\n",
    "af.array.save_array('pred_logits', toArrayFire(output['pred_logits']), filepath, True)\n",
    "af.array.save_array('pred_boxes', toArrayFire(output['pred_boxes']), filepath, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "criteiron(output, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([19, 20, 25, 41, 49, 54, 60, 79]), tensor([1, 4, 7, 5, 0, 6, 3, 2]))]\n",
      "here\n",
      "tensor([[91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 51, 55, 91, 91, 91, 91, 55, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 55, 91, 91, 91, 91, 91, 91, 91, 51, 91, 91, 91, 91,\n",
      "         55, 91, 91, 91, 91, 91, 51, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 56, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91]], device='cuda:1')\n",
      "tensor([[[-0.4003,  0.9255,  1.0972,  ..., -0.5616, -0.4813, -0.6507],\n",
      "         [-0.7505,  0.4491,  1.3047,  ..., -0.6015,  0.2295, -0.0884],\n",
      "         [-0.4807,  0.6435,  1.4609,  ..., -0.4893, -0.6034, -0.1083],\n",
      "         ...,\n",
      "         [-0.1278,  0.2133,  1.6843,  ...,  0.3013,  0.0066, -0.2945],\n",
      "         [-0.3218,  0.3704,  1.3105,  ..., -0.5942, -0.2094, -0.4208],\n",
      "         [-0.9136,  0.7373,  1.1797,  ..., -0.2602, -0.1954, -0.2176]]],\n",
      "       device='cuda:1', grad_fn=<SelectBackward>)\n",
      "> \u001b[0;32m/private/home/padentomasello/code/detection-transformer/models/detr.py\u001b[0m(116)\u001b[0;36mloss_labels\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    114 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    115 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 116 \u001b[0;31m        \u001b[0mloss_ce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    117 \u001b[0;31m        \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss_ce'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_ce\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    118 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n",
      "{'loss_ce': tensor(4.7681, device='cuda:1', grad_fn=<NllLoss2DBackward>), 'class_error': tensor(100., device='cuda:1'), 'loss_bbox': tensor(0.8930, device='cuda:1', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9785, device='cuda:1', grad_fn=<DivBackward0>), 'cardinality_error': tensor(92., device='cuda:1')}\n",
      "0\n",
      "tensor([[91, 91, 91, 91, 91, 91, 91, 91, 51, 91, 91, 91, 91, 91, 91, 55, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 56, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 51, 91, 91, 91, 91, 91,\n",
      "         51, 91, 91, 55, 91, 55, 91, 91, 91, 91, 91, 91, 55, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91]], device='cuda:1')\n",
      "tensor([[[ 1.2962, -0.0960, -0.1628,  ..., -0.0368,  0.1778, -0.0167],\n",
      "         [ 1.3912, -0.0837,  0.0872,  ...,  0.3883,  0.1894, -0.4947],\n",
      "         [ 0.8014,  0.3611, -0.0238,  ...,  0.2851,  0.4834, -0.1553],\n",
      "         ...,\n",
      "         [ 1.1673,  0.1670, -0.2164,  ...,  0.1387, -0.0020, -0.8401],\n",
      "         [ 0.9665, -0.0895,  0.0451,  ..., -0.0697,  0.1298, -0.1431],\n",
      "         [ 0.9766,  0.1634,  0.0312,  ..., -0.1151,  0.5190, -0.4751]]],\n",
      "       device='cuda:1', grad_fn=<UnbindBackward>)\n",
      "> \u001b[0;32m/private/home/padentomasello/code/detection-transformer/models/detr.py\u001b[0m(116)\u001b[0;36mloss_labels\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    114 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    115 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 116 \u001b[0;31m        \u001b[0mloss_ce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    117 \u001b[0;31m        \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss_ce'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_ce\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    118 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n",
      "1\n",
      "tensor([[91, 91, 91, 91, 91, 55, 91, 91, 91, 91, 55, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 51, 91, 91, 91, 91, 91, 91, 56, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 55, 91, 91,\n",
      "         51, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 51, 91, 91, 91, 91,\n",
      "         91, 55, 91, 91, 91, 91, 91, 91, 91, 91]], device='cuda:1')\n",
      "tensor([[[ 0.8197, -0.0990, -0.5846,  ...,  0.6443,  1.6575,  0.1680],\n",
      "         [ 1.2249,  0.1150, -0.5014,  ...,  0.8021,  1.1998,  0.2287],\n",
      "         [ 1.1372,  0.6634, -0.5075,  ...,  0.4994,  0.8220,  0.2282],\n",
      "         ...,\n",
      "         [ 1.0907,  0.4093, -0.4825,  ...,  0.9186,  1.2505, -0.1348],\n",
      "         [ 0.8983, -0.0697, -0.2367,  ...,  0.3931,  0.9532,  0.2548],\n",
      "         [ 0.8004,  0.4657, -0.6113,  ...,  0.4321,  1.5155,  0.0898]]],\n",
      "       device='cuda:1', grad_fn=<UnbindBackward>)\n",
      "> \u001b[0;32m/private/home/padentomasello/code/detection-transformer/models/detr.py\u001b[0m(116)\u001b[0;36mloss_labels\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    114 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    115 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 116 \u001b[0;31m        \u001b[0mloss_ce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    117 \u001b[0;31m        \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss_ce'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_ce\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    118 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n",
      "2\n",
      "tensor([[91, 91, 55, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 51, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 51, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         55, 91, 91, 91, 91, 91, 91, 91, 91, 91, 55, 91, 51, 91, 56, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 55, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91]], device='cuda:1')\n",
      "tensor([[[ 0.5692, -0.1504,  0.5054,  ...,  0.7340,  1.2496,  0.4051],\n",
      "         [ 0.4786, -0.2607,  0.4794,  ...,  0.8888,  0.4727,  0.8046],\n",
      "         [ 0.8147,  0.2669,  0.4343,  ...,  0.3554,  0.4156,  0.2191],\n",
      "         ...,\n",
      "         [ 0.5495,  0.0389,  0.8543,  ...,  1.0614,  0.3681,  0.3142],\n",
      "         [ 0.6870,  0.0035,  0.6599,  ...,  0.4336,  0.3102,  0.5370],\n",
      "         [ 0.5010,  0.3532,  0.4098,  ...,  0.4441,  0.5273,  0.4528]]],\n",
      "       device='cuda:1', grad_fn=<UnbindBackward>)\n",
      "> \u001b[0;32m/private/home/padentomasello/code/detection-transformer/models/detr.py\u001b[0m(116)\u001b[0;36mloss_labels\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    114 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    115 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 116 \u001b[0;31m        \u001b[0mloss_ce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    117 \u001b[0;31m        \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss_ce'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_ce\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    118 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n",
      "3\n",
      "tensor([[55, 91, 91, 55, 91, 91, 51, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 56, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         55, 91, 91, 91, 51, 91, 91, 91, 91, 55, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 51, 91, 91, 91]], device='cuda:1')\n",
      "tensor([[[ 0.1645,  0.0017,  0.0914,  ...,  0.1856,  1.1034, -0.1991],\n",
      "         [ 0.1114, -0.0102,  0.0188,  ..., -0.0274,  0.5362,  0.2609],\n",
      "         [ 0.3573, -0.0426,  0.0083,  ..., -0.0234,  0.1569, -0.0811],\n",
      "         ...,\n",
      "         [ 0.1599,  0.0555,  0.2801,  ...,  0.3164,  0.6365,  0.3323],\n",
      "         [ 0.3308,  0.2408,  0.1793,  ..., -0.1870,  0.0937, -0.0182],\n",
      "         [-0.1095,  0.5939, -0.2360,  ...,  0.1653,  0.3242,  0.1297]]],\n",
      "       device='cuda:1', grad_fn=<UnbindBackward>)\n",
      "> \u001b[0;32m/private/home/padentomasello/code/detection-transformer/models/detr.py\u001b[0m(116)\u001b[0;36mloss_labels\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    114 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    115 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 116 \u001b[0;31m        \u001b[0mloss_ce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    117 \u001b[0;31m        \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss_ce'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_ce\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    118 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> \n",
      "ipdb> c\n",
      "4\n",
      "tensor([[91, 91, 91, 91, 51, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         51, 91, 55, 91, 91, 91, 91, 91, 91, 91, 91, 55, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         55, 91, 91, 91, 91, 91, 91, 91, 91, 91, 55, 91, 91, 91, 56, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 51, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91]], device='cuda:1')\n",
      "tensor([[[0.6702, 0.9894, 0.4718,  ..., 0.6125, 0.1978, 0.2717],\n",
      "         [0.3011, 0.8240, 0.3921,  ..., 0.4535, 0.5619, 0.4615],\n",
      "         [0.3034, 0.9841, 0.5375,  ..., 0.2193, 0.0672, 0.4581],\n",
      "         ...,\n",
      "         [0.2953, 0.6225, 0.8112,  ..., 0.6245, 0.4731, 0.6887],\n",
      "         [0.5266, 0.5809, 0.7850,  ..., 0.1131, 0.0039, 0.1978],\n",
      "         [0.2269, 1.1456, 0.4177,  ..., 0.7021, 0.1615, 0.2307]]],\n",
      "       device='cuda:1', grad_fn=<UnbindBackward>)\n",
      "> \u001b[0;32m/private/home/padentomasello/code/detection-transformer/models/detr.py\u001b[0m(116)\u001b[0;36mloss_labels\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    114 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    115 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 116 \u001b[0;31m        \u001b[0mloss_ce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    117 \u001b[0;31m        \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss_ce'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_ce\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    118 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n"
     ]
    }
   ],
   "source": [
    "losses = criterion(output, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses['loss_giou'].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0738e-05,  2.9968e-06, -4.1803e-07,  ...,  4.1247e-06,\n",
       "            3.9031e-06, -4.9195e-06],\n",
       "          [ 1.4272e-05,  3.9245e-06,  8.9113e-06,  ...,  1.0274e-05,\n",
       "            4.2167e-06,  1.0273e-05],\n",
       "          [ 9.4969e-06,  9.7026e-06,  1.2353e-05,  ..., -1.9450e-05,\n",
       "           -2.8163e-06,  4.8002e-07],\n",
       "          ...,\n",
       "          [ 2.5895e-06,  5.1558e-07, -1.7987e-06,  ..., -7.7188e-08,\n",
       "           -2.0928e-06, -2.6594e-06],\n",
       "          [ 1.0131e-06, -9.8837e-06, -5.8390e-06,  ...,  1.2874e-06,\n",
       "           -1.3857e-07, -1.6747e-06],\n",
       "          [-4.2552e-06, -1.2070e-05, -1.3567e-05,  ...,  2.2641e-06,\n",
       "            2.4957e-06,  1.8481e-06]],\n",
       "\n",
       "         [[ 2.3748e-05,  1.6433e-05,  9.2160e-06,  ...,  1.0975e-05,\n",
       "            2.1906e-06, -1.1669e-05],\n",
       "          [ 2.9663e-05,  2.2031e-05,  2.4164e-05,  ...,  1.2048e-05,\n",
       "            3.2029e-06,  1.0524e-05],\n",
       "          [ 2.7922e-05,  3.2580e-05,  2.6750e-05,  ..., -2.2351e-05,\n",
       "           -4.8019e-06,  2.3344e-06],\n",
       "          ...,\n",
       "          [ 5.6174e-06,  9.3361e-06,  6.9638e-06,  ...,  1.2765e-07,\n",
       "           -7.8868e-07, -1.6143e-06],\n",
       "          [ 8.9687e-07, -1.0606e-05, -2.2827e-06,  ...,  1.8241e-06,\n",
       "            2.1983e-07, -2.3271e-06],\n",
       "          [-1.8395e-06, -1.0136e-05, -1.0475e-05,  ...,  5.2389e-06,\n",
       "            3.0860e-06,  1.7699e-06]],\n",
       "\n",
       "         [[ 7.3107e-06,  3.4112e-06,  1.2151e-06,  ...,  4.0677e-06,\n",
       "           -3.6284e-07, -8.3943e-06],\n",
       "          [ 9.7447e-06,  3.6829e-06,  5.7699e-06,  ...,  5.4116e-06,\n",
       "           -1.9387e-06,  4.1335e-06],\n",
       "          [ 1.2083e-05,  1.5113e-05,  1.2010e-05,  ..., -2.1611e-05,\n",
       "           -5.7641e-06, -2.6195e-07],\n",
       "          ...,\n",
       "          [ 1.2980e-07,  6.0594e-06,  7.5738e-06,  ..., -1.9369e-07,\n",
       "           -5.3686e-07,  1.7603e-07],\n",
       "          [ 1.5953e-06, -4.4805e-06,  3.3504e-06,  ...,  1.6860e-06,\n",
       "            6.7576e-07, -1.5849e-06],\n",
       "          [ 2.7912e-06, -1.6050e-06, -2.2143e-06,  ...,  2.7484e-06,\n",
       "            7.9693e-07, -6.4467e-08]]]], device='cuda:1')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'backbone' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6b5ba277cffc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbackbone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'backbone' is not defined"
     ]
    }
   ],
   "source": [
    "backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
