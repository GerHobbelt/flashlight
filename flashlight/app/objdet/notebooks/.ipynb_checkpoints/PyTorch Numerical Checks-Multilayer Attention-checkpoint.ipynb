{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arrayfire as af\n",
    "import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toArrayFire(x):\n",
    "    x_np = x.detach().contiguous().numpy()\n",
    "    shape = 1\n",
    "    if len(x_np.shape) == 0:\n",
    "        shape = (1,)\n",
    "    else:\n",
    "        shape = x_np.shape[::-1]\n",
    "    print(shape)\n",
    "    afArray = af.Array(x_np.ctypes.data, shape, x_np.dtype.char)\n",
    "    return afArray\n",
    "\n",
    "def saveStateDict(module, filepath):\n",
    "    i = 0\n",
    "    for (name, param) in module.named_parameters():\n",
    "        #param = module.state_dict()[name]\n",
    "        print(name, \"\\t\", param.size())\n",
    "        if 'in_proj' in name:\n",
    "            print(param.shape)\n",
    "            q, k, v = param.chunk(3, dim=0)\n",
    "            print('in_proj!')\n",
    "            af.array.save_array(name + 'q', toArrayFire(q), filepath, True)\n",
    "            af.array.save_array(name + 'k', toArrayFire(k), filepath, True)\n",
    "            af.array.save_array(name + 'v', toArrayFire(k), filepath, True)\n",
    "            continue\n",
    "        if len(param.size()) > 0:\n",
    "            af_array = toArrayFire(param)\n",
    "            if 'fc' in name and 'weight' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            af.array.save_array(name, af_array, filepath, True)\n",
    "            i = i + 1\n",
    "    print(i)\n",
    "    for name in module.state_dict():\n",
    "        if 'running' in name:\n",
    "            print(name)\n",
    "            af_array = toArrayFire(module.state_dict()[name])\n",
    "            af.array.save_array(name, af_array, filepath + 'running', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.backbone import *\n",
    "from models.position_encoding import *\n",
    "from models.matcher import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "embedding_size = 12\n",
    "src_len = 5\n",
    "src = torch.rand(src_len, batch_size, embedding_size)\n",
    "model = TransformerEncoderLayer(embedding_size, 1, dropout=0.0, dim_feedforward=128)\n",
    "output = model.forward(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 1, 5)\n",
      "(12, 12)\n",
      "self_attn.in_proj_bias\n",
      "here!\n",
      "0q_0self_attn.in_proj_weight 1\n",
      "(12,)\n",
      "self_attn.in_proj_bias\n",
      "0q_1self_attn.in_proj_bias 2\n",
      "(12, 12)\n",
      "self_attn.in_proj_bias\n",
      "here!\n",
      "1k_0self_attn.in_proj_weight 3\n",
      "(12,)\n",
      "self_attn.in_proj_bias\n",
      "1k_1self_attn.in_proj_bias 4\n",
      "(12, 12)\n",
      "self_attn.in_proj_bias\n",
      "here!\n",
      "2v_0self_attn.in_proj_weight 5\n",
      "(12,)\n",
      "self_attn.in_proj_bias\n",
      "2v_1self_attn.in_proj_bias 6\n",
      "(12, 12)\n",
      "self_attn.out_proj.weight 7\n",
      "(12,)\n",
      "self_attn.out_proj.bias 8\n",
      "(12, 128)\n",
      "linear1.weight 9\n",
      "(128,)\n",
      "linear1.bias 10\n",
      "(128, 12)\n",
      "linear2.weight 11\n",
      "(12,)\n",
      "linear2.bias 12\n",
      "(12,)\n",
      "norm1.weight 13\n",
      "(12,)\n",
      "norm1.bias 14\n",
      "(12,)\n",
      "norm2.weight 15\n",
      "(12,)\n",
      "norm2.bias 16\n",
      "(12, 1, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = '/private/home/padentomasello/scratch/pytorch_testing/transformer_encoder_layer.array'\n",
    "af.array.save_array('input', toArrayFire(src), filepath, False)\n",
    "i = 1\n",
    "params = {}\n",
    "for (name, param) in model.named_parameters():\n",
    "        if 'in_proj' in name:\n",
    "            q, k, v = param.chunk(3, dim=0)\n",
    "            hack = '0'\n",
    "            if 'in_proj_bias' in name: hack = '1'\n",
    "            params['0q_' + hack + name] = q\n",
    "            params['1k_' + hack + name] = k\n",
    "            params['2v_' + hack + name] = v\n",
    "            if 'in_proj_bias' in name:\n",
    "                for key in sorted(params.keys()):\n",
    "                    af_array = toArrayFire(params[key])\n",
    "                    print(name)\n",
    "                    if 'weight' in key:\n",
    "                        print('here!')\n",
    "                        af_array = af.array.transpose(af_array)\n",
    "                    print(key, i)\n",
    "                    af.array.save_array(key, af_array, filepath, True)  \n",
    "                    i = i + 1\n",
    "            continue\n",
    "        elif len(param.size()) > 0:\n",
    "            af_array = toArrayFire(param)\n",
    "            if 'fc' in name and 'weight' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            if 'weight' in name and 'proj' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            if 'weight' in name and 'linear' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            print(name, i)\n",
    "            af.array.save_array(name, af_array, filepath, True)\n",
    "            i = i + 1\n",
    "#af.array.save_array('output', toArrayFire(output), filepath, True)\n",
    "af.array.save_array('output', toArrayFire(output), filepath, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5014,  0.1196, -0.5075, -0.1015, -0.2118,  1.5463, -1.8383,\n",
       "           0.2376,  0.9020,  1.5983, -0.4331,  0.1896]],\n",
       "\n",
       "        [[-1.2909,  1.9060,  1.3883, -0.5120,  0.2343,  0.7350, -1.1004,\n",
       "          -0.8522,  0.1419,  0.1706,  0.4822, -1.3027]],\n",
       "\n",
       "        [[-1.4181,  0.4739,  0.6236, -0.1970,  0.0347, -0.1067, -0.3355,\n",
       "          -1.0924,  0.7528,  1.9335,  0.9865, -1.6553]],\n",
       "\n",
       "        [[-0.2432, -0.9983, -0.0453,  0.5848, -0.7483,  2.1629,  0.3817,\n",
       "          -0.6568, -1.4413,  1.5481,  0.0131, -0.5575]],\n",
       "\n",
       "        [[-1.2545,  0.1035,  0.4713, -0.4119,  1.0937,  1.3193, -1.6640,\n",
       "          -0.9041,  0.4546,  0.7479,  1.1912, -1.1471]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.modules.linear import _LinearWithBias\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.init import constant_\n",
    "from torch.nn.init import xavier_normal_\n",
    "from torch.nn.parameter import Parameter\n",
    "#from .module import Module\n",
    "#from .. import functional as F\n",
    "\n",
    "class MultiheadAttention2(nn.Module):\n",
    "    r\"\"\"Allows the model to jointly attend to information\n",
    "    from different representation subspaces.\n",
    "    See reference: Attention Is All You Need\n",
    "\n",
    "    .. math::\n",
    "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
    "        \\text{where} head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "\n",
    "    Args:\n",
    "        embed_dim: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n",
    "        bias: add bias as module parameter. Default: True.\n",
    "        add_bias_kv: add bias to the key and value sequences at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        kdim: total number of features in key. Default: None.\n",
    "        vdim: total number of features in value. Default: None.\n",
    "\n",
    "        Note: if kdim and vdim are None, they will be set to embed_dim such that\n",
    "        query, key, and value have the same number of features.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "    \"\"\"\n",
    "    bias_k: Optional[torch.Tensor]\n",
    "    bias_v: Optional[torch.Tensor]\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None):\n",
    "        super(MultiheadAttention2, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        if self._qkv_same_embed_dim is False:\n",
    "            self.q_proj_weight = Parameter(torch.Tensor(embed_dim, embed_dim))\n",
    "            self.k_proj_weight = Parameter(torch.Tensor(embed_dim, self.kdim))\n",
    "            self.v_proj_weight = Parameter(torch.Tensor(embed_dim, self.vdim))\n",
    "            self.register_parameter('in_proj_weight', None)\n",
    "        else:\n",
    "            self.in_proj_weight = Parameter(torch.empty(3 * embed_dim, embed_dim))\n",
    "            self.register_parameter('q_proj_weight', None)\n",
    "            self.register_parameter('k_proj_weight', None)\n",
    "            self.register_parameter('v_proj_weight', None)\n",
    "\n",
    "        if bias:\n",
    "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim))\n",
    "        else:\n",
    "            self.register_parameter('in_proj_bias', None)\n",
    "        self.out_proj = _LinearWithBias(embed_dim, embed_dim)\n",
    "\n",
    "        if add_bias_kv:\n",
    "            self.bias_k = Parameter(torch.empty(1, 1, embed_dim))\n",
    "            self.bias_v = Parameter(torch.empty(1, 1, embed_dim))\n",
    "        else:\n",
    "            self.bias_k = self.bias_v = None\n",
    "\n",
    "        self.add_zero_attn = add_zero_attn\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        if self._qkv_same_embed_dim:\n",
    "            xavier_uniform_(self.in_proj_weight)\n",
    "        else:\n",
    "            xavier_uniform_(self.q_proj_weight)\n",
    "            xavier_uniform_(self.k_proj_weight)\n",
    "            xavier_uniform_(self.v_proj_weight)\n",
    "\n",
    "        if self.in_proj_bias is not None:\n",
    "            constant_(self.in_proj_bias, 0.)\n",
    "            constant_(self.out_proj.bias, 0.)\n",
    "        if self.bias_k is not None:\n",
    "            xavier_normal_(self.bias_k)\n",
    "        if self.bias_v is not None:\n",
    "            xavier_normal_(self.bias_v)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
    "        if '_qkv_same_embed_dim' not in state:\n",
    "            state['_qkv_same_embed_dim'] = True\n",
    "\n",
    "        super(MultiheadAttention, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None,\n",
    "                need_weights=True, attn_mask=None):\n",
    "        # type: (Tensor, Tensor, Tensor, Optional[Tensor], bool, Optional[Tensor]) -> Tuple[Tensor, Optional[Tensor]]\n",
    "        r\"\"\"\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. When given a binary mask and a value is True,\n",
    "            the corresponding value on the attention layer will be ignored. When given\n",
    "            a byte mask and a value is non-zero, the corresponding value on the attention\n",
    "            layer will be ignored\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
    "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
    "\n",
    "    Shape:\n",
    "        - Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
    "          If a ByteTensor is provided, the non-zero positions will be ignored while the position\n",
    "          with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n",
    "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked\n",
    "          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
    "          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
    "          is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "          is provided, it will be added to the attention weight.\n",
    "\n",
    "        - Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension.\n",
    "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
    "          L is the target sequence length, S is the source sequence length.\n",
    "        \"\"\"\n",
    "        if not self._qkv_same_embed_dim:\n",
    "            return multi_head_attention_forward2(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask, use_separate_proj_weight=True,\n",
    "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
    "                v_proj_weight=self.v_proj_weight)\n",
    "        else:\n",
    "            return multi_head_attention_forward2(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(input, p=0.5, training=True, inplace=False):\n",
    "    # type: (Tensor, float, bool, bool) -> Tensor\n",
    "    r\"\"\"\n",
    "    During training, randomly zeroes some of the elements of the input\n",
    "    tensor with probability :attr:`p` using samples from a Bernoulli\n",
    "    distribution.\n",
    "    See :class:`~torch.nn.Dropout` for details.\n",
    "    Args:\n",
    "        p: probability of an element to be zeroed. Default: 0.5\n",
    "        training: apply dropout if is ``True``. Default: ``True``\n",
    "        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n",
    "    \"\"\"\n",
    "    assert(p == 0.0)\n",
    "    return input\n",
    "    if p < 0. or p > 1.:\n",
    "        raise ValueError(\"dropout probability has to be between 0 and 1, \"\n",
    "                         \"but got {}\".format(p))\n",
    "    return (_VF.dropout_(input, p, training)\n",
    "            if inplace\n",
    "            else _VF.dropout(input, p, training))\n",
    "\n",
    "\n",
    "def softmax(input, dim=None, _stacklevel=3, dtype=None):\n",
    "    # type: (Tensor, Optional[int], int, Optional[int]) -> Tensor\n",
    "    r\"\"\"Applies a softmax function.\n",
    "    Softmax is defined as:\n",
    "    :math:`\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}`\n",
    "    It is applied to all slices along dim, and will re-scale them so that the elements\n",
    "    lie in the range `[0, 1]` and sum to 1.\n",
    "    See :class:`~torch.nn.Softmax` for more details.\n",
    "    Arguments:\n",
    "        input (Tensor): input\n",
    "        dim (int): A dimension along which softmax will be computed.\n",
    "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
    "          If specified, the input tensor is casted to :attr:`dtype` before the operation\n",
    "          is performed. This is useful for preventing data type overflows. Default: None.\n",
    "    .. note::\n",
    "        This function doesn't work directly with NLLLoss,\n",
    "        which expects the Log to be computed between the Softmax and itself.\n",
    "        Use log_softmax instead (it's faster and has better numerical properties).\n",
    "    \"\"\"\n",
    "    if dim is None:\n",
    "        dim = _get_softmax_dim('softmax', input.dim(), _stacklevel)\n",
    "    if dtype is None:\n",
    "        ret = input.softmax(dim)\n",
    "    else:\n",
    "        ret = input.softmax(dim, dtype=dtype)\n",
    "    return ret\n",
    "\n",
    "def linear(input, weight, bias=None):\n",
    "    # type: (Tensor, Tensor, Optional[Tensor]) -> Tensor\n",
    "    r\"\"\"\n",
    "    Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.\n",
    "    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *, in\\_features)` N is the batch size, `*` means any number of\n",
    "          additional dimensions\n",
    "        - Weight: :math:`(out\\_features, in\\_features)`\n",
    "        - Bias: :math:`(out\\_features)`\n",
    "        - Output: :math:`(N, *, out\\_features)`\n",
    "    \"\"\"\n",
    "    tens_ops = (input, weight)\n",
    "    if input.dim() == 2 and bias is not None:\n",
    "        # fused op is marginally faster\n",
    "        ret = torch.addmm(bias, input, weight.t())\n",
    "    else:\n",
    "        output = input.matmul(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias\n",
    "        ret = output\n",
    "    return ret\n",
    "\n",
    "test = 0\n",
    "def multi_head_attention_forward2(query: Tensor,\n",
    "                                 key: Tensor,\n",
    "                                 value: Tensor,\n",
    "                                 embed_dim_to_check: int,\n",
    "                                 num_heads: int,\n",
    "                                 in_proj_weight: Tensor,\n",
    "                                 in_proj_bias: Tensor,\n",
    "                                 bias_k: Optional[Tensor],\n",
    "                                 bias_v: Optional[Tensor],\n",
    "                                 add_zero_attn: bool,\n",
    "                                 dropout_p: float,\n",
    "                                 out_proj_weight: Tensor,\n",
    "                                 out_proj_bias: Tensor,\n",
    "                                 training: bool = True,\n",
    "                                 key_padding_mask: Optional[Tensor] = None,\n",
    "                                 need_weights: bool = True,\n",
    "                                 attn_mask: Optional[Tensor] = None,\n",
    "                                 use_separate_proj_weight: bool = False,\n",
    "                                 q_proj_weight: Optional[Tensor] = None,\n",
    "                                 k_proj_weight: Optional[Tensor] = None,\n",
    "                                 v_proj_weight: Optional[Tensor] = None,\n",
    "                                 static_k: Optional[Tensor] = None,\n",
    "                                 static_v: Optional[Tensor] = None\n",
    "                                 ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        embed_dim_to_check: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        in_proj_weight, in_proj_bias: input projection weight and bias.\n",
    "        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        dropout_p: probability of an element to be zeroed.\n",
    "        out_proj_weight, out_proj_bias: the output projection weight and bias.\n",
    "        training: apply dropout if is ``True``.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. This is an binary mask. When the value is True,\n",
    "            the corresponding value on the attention layer will be filled with -inf.\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
    "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
    "        use_separate_proj_weight: the function accept the proj. weights for query, key,\n",
    "            and value in different forms. If false, in_proj_weight will be used, which is\n",
    "            a combination of q_proj_weight, k_proj_weight, v_proj_weight.\n",
    "        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.\n",
    "        static_k, static_v: static key and value used for attention operators.\n",
    "    Shape:\n",
    "        Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
    "          If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions\n",
    "          will be unchanged. If a BoolTensor is provided, the positions with the\n",
    "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked\n",
    "          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
    "          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
    "          are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "          is provided, it will be added to the attention weight.\n",
    "        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
    "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
    "        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
    "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
    "        Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension.\n",
    "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
    "          L is the target sequence length, S is the source sequence length.\n",
    "    \"\"\"\n",
    "    global test\n",
    "    tgt_len, bsz, embed_dim = query.size()\n",
    "    assert embed_dim == embed_dim_to_check\n",
    "    # allow MHA to have different sizes for the feature dimension\n",
    "    assert key.size(0) == value.size(0) and key.size(1) == value.size(1)\n",
    "\n",
    "    head_dim = embed_dim // num_heads\n",
    "    assert head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "    scaling = float(head_dim) ** -0.5\n",
    "\n",
    "    if not use_separate_proj_weight:\n",
    "        if (query is key or torch.equal(query, key)) and (key is value or torch.equal(key, value)):\n",
    "            # self-attention\n",
    "            print('in_proj_weight', in_proj_weight)\n",
    "            print('query', query)\n",
    "            print(in_proj_bias)\n",
    "            q, k, v = linear(query, in_proj_weight, in_proj_bias).chunk(3, dim=-1)\n",
    "            print('q', q)\n",
    "            print('here2')\n",
    "            test = (in_proj_weight, in_proj_bias)\n",
    "            print('here!')\n",
    "\n",
    "        elif (key is value or torch.equal(key, value)):\n",
    "            # encoder-decoder attention\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = 0\n",
    "            _end = embed_dim\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            q = linear(query, _w, _b)\n",
    "\n",
    "            if key is None:\n",
    "                assert value is None\n",
    "                k = None\n",
    "                v = None\n",
    "            else:\n",
    "\n",
    "                # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "                _b = in_proj_bias\n",
    "                _start = embed_dim\n",
    "                _end = None\n",
    "                _w = in_proj_weight[_start:, :]\n",
    "                if _b is not None:\n",
    "                    _b = _b[_start:]\n",
    "                k, v = linear(key, _w, _b).chunk(2, dim=-1)\n",
    "\n",
    "        else:\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = 0\n",
    "            _end = embed_dim\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            q = linear(query, _w, _b)\n",
    "\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = embed_dim\n",
    "            _end = embed_dim * 2\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            k = linear(key, _w, _b)\n",
    "\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = embed_dim * 2\n",
    "            _end = None\n",
    "            _w = in_proj_weight[_start:, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:]\n",
    "            v = linear(value, _w, _b)\n",
    "    else:\n",
    "        q_proj_weight_non_opt = torch.jit._unwrap_optional(q_proj_weight)\n",
    "        len1, len2 = q_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == query.size(-1)\n",
    "\n",
    "        k_proj_weight_non_opt = torch.jit._unwrap_optional(k_proj_weight)\n",
    "        len1, len2 = k_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == key.size(-1)\n",
    "\n",
    "        v_proj_weight_non_opt = torch.jit._unwrap_optional(v_proj_weight)\n",
    "        len1, len2 = v_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == value.size(-1)\n",
    "\n",
    "        if in_proj_bias is not None:\n",
    "            q = linear(query, q_proj_weight_non_opt, in_proj_bias[0:embed_dim])\n",
    "            k = linear(key, k_proj_weight_non_opt, in_proj_bias[embed_dim:(embed_dim * 2)])\n",
    "            v = linear(value, v_proj_weight_non_opt, in_proj_bias[(embed_dim * 2):])\n",
    "        else:\n",
    "            q = linear(query, q_proj_weight_non_opt, in_proj_bias)\n",
    "            k = linear(key, k_proj_weight_non_opt, in_proj_bias)\n",
    "            v = linear(value, v_proj_weight_non_opt, in_proj_bias)\n",
    "    q = q * scaling\n",
    "    \n",
    "    print('q', q)\n",
    "    print('k', k)\n",
    "    print('v', v)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        assert attn_mask.dtype == torch.float32 or attn_mask.dtype == torch.float64 or \\\n",
    "            attn_mask.dtype == torch.float16 or attn_mask.dtype == torch.uint8 or attn_mask.dtype == torch.bool, \\\n",
    "            'Only float, byte, and bool types are supported for attn_mask, not {}'.format(attn_mask.dtype)\n",
    "        if attn_mask.dtype == torch.uint8:\n",
    "            warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "            attn_mask = attn_mask.to(torch.bool)\n",
    "\n",
    "        if attn_mask.dim() == 2:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "            if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:\n",
    "                raise RuntimeError('The size of the 2D attn_mask is not correct.')\n",
    "        elif attn_mask.dim() == 3:\n",
    "            if list(attn_mask.size()) != [bsz * num_heads, query.size(0), key.size(0)]:\n",
    "                raise RuntimeError('The size of the 3D attn_mask is not correct.')\n",
    "        else:\n",
    "            raise RuntimeError(\"attn_mask's dimension {} is not supported\".format(attn_mask.dim()))\n",
    "        # attn_mask's dim is 3 now.\n",
    "\n",
    "    # convert ByteTensor key_padding_mask to bool\n",
    "    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n",
    "        warnings.warn(\"Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "        key_padding_mask = key_padding_mask.to(torch.bool)\n",
    "\n",
    "    if bias_k is not None and bias_v is not None:\n",
    "        if static_k is None and static_v is None:\n",
    "            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
    "            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = pad(attn_mask, (0, 1))\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "        else:\n",
    "            assert static_k is None, \"bias cannot be added to static key.\"\n",
    "            assert static_v is None, \"bias cannot be added to static value.\"\n",
    "    else:\n",
    "        assert bias_k is None\n",
    "        assert bias_v is None\n",
    "\n",
    "    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if k is not None:\n",
    "        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if v is not None:\n",
    "        v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    if static_k is not None:\n",
    "        assert static_k.size(0) == bsz * num_heads\n",
    "        assert static_k.size(2) == head_dim\n",
    "        k = static_k\n",
    "\n",
    "    if static_v is not None:\n",
    "        assert static_v.size(0) == bsz * num_heads\n",
    "        assert static_v.size(2) == head_dim\n",
    "        v = static_v\n",
    "\n",
    "    src_len = k.size(1)\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        assert key_padding_mask.size(0) == bsz\n",
    "        assert key_padding_mask.size(1) == src_len\n",
    "\n",
    "    if add_zero_attn:\n",
    "        src_len += 1\n",
    "        k = torch.cat([k, torch.zeros((k.size(0), 1) + k.size()[2:], dtype=k.dtype, device=k.device)], dim=1)\n",
    "        v = torch.cat([v, torch.zeros((v.size(0), 1) + v.size()[2:], dtype=v.dtype, device=v.device)], dim=1)\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = pad(attn_mask, (0, 1))\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "\n",
    "    attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "    print('presoftmax', attn_output_weights)\n",
    "    assert list(attn_output_weights.size()) == [bsz * num_heads, tgt_len, src_len]\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_output_weights.masked_fill_(attn_mask, float('-inf'))\n",
    "        else:\n",
    "            attn_output_weights += attn_mask\n",
    "\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        attn_output_weights = attn_output_weights.masked_fill(\n",
    "            key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "            float('-inf'),\n",
    "        )\n",
    "        attn_output_weights = attn_output_weights.view(bsz * num_heads, tgt_len, src_len)\n",
    "\n",
    "    attn_output_weights = softmax(\n",
    "        attn_output_weights, dim=-1)\n",
    "    print('post softmax', attn_output_weights)\n",
    "    attn_output_weights = dropout(attn_output_weights, p=dropout_p, training=training)\n",
    "\n",
    "    attn_output = torch.bmm(attn_output_weights, v)\n",
    "    print(attn_output)\n",
    "    assert list(attn_output.size()) == [bsz * num_heads, tgt_len, head_dim]\n",
    "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "    attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
    "    print('postlinear', attn_output)\n",
    "\n",
    "    if need_weights:\n",
    "        # average attention weights over heads\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        return attn_output, attn_output_weights.sum(dim=1) / num_heads\n",
    "    else:\n",
    "        return attn_output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "embedding_size = 128\n",
    "src_len = 5\n",
    "model = MultiheadAttention2(embedding_size, 1, 0.0)\n",
    "#saveStateDict(model, filepath)\n",
    "src = torch.rand(src_len, batch_size, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_proj_weight Parameter containing:\n",
      "tensor([[ 0.0221, -0.0871,  0.0028,  ..., -0.0816, -0.0740, -0.0484],\n",
      "        [ 0.0018, -0.1057,  0.1082,  ..., -0.0414, -0.0438, -0.0599],\n",
      "        [-0.0923,  0.0027,  0.0414,  ...,  0.0730, -0.0603,  0.0941],\n",
      "        ...,\n",
      "        [-0.0900,  0.0938, -0.0393,  ..., -0.0951, -0.0991,  0.0996],\n",
      "        [ 0.0688,  0.0846, -0.1076,  ...,  0.0786,  0.0087, -0.0278],\n",
      "        [-0.0387,  0.0337,  0.0541,  ...,  0.0106,  0.0518, -0.0577]],\n",
      "       requires_grad=True)\n",
      "query tensor([[[0.4507, 0.3978, 0.3292, 0.7993, 0.1455, 0.5029, 0.4128, 0.6619,\n",
      "          0.7281, 0.7888, 0.4567, 0.1429, 0.1231, 0.1555, 0.0853, 0.1047,\n",
      "          0.8893, 0.1699, 0.7958, 0.1827, 0.6350, 0.4942, 0.2215, 0.2232,\n",
      "          0.2446, 0.0082, 0.8311, 0.2873, 0.1066, 0.3383, 0.9547, 0.7938,\n",
      "          0.7326, 0.2105, 0.9199, 0.8011, 0.7520, 0.3603, 0.1147, 0.6205,\n",
      "          0.0443, 0.7305, 0.7692, 0.9795, 0.3291, 0.2285, 0.4546, 0.6528,\n",
      "          0.8171, 0.6167, 0.8848, 0.0936, 0.5757, 0.6838, 0.3168, 0.5413,\n",
      "          0.6204, 0.0368, 0.2150, 0.2175, 0.9606, 0.5120, 0.0049, 0.3854,\n",
      "          0.2879, 0.2369, 0.1137, 0.9705, 0.4318, 0.1572, 0.4127, 0.6569,\n",
      "          0.2861, 0.6285, 0.8095, 0.7338, 0.2877, 0.0868, 0.9712, 0.7986,\n",
      "          0.0173, 0.8577, 0.6411, 0.2505, 0.8589, 0.5242, 0.5540, 0.3214,\n",
      "          0.6041, 0.8022, 0.1101, 0.7983, 0.2882, 0.6311, 0.4015, 0.9389,\n",
      "          0.8762, 0.1065, 0.7766, 0.5289, 0.4745, 0.9961, 0.8459, 0.6414,\n",
      "          0.6230, 0.3945, 0.9672, 0.8220, 0.2352, 0.6368, 0.3176, 0.8986,\n",
      "          0.0899, 0.3532, 0.0204, 0.9836, 0.7683, 0.7756, 0.9938, 0.4223,\n",
      "          0.3388, 0.2263, 0.6733, 0.0650, 0.7375, 0.2708, 0.6356, 0.4239]],\n",
      "\n",
      "        [[0.1749, 0.7107, 0.1100, 0.2149, 0.9599, 0.6334, 0.2995, 0.2162,\n",
      "          0.7623, 0.6740, 0.8056, 0.0817, 0.3472, 0.8716, 0.4102, 0.6048,\n",
      "          0.3892, 0.4335, 0.5459, 0.9507, 0.9511, 0.1752, 0.3751, 0.4836,\n",
      "          0.0095, 0.6496, 0.5072, 0.9443, 0.1053, 0.4889, 0.5366, 0.5429,\n",
      "          0.5399, 0.7186, 0.3534, 0.5626, 0.9440, 0.3255, 0.5618, 0.9386,\n",
      "          0.5472, 0.7866, 0.7920, 0.6622, 0.5835, 0.4829, 0.9038, 0.4355,\n",
      "          0.2760, 0.1117, 0.5473, 0.4645, 0.4486, 0.3905, 0.7960, 0.8021,\n",
      "          0.5210, 0.6099, 0.7549, 0.2648, 0.9286, 0.9073, 0.6739, 0.2812,\n",
      "          0.3265, 0.7751, 0.9610, 0.8852, 0.5173, 0.9205, 0.0796, 0.8643,\n",
      "          0.9760, 0.6171, 0.6703, 0.4089, 0.7907, 0.9793, 0.4718, 0.5677,\n",
      "          0.6722, 0.3112, 0.1144, 0.1169, 0.4668, 0.8001, 0.3210, 0.7077,\n",
      "          0.1503, 0.5764, 0.2289, 0.3166, 0.7495, 0.6100, 0.7609, 0.0358,\n",
      "          0.8788, 0.2974, 0.8188, 0.2989, 0.0258, 0.7259, 0.1283, 0.0437,\n",
      "          0.5054, 0.8281, 0.6970, 0.1117, 0.4282, 0.5494, 0.7293, 0.4143,\n",
      "          0.6183, 0.6932, 0.6984, 0.1401, 0.4681, 0.0428, 0.5245, 0.0109,\n",
      "          0.4197, 0.0837, 0.4118, 0.3921, 0.6347, 0.1862, 0.4111, 0.1912]],\n",
      "\n",
      "        [[0.4177, 0.3563, 0.3172, 0.1467, 0.6723, 0.6324, 0.5854, 0.9566,\n",
      "          0.7526, 0.8653, 0.9852, 0.7554, 0.4810, 0.4877, 0.0681, 0.8572,\n",
      "          0.6457, 0.1340, 0.1394, 0.0072, 0.7221, 0.2947, 0.5094, 0.6084,\n",
      "          0.6251, 0.9196, 0.7328, 0.8572, 0.2893, 0.9077, 0.1020, 0.4496,\n",
      "          0.5394, 0.1269, 0.1315, 0.7277, 0.8441, 0.9937, 0.5664, 0.3357,\n",
      "          0.3029, 0.1774, 0.1591, 0.3367, 0.1998, 0.8422, 0.1241, 0.3069,\n",
      "          0.9039, 0.4640, 0.2311, 0.0381, 0.6925, 0.7719, 0.4858, 0.1661,\n",
      "          0.0332, 0.5320, 0.6988, 0.1566, 0.9380, 0.4231, 0.5885, 0.1702,\n",
      "          0.3327, 0.2340, 0.1424, 0.0844, 0.1513, 0.8665, 0.9416, 0.1903,\n",
      "          0.4887, 0.5925, 0.6763, 0.8016, 0.4293, 0.5645, 0.8226, 0.2114,\n",
      "          0.9096, 0.8283, 0.0452, 0.7566, 0.1700, 0.1073, 0.6318, 0.8279,\n",
      "          0.4227, 0.4784, 0.2125, 0.7803, 0.6418, 0.9841, 0.3002, 0.5248,\n",
      "          0.4159, 0.4268, 0.1004, 0.4039, 0.0509, 0.2993, 0.8831, 0.4105,\n",
      "          0.0026, 0.3720, 0.6522, 0.8565, 0.3006, 0.2928, 0.2090, 0.9001,\n",
      "          0.9995, 0.8334, 0.9280, 0.1986, 0.1831, 0.3665, 0.4678, 0.5042,\n",
      "          0.0255, 0.6745, 0.0626, 0.0375, 0.1288, 0.0329, 0.0605, 0.6480]],\n",
      "\n",
      "        [[0.8752, 0.2973, 0.5161, 0.1777, 0.1581, 0.2885, 0.1926, 0.9106,\n",
      "          0.3742, 0.1902, 0.5479, 0.3391, 0.4655, 0.9342, 0.9632, 0.6465,\n",
      "          0.0786, 0.3519, 0.1964, 0.6614, 0.5090, 0.4231, 0.8079, 0.2412,\n",
      "          0.3891, 0.6719, 0.1872, 0.4009, 0.2688, 0.0485, 0.9832, 0.1338,\n",
      "          0.8421, 0.8411, 0.7082, 0.7423, 0.6663, 0.5225, 0.4935, 0.6283,\n",
      "          0.2781, 0.8851, 0.6514, 0.2012, 0.1185, 0.8367, 0.9632, 0.8739,\n",
      "          0.9027, 0.9446, 0.7098, 0.1730, 0.4687, 0.4132, 0.7035, 0.5454,\n",
      "          0.9237, 0.2093, 0.8296, 0.1468, 0.4088, 0.2798, 0.1634, 0.4740,\n",
      "          0.7177, 0.4087, 0.6129, 0.6247, 0.8466, 0.1778, 0.8724, 0.1470,\n",
      "          0.6162, 0.3403, 0.8716, 0.6400, 0.2203, 0.6163, 0.4359, 0.4967,\n",
      "          0.8013, 0.3886, 0.5412, 0.4817, 0.3035, 0.5090, 0.7399, 0.3381,\n",
      "          0.5751, 0.6921, 0.1078, 0.7695, 0.4826, 0.0816, 0.5300, 0.4010,\n",
      "          0.5340, 0.3027, 0.8649, 0.5089, 0.8081, 0.6978, 0.8652, 0.5059,\n",
      "          0.0203, 0.2969, 0.2814, 0.4140, 0.5387, 0.1620, 0.4684, 0.5671,\n",
      "          0.5168, 0.4638, 0.7087, 0.6778, 0.1052, 0.5649, 0.2760, 0.4987,\n",
      "          0.3786, 0.5818, 0.3939, 0.2947, 0.6111, 0.3863, 0.2830, 0.8739]],\n",
      "\n",
      "        [[0.1623, 0.2978, 0.5715, 0.7167, 0.7611, 0.3193, 0.4173, 0.2910,\n",
      "          0.5826, 0.9073, 0.0912, 0.4850, 0.3525, 0.8638, 0.1002, 0.5846,\n",
      "          0.5037, 0.4103, 0.9765, 0.6785, 0.8368, 0.3125, 0.5420, 0.4394,\n",
      "          0.6567, 0.7544, 0.7351, 0.0710, 0.5610, 0.9261, 0.2664, 0.3887,\n",
      "          0.7898, 0.3622, 0.1778, 0.2839, 0.8711, 0.0207, 0.9966, 0.7966,\n",
      "          0.8547, 0.0599, 0.0466, 0.1142, 0.9388, 0.9600, 0.9930, 0.0840,\n",
      "          0.4501, 0.9935, 0.1645, 0.3148, 0.8056, 0.4466, 0.3903, 0.5936,\n",
      "          0.5100, 0.8169, 0.4346, 0.1099, 0.3256, 0.6395, 0.3091, 0.9531,\n",
      "          0.5441, 0.9593, 0.4353, 0.5403, 0.4560, 0.9159, 0.3207, 0.3595,\n",
      "          0.4323, 0.1804, 0.2465, 0.0276, 0.7006, 0.9288, 0.5169, 0.8680,\n",
      "          0.8409, 0.2534, 0.4621, 0.2558, 0.7481, 0.6981, 0.1018, 0.2699,\n",
      "          0.7016, 0.0711, 0.8179, 0.6126, 0.9989, 0.9778, 0.0784, 0.9517,\n",
      "          0.8693, 0.1001, 0.0178, 0.4541, 0.8805, 0.6793, 0.2161, 0.4003,\n",
      "          0.6487, 0.0426, 0.8425, 0.4975, 0.7775, 0.2855, 0.5707, 0.8256,\n",
      "          0.9116, 0.9660, 0.8989, 0.9712, 0.3411, 0.7296, 0.3786, 0.5895,\n",
      "          0.9543, 0.0149, 0.8157, 0.7415, 0.6228, 0.3738, 0.4831, 0.1066]]])\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "q tensor([[[ 1.1843e-01, -3.9818e-01,  4.1569e-01, -2.0531e-01, -1.0233e-01,\n",
      "           1.5265e-01, -5.8772e-02,  1.9883e-01, -7.6678e-02, -6.3702e-01,\n",
      "           1.7004e-01,  3.3039e-01, -8.5367e-01, -4.0842e-01,  7.7080e-01,\n",
      "           3.2439e-01,  1.7851e-02,  3.6391e-01,  4.8281e-01,  2.2427e-01,\n",
      "          -9.9034e-02, -8.7189e-02, -1.7244e-01, -3.6156e-01, -1.0704e-02,\n",
      "           1.5190e-01, -3.1566e-01, -4.5111e-01, -4.5297e-01,  7.7883e-02,\n",
      "           3.6589e-01, -5.5370e-02, -5.9324e-01,  5.2566e-02,  1.4715e-01,\n",
      "           8.5429e-01, -2.8444e-02,  4.9485e-02,  1.8552e-01, -8.3274e-01,\n",
      "          -1.7850e-01,  4.2434e-01, -2.1187e-01, -2.0434e-02,  5.5622e-02,\n",
      "          -7.9916e-02,  4.0158e-01, -2.1898e-01, -7.9048e-02, -5.7037e-01,\n",
      "          -2.1783e-01, -9.2889e-02, -1.8150e-01,  3.7291e-01, -3.0470e-01,\n",
      "           7.0609e-01,  1.0560e-01, -2.4513e-01, -1.0296e-01,  5.5302e-02,\n",
      "          -6.7402e-02, -8.7533e-01,  8.4875e-02,  2.3866e-02, -5.3747e-01,\n",
      "          -1.5083e-01, -6.5523e-03, -8.4686e-02, -3.1522e-01, -3.3012e-01,\n",
      "           4.6553e-01, -3.1538e-01,  2.3814e-01, -5.0027e-01,  4.4394e-01,\n",
      "           1.4071e-01,  1.2159e-01,  5.0898e-01, -3.6425e-01,  2.8435e-01,\n",
      "           2.9743e-02, -1.2216e-01,  5.0326e-01, -1.0862e-01, -2.6617e-01,\n",
      "          -3.7751e-01, -3.0977e-01,  5.5664e-01, -2.5476e-01,  4.0248e-01,\n",
      "          -3.6762e-01, -5.2734e-01, -3.2380e-01,  1.9359e-01, -6.6432e-02,\n",
      "          -4.4830e-01, -8.2773e-02,  2.4491e-01, -7.6294e-01,  2.6077e-01,\n",
      "          -1.8809e-01, -9.5196e-01, -3.2400e-01, -3.6526e-01,  7.6152e-01,\n",
      "          -7.6056e-01,  4.6120e-01, -4.0932e-01,  3.8435e-01,  1.0579e-01,\n",
      "           4.4708e-01, -3.1769e-01, -2.4886e-01, -1.8255e-01, -2.7892e-01,\n",
      "          -2.5295e-01, -7.3409e-01,  6.3331e-02,  6.1977e-02,  6.3562e-01,\n",
      "           1.4376e-02, -5.8249e-01,  6.9957e-02,  2.2352e-01,  2.1903e-01,\n",
      "          -3.9666e-02, -3.3042e-02,  4.1515e-01]],\n",
      "\n",
      "        [[ 2.2207e-01, -1.9311e-01,  4.0447e-01,  2.3337e-01, -8.8977e-02,\n",
      "          -1.2624e-01,  2.5929e-01,  1.9285e-01,  1.1213e-01, -9.5461e-01,\n",
      "           1.0050e-02,  2.5477e-01, -5.1075e-01, -5.1725e-01,  5.1497e-01,\n",
      "           5.0247e-01,  2.6655e-02,  1.9570e-01,  4.3495e-01,  7.3012e-02,\n",
      "           2.1691e-02, -6.8974e-02, -7.0613e-01, -5.5321e-01,  6.5998e-02,\n",
      "           2.2548e-01, -4.9303e-01, -5.1692e-01, -4.3134e-01,  1.5691e-01,\n",
      "           9.6362e-02, -3.2479e-01, -7.3786e-01,  1.1895e-01,  1.6057e-01,\n",
      "           3.7358e-01, -1.2491e-01,  3.5245e-01, -4.2183e-01, -4.7952e-01,\n",
      "          -2.7629e-01,  6.0974e-01, -9.5873e-02, -9.6537e-02, -1.8786e-02,\n",
      "          -2.7366e-01,  1.7021e-01, -4.7045e-01, -4.7488e-01, -9.9876e-02,\n",
      "          -3.4692e-02, -1.3018e-01, -1.0084e-01,  2.2793e-01, -5.7201e-01,\n",
      "           5.5589e-01,  4.1482e-01, -3.5490e-01,  7.9501e-02,  9.5217e-02,\n",
      "          -2.9965e-01, -5.7028e-01,  1.9845e-01, -2.6177e-01, -3.3039e-01,\n",
      "          -1.0942e-02, -2.6931e-01,  3.6819e-01, -6.8905e-01, -2.4208e-01,\n",
      "           7.8280e-01, -3.5401e-01,  1.5698e-02,  1.6132e-01,  4.7070e-01,\n",
      "           5.7215e-01, -1.0687e-01, -1.8879e-02, -4.1899e-01, -3.2542e-01,\n",
      "           2.3536e-01, -2.1158e-03,  7.6243e-01, -3.6431e-01, -3.9532e-01,\n",
      "          -5.0723e-01, -5.5588e-01,  6.2692e-01, -3.5382e-01,  1.0749e-01,\n",
      "           5.5468e-01, -4.1711e-01, -2.0002e-01,  1.6631e-01, -1.4820e-01,\n",
      "          -4.9490e-01,  3.7110e-02,  1.4699e-01, -5.2470e-01,  1.2548e-01,\n",
      "          -5.4567e-01, -5.2769e-01,  2.8911e-02, -3.2583e-01,  4.8140e-01,\n",
      "          -6.4353e-01,  3.9663e-01, -3.9098e-01, -1.9137e-01, -1.2746e-01,\n",
      "           6.7171e-01, -4.2846e-01,  3.9378e-01, -5.4546e-02, -1.5557e-01,\n",
      "          -5.6481e-02, -9.0000e-01,  2.1335e-01,  1.1332e-01,  4.8498e-01,\n",
      "          -1.6856e-01, -5.0029e-01,  2.8819e-01,  3.3592e-01, -8.4474e-02,\n",
      "          -1.2874e-02, -8.9112e-02,  6.0488e-01]],\n",
      "\n",
      "        [[ 2.1789e-01, -7.4561e-01,  5.0267e-01,  5.3086e-02,  2.7601e-01,\n",
      "           4.9467e-01, -4.5173e-02,  2.0503e-01, -6.2863e-02, -1.2053e-01,\n",
      "           1.7450e-01,  7.1690e-02, -4.0545e-01, -3.2449e-01,  3.8634e-01,\n",
      "           1.6891e-01,  1.4762e-01,  2.0036e-01,  2.6215e-01,  1.9306e-01,\n",
      "          -1.5785e-01, -2.7236e-01, -9.1096e-01, -5.8043e-01, -1.3696e-01,\n",
      "           1.5722e-01, -2.2641e-01, -7.2613e-01, -3.2960e-01,  8.0819e-03,\n",
      "           4.9519e-01, -5.4921e-01, -1.9569e-01, -3.4489e-01,  1.3207e-01,\n",
      "           4.1473e-01, -8.4350e-02,  7.5058e-01, -2.2866e-01, -7.7248e-01,\n",
      "          -4.8621e-01,  3.3874e-01, -3.4608e-02, -7.1090e-02, -9.2925e-02,\n",
      "          -3.8567e-01,  3.3544e-02, -3.0231e-01, -5.3756e-01, -4.3385e-01,\n",
      "          -3.8476e-01, -3.7431e-01, -5.6732e-01,  1.6568e-01, -3.3052e-01,\n",
      "           3.4266e-01,  2.9301e-01, -4.7955e-01,  2.7268e-01,  1.6589e-01,\n",
      "           5.8321e-02, -6.5126e-01,  9.0917e-02, -1.6736e-01,  5.6821e-02,\n",
      "          -2.1668e-01,  2.8364e-01,  1.4160e-01, -3.5406e-01, -2.9216e-01,\n",
      "           5.9797e-01, -2.8649e-01, -2.2959e-02, -4.4215e-01,  6.5437e-01,\n",
      "          -8.5311e-02, -1.0011e-01,  2.6521e-01, -5.3131e-01, -8.4704e-02,\n",
      "           1.2857e-01, -5.3383e-01,  6.4335e-01, -8.1452e-02, -8.8892e-02,\n",
      "          -5.0807e-01, -4.1269e-01,  3.8057e-01, -2.4180e-01,  3.0723e-01,\n",
      "           4.8350e-01, -7.0558e-01, -8.9586e-02,  1.5747e-02,  4.0828e-01,\n",
      "          -5.8579e-01,  1.6315e-01,  6.0509e-01, -1.0229e+00,  4.5592e-02,\n",
      "          -1.6836e-01, -4.6198e-01,  1.8577e-01, -4.7077e-01,  4.7725e-01,\n",
      "          -8.9280e-02,  6.4257e-01, -6.4601e-01,  4.7424e-01,  1.2915e-01,\n",
      "           3.7904e-01, -7.7660e-01, -8.6254e-02, -1.3401e-01, -2.9470e-01,\n",
      "          -2.4461e-01, -4.1181e-01,  4.1917e-01,  4.7132e-01,  8.6433e-01,\n",
      "           1.2695e-01, -4.4156e-01,  7.6209e-02,  3.9729e-01,  4.5535e-01,\n",
      "          -4.5672e-01, -2.9421e-02,  5.5886e-01]],\n",
      "\n",
      "        [[-5.5702e-02, -2.7329e-01,  3.8824e-01, -9.1954e-02,  1.5807e-02,\n",
      "           6.4839e-01,  4.0406e-01,  1.0987e-01, -7.8951e-04, -6.8826e-01,\n",
      "           2.4547e-01, -7.0767e-02, -7.1158e-01, -7.7394e-02,  5.6954e-01,\n",
      "           3.0819e-01,  2.9598e-01,  2.4816e-01,  6.3009e-01, -1.1358e-01,\n",
      "           2.4918e-01, -8.2306e-01, -6.8774e-01, -8.3370e-01,  2.1669e-01,\n",
      "           2.1114e-01, -5.3513e-01, -4.0539e-01, -4.9519e-01,  4.5041e-01,\n",
      "           1.2799e-01, -5.2009e-01, -5.9673e-01,  1.4094e-02, -5.4036e-02,\n",
      "           8.4110e-01,  9.0076e-02,  4.1844e-01, -6.1595e-01, -6.9679e-01,\n",
      "          -1.2736e-01,  1.4022e-01, -2.0554e-01,  4.5267e-01, -1.8356e-02,\n",
      "          -2.9859e-01,  3.9877e-01, -1.3730e-01, -5.0295e-01, -6.3426e-01,\n",
      "          -7.1388e-01, -2.4156e-02, -3.2313e-01,  2.7298e-01, -2.5893e-01,\n",
      "           8.8806e-01, -2.2887e-02, -2.6466e-01,  1.7061e-02,  4.0527e-01,\n",
      "          -1.4048e-01, -1.0314e+00, -1.9192e-01, -2.6252e-01, -6.7729e-01,\n",
      "           7.5534e-02, -2.8527e-01,  6.5966e-02, -3.9093e-01, -8.3660e-01,\n",
      "           6.6475e-01, -4.6106e-01, -2.0552e-01, -1.3084e-01,  5.0606e-01,\n",
      "           9.6092e-02, -4.0061e-01, -2.1025e-01, -2.3460e-01, -7.3124e-02,\n",
      "           1.1838e-01, -3.5003e-01,  7.9432e-01,  2.0539e-02, -3.0460e-01,\n",
      "          -4.3635e-01, -4.1757e-01,  7.0443e-01,  4.9660e-02,  3.0420e-01,\n",
      "           8.3967e-02, -7.9892e-01, -3.7945e-01,  4.6617e-01,  1.3510e-01,\n",
      "          -2.2207e-01, -2.8056e-01,  4.6278e-02, -6.8339e-01,  1.3730e-01,\n",
      "          -1.3682e-01, -8.7883e-01,  2.3647e-01, -3.3403e-01,  5.8273e-01,\n",
      "          -7.9366e-02,  4.3023e-01, -5.7072e-01, -1.1819e-01, -2.8256e-01,\n",
      "           3.5378e-01, -3.4120e-01,  1.4178e-01, -4.2339e-01,  6.3153e-02,\n",
      "          -3.8420e-01, -6.3900e-01, -9.9935e-02, -5.5197e-02,  7.2453e-01,\n",
      "          -8.6662e-02, -4.9237e-01,  6.0229e-02,  6.7372e-01,  4.6776e-01,\n",
      "          -2.4795e-01, -1.9839e-01,  3.6715e-01]],\n",
      "\n",
      "        [[ 5.8811e-01, -1.2181e-01,  1.2993e-01,  5.6257e-02, -3.6431e-02,\n",
      "           2.3783e-01, -1.2467e-01,  4.5206e-01,  7.5449e-02,  2.1964e-02,\n",
      "           1.8018e-01,  6.6340e-02, -5.4055e-01, -2.2723e-01,  2.4021e-01,\n",
      "          -3.6593e-01,  2.9743e-02,  3.7301e-01,  4.3242e-01, -2.5065e-01,\n",
      "           1.6601e-01, -1.3188e-01, -6.4736e-01, -5.6372e-01,  1.6992e-01,\n",
      "           1.8901e-01, -1.4440e-01, -2.1874e-01, -2.7475e-01,  1.6079e-01,\n",
      "          -1.1420e-01, -1.7366e-01, -6.8130e-01,  1.7845e-01, -6.0164e-02,\n",
      "           3.0309e-01,  2.2853e-01,  4.6471e-01, -7.1670e-01, -6.3853e-01,\n",
      "          -2.0976e-01,  5.7792e-01, -2.6017e-01,  6.4442e-02,  1.7231e-01,\n",
      "          -3.2250e-01,  3.8432e-01, -5.0225e-01, -4.1574e-01, -2.7490e-01,\n",
      "          -4.6027e-01, -1.7480e-01, -1.1067e-01,  6.7256e-02, -4.0793e-01,\n",
      "           7.3450e-01, -6.9020e-02, -1.4687e-02, -1.1999e-01,  7.4557e-01,\n",
      "           8.0486e-02, -7.2668e-01,  2.3408e-01, -2.9081e-01, -4.1655e-01,\n",
      "           5.9560e-02, -6.3123e-02,  3.0769e-01, -3.3265e-01, -3.1578e-01,\n",
      "           5.3755e-01, -2.3962e-01, -1.9238e-01,  4.4294e-02,  4.9405e-01,\n",
      "           3.0418e-01,  7.1786e-02,  1.1483e-01, -4.3647e-01, -3.2360e-02,\n",
      "           2.4983e-01,  2.0406e-01,  9.6425e-01,  2.7847e-02, -2.1744e-02,\n",
      "          -2.4751e-01, -5.2469e-01,  1.0412e+00, -3.7251e-01,  3.1215e-01,\n",
      "           6.1127e-01, -4.2187e-02, -4.8084e-01,  3.0699e-01,  8.1066e-02,\n",
      "          -5.2283e-01,  3.3347e-01,  5.8960e-01, -5.3883e-01,  3.9163e-01,\n",
      "          -4.6065e-01, -3.5535e-01, -6.5171e-02, -3.4684e-02,  5.5545e-01,\n",
      "          -1.7977e-01,  3.5454e-01, -4.6541e-01,  3.2793e-02,  1.0791e-02,\n",
      "           3.6113e-01, -7.6715e-01,  2.0236e-01, -1.8284e-01,  1.8690e-02,\n",
      "          -2.9328e-01, -6.2141e-01,  1.3884e-02,  3.4665e-01,  3.9099e-01,\n",
      "          -1.7617e-01, -4.8308e-01,  2.4721e-01,  3.6662e-01,  6.1184e-01,\n",
      "          -7.5776e-02,  1.7363e-01,  8.3614e-01]]], grad_fn=<SplitBackward>)\n",
      "here2\n",
      "here!\n",
      "q tensor([[[ 1.0468e-02, -3.5195e-02,  3.6742e-02, -1.8147e-02, -9.0447e-03,\n",
      "           1.3493e-02, -5.1947e-03,  1.7574e-02, -6.7774e-03, -5.6305e-02,\n",
      "           1.5030e-02,  2.9202e-02, -7.5455e-02, -3.6100e-02,  6.8130e-02,\n",
      "           2.8672e-02,  1.5778e-03,  3.2165e-02,  4.2675e-02,  1.9823e-02,\n",
      "          -8.7534e-03, -7.7065e-03, -1.5242e-02, -3.1957e-02, -9.4607e-04,\n",
      "           1.3426e-02, -2.7900e-02, -3.9872e-02, -4.0037e-02,  6.8839e-03,\n",
      "           3.2340e-02, -4.8940e-03, -5.2435e-02,  4.6462e-03,  1.3006e-02,\n",
      "           7.5510e-02, -2.5141e-03,  4.3739e-03,  1.6398e-02, -7.3605e-02,\n",
      "          -1.5777e-02,  3.7507e-02, -1.8727e-02, -1.8061e-03,  4.9163e-03,\n",
      "          -7.0636e-03,  3.5495e-02, -1.9356e-02, -6.9869e-03, -5.0414e-02,\n",
      "          -1.9254e-02, -8.2103e-03, -1.6043e-02,  3.2961e-02, -2.6932e-02,\n",
      "           6.2410e-02,  9.3338e-03, -2.1667e-02, -9.1005e-03,  4.8881e-03,\n",
      "          -5.9576e-03, -7.7369e-02,  7.5020e-03,  2.1095e-03, -4.7506e-02,\n",
      "          -1.3331e-02, -5.7915e-04, -7.4853e-03, -2.7862e-02, -2.9179e-02,\n",
      "           4.1147e-02, -2.7876e-02,  2.1049e-02, -4.4218e-02,  3.9239e-02,\n",
      "           1.2437e-02,  1.0747e-02,  4.4988e-02, -3.2195e-02,  2.5133e-02,\n",
      "           2.6290e-03, -1.0798e-02,  4.4482e-02, -9.6005e-03, -2.3526e-02,\n",
      "          -3.3367e-02, -2.7380e-02,  4.9200e-02, -2.2518e-02,  3.5574e-02,\n",
      "          -3.2493e-02, -4.6611e-02, -2.8620e-02,  1.7111e-02, -5.8718e-03,\n",
      "          -3.9625e-02, -7.3162e-03,  2.1647e-02, -6.7435e-02,  2.3049e-02,\n",
      "          -1.6625e-02, -8.4142e-02, -2.8638e-02, -3.2284e-02,  6.7310e-02,\n",
      "          -6.7225e-02,  4.0765e-02, -3.6179e-02,  3.3972e-02,  9.3509e-03,\n",
      "           3.9517e-02, -2.8080e-02, -2.1997e-02, -1.6135e-02, -2.4654e-02,\n",
      "          -2.2358e-02, -6.4885e-02,  5.5977e-03,  5.4781e-03,  5.6181e-02,\n",
      "           1.2707e-03, -5.1485e-02,  6.1834e-03,  1.9756e-02,  1.9360e-02,\n",
      "          -3.5060e-03, -2.9206e-03,  3.6695e-02]],\n",
      "\n",
      "        [[ 1.9628e-02, -1.7069e-02,  3.5751e-02,  2.0628e-02, -7.8646e-03,\n",
      "          -1.1158e-02,  2.2918e-02,  1.7046e-02,  9.9112e-03, -8.4377e-02,\n",
      "           8.8835e-04,  2.2518e-02, -4.5144e-02, -4.5719e-02,  4.5518e-02,\n",
      "           4.4413e-02,  2.3560e-03,  1.7298e-02,  3.8444e-02,  6.4535e-03,\n",
      "           1.9172e-03, -6.0965e-03, -6.2413e-02, -4.8897e-02,  5.8335e-03,\n",
      "           1.9930e-02, -4.3578e-02, -4.5689e-02, -3.8126e-02,  1.3869e-02,\n",
      "           8.5173e-03, -2.8707e-02, -6.5218e-02,  1.0513e-02,  1.4192e-02,\n",
      "           3.3020e-02, -1.1040e-02,  3.1153e-02, -3.7285e-02, -4.2384e-02,\n",
      "          -2.4421e-02,  5.3894e-02, -8.4740e-03, -8.5328e-03, -1.6605e-03,\n",
      "          -2.4189e-02,  1.5045e-02, -4.1583e-02, -4.1974e-02, -8.8279e-03,\n",
      "          -3.0664e-03, -1.1506e-02, -8.9129e-03,  2.0147e-02, -5.0559e-02,\n",
      "           4.9135e-02,  3.6665e-02, -3.1369e-02,  7.0269e-03,  8.4161e-03,\n",
      "          -2.6486e-02, -5.0406e-02,  1.7540e-02, -2.3138e-02, -2.9203e-02,\n",
      "          -9.6718e-04, -2.3804e-02,  3.2543e-02, -6.0904e-02, -2.1397e-02,\n",
      "           6.9191e-02, -3.1291e-02,  1.3875e-03,  1.4259e-02,  4.1604e-02,\n",
      "           5.0572e-02, -9.4459e-03, -1.6687e-03, -3.7034e-02, -2.8764e-02,\n",
      "           2.0803e-02, -1.8701e-04,  6.7390e-02, -3.2201e-02, -3.4942e-02,\n",
      "          -4.4833e-02, -4.9133e-02,  5.5412e-02, -3.1274e-02,  9.5012e-03,\n",
      "           4.9027e-02, -3.6868e-02, -1.7680e-02,  1.4700e-02, -1.3099e-02,\n",
      "          -4.3743e-02,  3.2801e-03,  1.2992e-02, -4.6377e-02,  1.1091e-02,\n",
      "          -4.8231e-02, -4.6642e-02,  2.5554e-03, -2.8800e-02,  4.2550e-02,\n",
      "          -5.6880e-02,  3.5058e-02, -3.4558e-02, -1.6915e-02, -1.1266e-02,\n",
      "           5.9371e-02, -3.7871e-02,  3.4806e-02, -4.8212e-03, -1.3751e-02,\n",
      "          -4.9922e-03, -7.9550e-02,  1.8857e-02,  1.0016e-02,  4.2866e-02,\n",
      "          -1.4899e-02, -4.4219e-02,  2.5473e-02,  2.9692e-02, -7.4665e-03,\n",
      "          -1.1379e-03, -7.8765e-03,  5.3464e-02]],\n",
      "\n",
      "        [[ 1.9259e-02, -6.5903e-02,  4.4430e-02,  4.6922e-03,  2.4396e-02,\n",
      "           4.3723e-02, -3.9927e-03,  1.8122e-02, -5.5564e-03, -1.0654e-02,\n",
      "           1.5424e-02,  6.3366e-03, -3.5837e-02, -2.8682e-02,  3.4148e-02,\n",
      "           1.4929e-02,  1.3048e-02,  1.7710e-02,  2.3171e-02,  1.7065e-02,\n",
      "          -1.3952e-02, -2.4073e-02, -8.0518e-02, -5.1303e-02, -1.2106e-02,\n",
      "           1.3897e-02, -2.0012e-02, -6.4181e-02, -2.9133e-02,  7.1435e-04,\n",
      "           4.3769e-02, -4.8544e-02, -1.7297e-02, -3.0484e-02,  1.1673e-02,\n",
      "           3.6657e-02, -7.4556e-03,  6.6343e-02, -2.0211e-02, -6.8279e-02,\n",
      "          -4.2975e-02,  2.9941e-02, -3.0589e-03, -6.2836e-03, -8.2135e-03,\n",
      "          -3.4089e-02,  2.9649e-03, -2.6721e-02, -4.7514e-02, -3.8347e-02,\n",
      "          -3.4008e-02, -3.3084e-02, -5.0145e-02,  1.4644e-02, -2.9214e-02,\n",
      "           3.0287e-02,  2.5899e-02, -4.2386e-02,  2.4102e-02,  1.4662e-02,\n",
      "           5.1549e-03, -5.7563e-02,  8.0360e-03, -1.4793e-02,  5.0223e-03,\n",
      "          -1.9152e-02,  2.5070e-02,  1.2516e-02, -3.1295e-02, -2.5823e-02,\n",
      "           5.2854e-02, -2.5322e-02, -2.0293e-03, -3.9081e-02,  5.7839e-02,\n",
      "          -7.5405e-03, -8.8483e-03,  2.3441e-02, -4.6962e-02, -7.4868e-03,\n",
      "           1.1364e-02, -4.7184e-02,  5.6864e-02, -7.1994e-03, -7.8570e-03,\n",
      "          -4.4908e-02, -3.6477e-02,  3.3638e-02, -2.1373e-02,  2.7155e-02,\n",
      "           4.2735e-02, -6.2365e-02, -7.9184e-03,  1.3918e-03,  3.6087e-02,\n",
      "          -5.1777e-02,  1.4421e-02,  5.3483e-02, -9.0415e-02,  4.0298e-03,\n",
      "          -1.4881e-02, -4.0834e-02,  1.6420e-02, -4.1610e-02,  4.2184e-02,\n",
      "          -7.8913e-03,  5.6795e-02, -5.7100e-02,  4.1917e-02,  1.1415e-02,\n",
      "           3.3502e-02, -6.8643e-02, -7.6238e-03, -1.1845e-02, -2.6048e-02,\n",
      "          -2.1620e-02, -3.6399e-02,  3.7049e-02,  4.1659e-02,  7.6397e-02,\n",
      "           1.1221e-02, -3.9029e-02,  6.7360e-03,  3.5116e-02,  4.0247e-02,\n",
      "          -4.0369e-02, -2.6004e-03,  4.9397e-02]],\n",
      "\n",
      "        [[-4.9234e-03, -2.4156e-02,  3.4316e-02, -8.1277e-03,  1.3972e-03,\n",
      "           5.7310e-02,  3.5714e-02,  9.7112e-03, -6.9783e-05, -6.0834e-02,\n",
      "           2.1696e-02, -6.2550e-03, -6.2896e-02, -6.8407e-03,  5.0341e-02,\n",
      "           2.7240e-02,  2.6161e-02,  2.1935e-02,  5.5692e-02, -1.0039e-02,\n",
      "           2.2024e-02, -7.2749e-02, -6.0788e-02, -7.3689e-02,  1.9152e-02,\n",
      "           1.8662e-02, -4.7299e-02, -3.5832e-02, -4.3769e-02,  3.9811e-02,\n",
      "           1.1313e-02, -4.5970e-02, -5.2744e-02,  1.2458e-03, -4.7761e-03,\n",
      "           7.4343e-02,  7.9617e-03,  3.6986e-02, -5.4443e-02, -6.1588e-02,\n",
      "          -1.1257e-02,  1.2394e-02, -1.8167e-02,  4.0011e-02, -1.6225e-03,\n",
      "          -2.6392e-02,  3.5247e-02, -1.2136e-02, -4.4455e-02, -5.6061e-02,\n",
      "          -6.3098e-02, -2.1351e-03, -2.8561e-02,  2.4128e-02, -2.2887e-02,\n",
      "           7.8494e-02, -2.0230e-03, -2.3393e-02,  1.5080e-03,  3.5821e-02,\n",
      "          -1.2417e-02, -9.1166e-02, -1.6963e-02, -2.3204e-02, -5.9864e-02,\n",
      "           6.6763e-03, -2.5214e-02,  5.8306e-03, -3.4554e-02, -7.3946e-02,\n",
      "           5.8756e-02, -4.0752e-02, -1.8166e-02, -1.1565e-02,  4.4730e-02,\n",
      "           8.4934e-03, -3.5409e-02, -1.8583e-02, -2.0736e-02, -6.4633e-03,\n",
      "           1.0463e-02, -3.0938e-02,  7.0208e-02,  1.8154e-03, -2.6923e-02,\n",
      "          -3.8568e-02, -3.6909e-02,  6.2264e-02,  4.3894e-03,  2.6887e-02,\n",
      "           7.4217e-03, -7.0615e-02, -3.3539e-02,  4.1204e-02,  1.1941e-02,\n",
      "          -1.9628e-02, -2.4799e-02,  4.0905e-03, -6.0404e-02,  1.2136e-02,\n",
      "          -1.2093e-02, -7.7678e-02,  2.0901e-02, -2.9524e-02,  5.1507e-02,\n",
      "          -7.0150e-03,  3.8027e-02, -5.0445e-02, -1.0447e-02, -2.4975e-02,\n",
      "           3.1270e-02, -3.0158e-02,  1.2532e-02, -3.7423e-02,  5.5820e-03,\n",
      "          -3.3959e-02, -5.6480e-02, -8.8331e-03, -4.8788e-03,  6.4040e-02,\n",
      "          -7.6599e-03, -4.3520e-02,  5.3236e-03,  5.9549e-02,  4.1345e-02,\n",
      "          -2.1916e-02, -1.7535e-02,  3.2451e-02]],\n",
      "\n",
      "        [[ 5.1982e-02, -1.0767e-02,  1.1484e-02,  4.9725e-03, -3.2201e-03,\n",
      "           2.1022e-02, -1.1019e-02,  3.9957e-02,  6.6688e-03,  1.9414e-03,\n",
      "           1.5925e-02,  5.8637e-03, -4.7778e-02, -2.0085e-02,  2.1232e-02,\n",
      "          -3.2344e-02,  2.6289e-03,  3.2970e-02,  3.8221e-02, -2.2154e-02,\n",
      "           1.4673e-02, -1.1657e-02, -5.7219e-02, -4.9826e-02,  1.5019e-02,\n",
      "           1.6706e-02, -1.2763e-02, -1.9334e-02, -2.4285e-02,  1.4212e-02,\n",
      "          -1.0094e-02, -1.5350e-02, -6.0219e-02,  1.5773e-02, -5.3178e-03,\n",
      "           2.6790e-02,  2.0200e-02,  4.1075e-02, -6.3348e-02, -5.6439e-02,\n",
      "          -1.8541e-02,  5.1081e-02, -2.2996e-02,  5.6960e-03,  1.5230e-02,\n",
      "          -2.8506e-02,  3.3970e-02, -4.4393e-02, -3.6747e-02, -2.4298e-02,\n",
      "          -4.0682e-02, -1.5450e-02, -9.7822e-03,  5.9447e-03, -3.6056e-02,\n",
      "           6.4921e-02, -6.1006e-03, -1.2982e-03, -1.0605e-02,  6.5899e-02,\n",
      "           7.1140e-03, -6.4230e-02,  2.0690e-02, -2.5705e-02, -3.6818e-02,\n",
      "           5.2644e-03, -5.5794e-03,  2.7197e-02, -2.9402e-02, -2.7912e-02,\n",
      "           4.7513e-02, -2.1179e-02, -1.7004e-02,  3.9151e-03,  4.3668e-02,\n",
      "           2.6886e-02,  6.3450e-03,  1.0150e-02, -3.8579e-02, -2.8603e-03,\n",
      "           2.2082e-02,  1.8037e-02,  8.5228e-02,  2.4614e-03, -1.9219e-03,\n",
      "          -2.1877e-02, -4.6377e-02,  9.2030e-02, -3.2926e-02,  2.7590e-02,\n",
      "           5.4029e-02, -3.7288e-03, -4.2501e-02,  2.7135e-02,  7.1653e-03,\n",
      "          -4.6212e-02,  2.9475e-02,  5.2114e-02, -4.7627e-02,  3.4615e-02,\n",
      "          -4.0716e-02, -3.1409e-02, -5.7604e-03, -3.0657e-03,  4.9095e-02,\n",
      "          -1.5889e-02,  3.1337e-02, -4.1137e-02,  2.8985e-03,  9.5384e-04,\n",
      "           3.1919e-02, -6.7807e-02,  1.7886e-02, -1.6161e-02,  1.6520e-03,\n",
      "          -2.5923e-02, -5.4925e-02,  1.2272e-03,  3.0640e-02,  3.4559e-02,\n",
      "          -1.5571e-02, -4.2699e-02,  2.1851e-02,  3.2405e-02,  5.4080e-02,\n",
      "          -6.6977e-03,  1.5346e-02,  7.3905e-02]]], grad_fn=<MulBackward0>)\n",
      "k tensor([[[ 2.4518e-03, -2.2683e-01, -4.2591e-01,  4.1110e-02,  3.9727e-01,\n",
      "           2.2517e-02, -2.3253e-01,  3.1365e-01, -5.2984e-01, -5.2526e-01,\n",
      "           2.7118e-01,  1.1388e+00,  3.6493e-01, -5.6850e-02,  4.4871e-02,\n",
      "          -2.8613e-01, -3.6383e-01,  6.2057e-01,  5.0556e-01,  9.7367e-01,\n",
      "          -9.2170e-01,  4.4976e-01, -1.3176e-01,  9.0106e-02,  2.9767e-01,\n",
      "           8.7584e-01,  3.7367e-01,  3.6906e-01,  3.6737e-01,  1.5598e-01,\n",
      "          -1.4870e-01, -5.9212e-02, -4.9703e-03,  5.7609e-02,  5.4572e-01,\n",
      "           1.8585e-01,  3.5315e-01, -4.4317e-01, -3.7672e-01, -6.8979e-02,\n",
      "          -3.8191e-02, -1.5054e-01,  1.1017e-01, -8.6237e-02,  3.4386e-01,\n",
      "          -3.0249e-01,  1.2851e-01, -5.2941e-01,  5.4840e-01, -2.7171e-01,\n",
      "          -2.8701e-01,  5.6486e-02,  2.9438e-01, -8.7356e-01,  3.0332e-01,\n",
      "           9.8079e-02, -2.7021e-02,  1.9230e-01, -4.8376e-01,  7.9935e-02,\n",
      "           1.4032e-01,  2.8683e-01,  1.7389e-02, -3.7527e-01, -1.6093e-02,\n",
      "          -4.4121e-01, -1.2934e-01, -6.4680e-01,  3.7013e-01, -8.8712e-01,\n",
      "           2.8593e-01,  5.4564e-01, -9.5366e-02,  6.5893e-01,  2.0684e-02,\n",
      "          -2.2307e-01, -5.3632e-01, -1.9643e-01, -2.8752e-01,  7.9247e-01,\n",
      "          -1.4627e-01,  1.9299e-01,  4.5711e-01, -7.2978e-01,  1.6926e-01,\n",
      "          -4.1697e-02, -4.3698e-01, -4.3746e-01,  6.1208e-01,  2.5199e-01,\n",
      "          -2.9412e-01,  5.7940e-01, -1.3173e-01, -8.2324e-01, -1.5684e-02,\n",
      "          -9.2181e-04, -1.3332e-01, -4.4382e-01, -1.5820e-01,  1.0263e+00,\n",
      "          -3.2930e-01, -2.4098e-01, -1.4620e-01, -1.9455e-02,  2.7977e-01,\n",
      "           3.6095e-01,  9.6103e-01, -1.1879e+00,  2.7409e-01, -3.3798e-01,\n",
      "          -3.8564e-01, -1.2014e-02, -1.0132e+00, -6.3376e-01,  2.1334e-01,\n",
      "           4.8116e-02,  4.2655e-01,  7.9909e-01, -1.3567e-01, -4.6671e-01,\n",
      "          -6.8953e-01,  4.0475e-01,  1.1071e-01,  1.6680e-01,  5.0364e-01,\n",
      "          -4.4851e-01, -1.9322e-02, -4.7329e-02]],\n",
      "\n",
      "        [[-4.3933e-02, -3.4665e-01, -7.2394e-01,  1.7966e-01,  7.3024e-01,\n",
      "           2.0782e-01,  1.0446e-01,  3.0097e-01, -5.2884e-01, -6.8195e-01,\n",
      "           1.8472e-01,  7.9846e-01,  3.2688e-01, -5.0276e-02,  4.2404e-02,\n",
      "           1.0553e-01, -5.4421e-02, -3.9748e-02,  6.8275e-01,  8.8595e-01,\n",
      "          -7.4039e-01,  1.1054e-01,  3.0225e-02,  1.6470e-01,  4.7816e-01,\n",
      "           7.5804e-01,  2.8852e-01, -2.0038e-01,  4.4225e-01, -1.3655e-01,\n",
      "           5.9501e-01, -3.9474e-01, -4.2445e-01,  2.8438e-01,  3.8863e-01,\n",
      "          -9.4719e-02,  9.4857e-02, -4.2358e-01,  2.7030e-01,  2.1749e-01,\n",
      "          -2.5045e-01, -3.2598e-01,  4.6047e-01,  4.8626e-02,  3.9460e-01,\n",
      "          -3.5216e-01,  4.0482e-01, -6.3773e-01,  6.4649e-01, -3.5217e-01,\n",
      "          -2.6403e-01, -1.8407e-01,  5.8690e-01, -6.6107e-01,  4.0115e-01,\n",
      "          -2.8718e-03,  4.2565e-01,  3.1271e-01, -3.3884e-01, -4.1234e-02,\n",
      "           2.0103e-01,  5.2970e-01, -2.7070e-01, -6.0070e-01, -9.8864e-02,\n",
      "          -5.3059e-01, -2.4517e-01, -6.7466e-01,  1.1321e-01, -5.5379e-01,\n",
      "          -2.5938e-01,  5.0675e-01, -1.6533e-02,  3.6313e-01,  5.7330e-02,\n",
      "          -4.8572e-01, -4.6591e-01, -1.3676e-01,  3.3210e-01,  4.9505e-01,\n",
      "          -6.1313e-02,  3.0787e-01, -1.8211e-01, -4.9869e-01, -1.2975e-01,\n",
      "          -3.9633e-01,  1.9385e-01, -1.0479e-01,  5.5841e-01, -1.0437e-01,\n",
      "          -5.1116e-01,  4.0494e-01, -2.2935e-01, -1.1613e+00, -1.6977e-02,\n",
      "          -7.7099e-01, -2.3772e-01, -8.0231e-02,  1.2609e-01,  7.1074e-01,\n",
      "          -5.0702e-01, -8.0616e-02,  4.1149e-01, -4.4804e-02, -1.2741e-02,\n",
      "           3.2993e-01,  1.3926e-01, -4.9467e-01, -2.0265e-01, -6.8824e-02,\n",
      "          -4.7411e-01, -3.2920e-01, -6.5311e-01, -2.5590e-01, -1.9617e-01,\n",
      "          -8.8626e-03,  7.9689e-01,  6.1648e-01,  3.8150e-01, -6.2232e-01,\n",
      "          -7.7061e-01,  4.4319e-01, -6.0595e-02,  2.1969e-01,  3.3869e-01,\n",
      "          -2.8156e-01, -1.1519e-01, -3.9292e-01]],\n",
      "\n",
      "        [[ 4.6780e-01, -4.0893e-01, -3.8959e-01,  2.5077e-01,  7.4534e-01,\n",
      "           5.0202e-01,  2.4507e-02,  3.3517e-03, -7.0287e-01, -5.2856e-01,\n",
      "           2.5349e-01,  5.9541e-01,  1.6618e-01,  1.3165e-02, -8.9242e-02,\n",
      "           2.4599e-01, -1.3031e-01,  2.4279e-01,  1.0727e+00,  8.0585e-01,\n",
      "          -3.8386e-01,  4.9870e-01,  2.4935e-01, -2.3961e-02,  3.5198e-01,\n",
      "           4.2467e-01,  1.3391e-01, -1.4616e-01,  4.1125e-01,  1.6300e-01,\n",
      "          -1.0615e-01,  3.6362e-02,  3.4773e-01,  2.1777e-02,  2.1720e-01,\n",
      "           3.6199e-01,  3.6237e-01, -6.5035e-01, -2.7213e-01,  3.0735e-01,\n",
      "           6.9067e-02, -5.9473e-02,  2.4932e-01,  1.8477e-01,  1.4019e-01,\n",
      "          -2.1464e-01, -3.6185e-02, -2.5183e-01,  4.9498e-01, -4.2593e-01,\n",
      "          -6.7192e-01,  2.6227e-01,  4.1324e-01, -3.2045e-01,  5.2708e-01,\n",
      "          -4.3703e-01,  1.1737e-03, -3.9859e-02, -7.5750e-02,  2.0608e-01,\n",
      "          -8.7563e-02, -4.5388e-02,  3.6611e-02, -3.3522e-01, -3.4515e-02,\n",
      "          -5.3486e-01, -7.3195e-01, -7.3022e-01,  5.8868e-02, -9.3408e-01,\n",
      "          -1.5796e-01,  3.3996e-01, -9.0502e-02,  7.5091e-01, -1.4542e-01,\n",
      "          -5.2267e-02, -4.5427e-01, -3.1686e-01,  3.0295e-01,  2.2166e-01,\n",
      "           2.4049e-02,  9.7722e-02,  2.5145e-01, -6.5529e-01,  3.1186e-02,\n",
      "          -5.7551e-01, -5.6761e-01, -2.1350e-01,  2.1517e-01, -3.4076e-01,\n",
      "          -4.1402e-01,  3.4954e-01,  1.6944e-01, -1.2123e+00,  5.2894e-01,\n",
      "          -3.2775e-01,  1.2381e-01, -1.1285e-01,  6.5960e-02,  7.2695e-01,\n",
      "          -2.7768e-01, -3.7653e-01,  3.7276e-01,  1.4680e-01, -1.6303e-01,\n",
      "           4.1476e-01, -2.6120e-01, -7.4440e-01,  1.8629e-01, -4.7442e-02,\n",
      "          -4.4159e-01, -3.2639e-01, -8.2745e-01, -4.3672e-01, -1.7024e-01,\n",
      "          -1.7639e-03,  8.6027e-01,  6.6471e-01,  1.2802e-01, -1.0372e+00,\n",
      "          -4.6133e-01,  6.4657e-01, -2.8746e-01,  4.4255e-01,  1.7515e-01,\n",
      "          -4.9681e-01,  9.2680e-02, -5.0564e-01]],\n",
      "\n",
      "        [[ 2.3953e-01, -4.6201e-01, -4.0418e-01,  4.8254e-01,  4.0668e-01,\n",
      "           2.5727e-01,  4.2187e-02,  1.3320e-01, -6.8175e-01, -7.4105e-01,\n",
      "           1.3030e-02,  1.0637e+00,  4.6529e-01, -8.0318e-02,  1.2662e-01,\n",
      "          -7.4476e-02,  3.7388e-01,  5.7303e-01,  3.7285e-01,  9.3795e-01,\n",
      "          -9.0249e-01,  1.3896e-01, -3.3469e-01,  1.1537e-01,  1.2023e-01,\n",
      "           8.5269e-01,  2.2513e-01,  2.6093e-01,  9.2326e-03, -1.9915e-02,\n",
      "           1.4761e-01, -1.5857e-01, -1.1019e-01, -2.1199e-01,  2.7520e-01,\n",
      "           2.0256e-01,  2.3039e-01, -2.7323e-01, -2.1023e-01,  1.1525e-02,\n",
      "           2.1868e-01, -1.2364e-01,  4.3952e-01,  1.6170e-01,  5.8982e-01,\n",
      "          -2.1625e-01,  3.5400e-01, -4.6890e-01,  1.6200e-01, -1.8164e-01,\n",
      "          -3.8676e-01, -1.8297e-01,  5.7646e-01, -6.4873e-01,  1.3118e-01,\n",
      "           3.7098e-02, -2.2074e-02,  2.9811e-01, -2.1326e-01,  2.0412e-01,\n",
      "          -4.6326e-02,  6.5330e-02,  9.6170e-03, -3.7167e-01, -2.0094e-01,\n",
      "          -5.8324e-01, -6.1580e-01, -4.8307e-01,  2.8388e-01, -8.2905e-01,\n",
      "          -1.0772e-01,  1.2359e-01, -2.7049e-01,  7.8943e-01, -2.0559e-01,\n",
      "          -9.9844e-02, -6.6245e-01,  5.2938e-02, -1.3596e-01,  7.1941e-01,\n",
      "          -1.8040e-01,  1.0435e-01,  2.2055e-02, -7.7861e-01, -1.4074e-01,\n",
      "           1.2083e-01, -2.7485e-01, -2.8742e-01,  4.4161e-01, -3.6365e-02,\n",
      "          -1.6555e-01,  2.8884e-01, -2.9185e-01, -1.1168e+00, -9.7646e-02,\n",
      "          -8.9188e-01, -2.1672e-01, -1.4942e-01, -1.1464e-01,  6.2569e-01,\n",
      "          -7.6280e-02, -9.1241e-02, -8.8138e-02,  3.0781e-01,  1.6348e-01,\n",
      "           7.7840e-02,  4.4323e-01, -5.1145e-01,  9.4825e-02, -1.8261e-02,\n",
      "           6.4562e-02, -5.4969e-02, -6.8448e-01, -4.3107e-01, -2.5980e-02,\n",
      "          -3.9785e-01,  9.8494e-01,  7.6778e-01,  2.6523e-01, -6.9075e-01,\n",
      "          -7.7803e-01,  3.6156e-01,  3.5373e-02,  2.2823e-01,  6.6040e-01,\n",
      "          -7.5406e-01, -3.4648e-01, -1.4842e-01]],\n",
      "\n",
      "        [[-2.0095e-01, -9.4649e-02, -3.7031e-01, -1.9258e-02,  5.7653e-01,\n",
      "           4.8470e-01, -2.1292e-01,  4.3707e-01, -5.1619e-01, -3.0919e-01,\n",
      "           5.7903e-01,  6.6093e-01,  3.7223e-01,  5.7648e-02, -2.3366e-01,\n",
      "           4.2220e-01, -4.5467e-01,  1.1576e-01,  7.7192e-01,  9.0801e-01,\n",
      "          -4.4383e-01,  1.2219e-01,  2.4043e-01,  1.2082e-01, -1.4207e-02,\n",
      "           5.3363e-01,  1.6059e-01, -4.2185e-01,  3.5087e-01, -3.1432e-03,\n",
      "           4.0471e-01, -1.8443e-01,  2.8805e-01, -7.5023e-02,  6.4845e-01,\n",
      "           1.4916e-01,  5.4153e-01, -7.8425e-01, -1.8974e-02, -1.5032e-01,\n",
      "           6.7848e-01, -1.2140e-01,  6.9586e-01, -4.9698e-01,  6.2125e-01,\n",
      "          -5.2095e-01,  1.9029e-01, -5.8424e-01,  4.7626e-01, -3.7770e-01,\n",
      "          -3.8119e-01, -1.0007e-01,  2.3170e-01, -7.4664e-01,  8.0831e-01,\n",
      "          -2.3965e-02,  5.1695e-01,  5.3172e-01, -5.1387e-02,  4.4950e-01,\n",
      "           3.0213e-02,  1.6164e-01, -1.9337e-01, -4.7375e-01, -1.1944e-01,\n",
      "          -1.7043e-01, -4.2662e-01, -9.5514e-01,  1.5374e-01, -1.3526e+00,\n",
      "          -2.1074e-01,  5.4312e-01, -3.8879e-04,  9.3288e-01,  1.3377e-01,\n",
      "          -1.4439e-01, -5.5265e-01, -1.7961e-01, -5.8197e-02,  6.1446e-01,\n",
      "          -2.2276e-01,  2.6306e-01,  4.3161e-02, -6.8279e-01,  1.0907e-01,\n",
      "           1.9319e-01, -1.0269e-01, -3.8816e-02,  3.2324e-01, -1.7302e-01,\n",
      "          -3.5055e-01,  3.3470e-01, -4.6919e-02, -1.0603e+00,  1.2280e-01,\n",
      "          -3.8635e-01, -2.0318e-01, -3.4200e-01,  2.4148e-02,  1.2249e+00,\n",
      "          -4.5401e-02, -1.7206e-01,  6.6802e-01, -3.3911e-01, -5.0713e-01,\n",
      "           5.4462e-02,  8.0369e-01, -8.3553e-01, -1.0630e-01, -3.0561e-01,\n",
      "          -4.7039e-01, -1.8680e-01, -7.3795e-01, -4.9063e-01, -2.6364e-01,\n",
      "          -1.3196e-01,  6.1139e-01,  7.7999e-01,  1.2106e-01, -6.3449e-01,\n",
      "          -1.0669e+00,  3.4998e-01,  1.4549e-01,  6.7703e-01,  4.7293e-01,\n",
      "          -5.6296e-01,  2.7367e-02, -1.4324e-01]]], grad_fn=<SplitBackward>)\n",
      "v tensor([[[ 2.4296e-01, -5.1301e-01, -5.7445e-02,  1.7433e-01,  2.0271e-01,\n",
      "           4.3155e-02,  1.4369e-01, -1.3488e-01,  5.9930e-01,  3.3574e-01,\n",
      "          -2.7930e-01,  1.4336e-01,  4.2440e-01, -2.3545e-01, -6.9950e-02,\n",
      "           6.0297e-01,  4.9236e-01, -2.4682e-01, -1.0627e-01, -4.0029e-01,\n",
      "           6.2851e-01, -4.9257e-01,  1.6583e-01,  3.1637e-01, -1.3322e-01,\n",
      "          -2.6810e-01,  2.7158e-01, -6.2388e-04, -2.7511e-01, -4.5706e-01,\n",
      "           1.0736e-01,  9.4287e-03,  8.6529e-02,  3.4849e-01,  4.1205e-01,\n",
      "           5.8909e-01,  2.2429e-01, -2.2126e-01,  2.2276e-01, -8.0583e-02,\n",
      "          -1.5074e-01,  6.2432e-01,  3.2073e-01,  8.5893e-01, -5.9613e-01,\n",
      "          -5.9738e-01,  4.9036e-01, -1.7592e-01,  1.6566e-01, -2.3863e-04,\n",
      "          -2.4394e-01, -1.7514e-01,  5.6860e-01, -2.1546e-02, -3.9261e-01,\n",
      "          -1.5047e-01,  2.7609e-01, -1.0081e-01,  1.7639e-01, -5.4300e-01,\n",
      "          -6.9506e-01,  1.3921e-01, -7.2961e-02,  4.3348e-01, -4.3593e-01,\n",
      "           2.7930e-01,  3.3974e-01, -1.1745e-01, -3.3849e-02,  5.5084e-02,\n",
      "          -7.1983e-01, -9.1629e-01, -2.2100e-01, -5.0950e-01, -1.5222e-01,\n",
      "           2.0530e-03,  1.1212e-01,  4.0234e-01, -5.0742e-01,  4.1803e-01,\n",
      "          -2.6860e-01,  3.9484e-01, -6.5496e-01,  3.5954e-01,  3.5152e-01,\n",
      "          -4.9157e-01, -4.3979e-01,  7.9993e-02,  3.1694e-01,  8.9517e-02,\n",
      "          -8.7593e-02, -1.0051e-01,  7.9824e-01, -1.8706e-01, -5.7094e-01,\n",
      "          -3.0989e-01, -5.0160e-01,  3.7110e-05, -2.6713e-01, -3.5645e-01,\n",
      "          -3.4259e-01,  2.9068e-01,  2.6368e-01,  4.3375e-01, -2.0629e-01,\n",
      "           1.1496e+00, -5.8236e-01, -4.2024e-01,  3.1955e-01,  1.2587e-01,\n",
      "           3.5529e-01, -4.1154e-01, -1.8227e-01,  1.6125e-01, -1.8686e-02,\n",
      "           4.2856e-01,  8.4909e-02,  3.3414e-01, -8.2787e-02, -3.1949e-01,\n",
      "          -7.4406e-01,  4.5198e-01, -4.3296e-01, -6.5667e-01,  1.3979e-01,\n",
      "           5.2416e-01, -2.8515e-01, -6.6133e-02]],\n",
      "\n",
      "        [[ 3.8062e-01, -8.2530e-01, -1.6405e-01,  6.7825e-01,  2.0879e-01,\n",
      "           9.3672e-02, -6.2348e-02, -1.5725e-01,  5.4303e-01,  1.1478e-01,\n",
      "          -7.5543e-01,  1.3276e-01, -5.6251e-02, -4.1623e-01,  3.2087e-02,\n",
      "           7.6124e-01,  2.8972e-01, -6.0106e-01, -1.8584e-01, -3.5122e-01,\n",
      "           1.4120e-01, -3.8947e-01,  1.0000e-01, -3.5886e-03, -1.9175e-01,\n",
      "          -7.3039e-02, -2.6405e-01,  3.8842e-01, -1.4769e-01, -3.8568e-01,\n",
      "          -5.5956e-03, -1.8202e-01,  3.0405e-01,  3.1296e-01,  3.0561e-01,\n",
      "           7.8461e-01,  9.3876e-02,  2.8174e-01,  5.0253e-02, -1.0116e-01,\n",
      "           3.8804e-01,  9.7345e-03,  1.8307e-01,  1.0508e+00, -4.2908e-01,\n",
      "          -6.2043e-01, -5.0845e-02, -2.4691e-01,  8.8077e-02,  3.7385e-02,\n",
      "          -3.6178e-02, -1.6243e-01,  5.2970e-01,  1.6826e-01,  7.5876e-02,\n",
      "          -7.8352e-02,  5.4946e-01, -9.7277e-03,  3.8366e-02, -7.2077e-01,\n",
      "          -4.1961e-01,  9.5006e-02, -5.8136e-02,  2.7727e-01, -5.7724e-01,\n",
      "           7.5664e-01,  3.6367e-01, -2.4819e-01, -6.6980e-02,  1.0443e-01,\n",
      "          -4.1457e-01, -8.9518e-01, -2.1521e-01, -1.4528e-01, -2.2668e-01,\n",
      "           3.3371e-01, -5.7983e-02,  1.1139e-01, -7.8537e-01,  6.4534e-01,\n",
      "          -5.4864e-01, -8.4600e-02, -8.8381e-01,  3.6861e-01, -8.4095e-02,\n",
      "          -5.5762e-01, -3.8856e-01,  2.5756e-01,  2.0407e-01,  8.7351e-02,\n",
      "          -1.0115e-01, -4.1143e-01,  8.9811e-01, -6.5694e-01, -8.2218e-01,\n",
      "          -2.5162e-02, -1.8109e-01, -3.2776e-01,  3.6163e-01, -2.7767e-01,\n",
      "          -3.2904e-01, -2.6502e-01,  3.9810e-01, -2.1165e-01, -3.0491e-01,\n",
      "           1.2643e+00, -3.0948e-01, -3.4423e-01,  3.0787e-01,  7.7530e-03,\n",
      "           2.4061e-01, -3.0347e-01, -1.3190e-01,  2.9502e-01, -2.9476e-01,\n",
      "           1.3013e-01,  5.5360e-01,  1.3799e-01,  4.5629e-01, -1.4170e-01,\n",
      "          -9.9826e-01,  6.8999e-01, -4.3749e-01, -7.6082e-01, -2.3218e-01,\n",
      "           1.1056e-01, -3.0008e-01, -2.0442e-01]],\n",
      "\n",
      "        [[ 1.0856e-01, -4.3647e-01, -3.7555e-01,  5.4861e-01,  3.8372e-01,\n",
      "          -2.6523e-01,  1.5826e-02,  6.5127e-02,  2.4216e-01,  2.6232e-01,\n",
      "          -2.9676e-01,  3.3701e-01,  5.4460e-02, -1.7587e-01,  2.1357e-01,\n",
      "           4.5377e-01,  3.2558e-01, -5.0298e-01, -3.4290e-01, -2.6058e-01,\n",
      "           5.3106e-02, -3.5974e-01,  2.8721e-01, -4.2637e-01, -2.8147e-01,\n",
      "          -2.8503e-01,  5.3104e-01,  1.3123e-01, -2.1944e-01, -5.9900e-01,\n",
      "          -7.1225e-02,  7.3103e-03,  4.1311e-01,  3.6593e-01,  4.5060e-01,\n",
      "           8.4611e-01,  3.7436e-03, -2.0152e-01,  4.4354e-02,  1.0530e-01,\n",
      "          -1.0447e-01, -1.8492e-01,  4.0331e-01,  9.5476e-01, -3.9462e-01,\n",
      "          -7.3098e-01, -1.6041e-01, -9.2687e-02, -8.5939e-02,  3.6342e-01,\n",
      "          -2.4040e-01, -4.6727e-01,  1.7051e-01, -1.3806e-01,  3.2784e-01,\n",
      "          -1.4561e-01, -8.0681e-02, -3.8093e-02, -1.3690e-03, -2.5187e-01,\n",
      "          -5.9351e-01,  1.7647e-01, -2.4122e-01,  3.0874e-01, -4.1908e-01,\n",
      "           6.0961e-01,  2.8531e-01,  8.1326e-02,  1.8651e-01, -3.4265e-01,\n",
      "          -7.8764e-01, -4.4095e-01, -1.4387e-01, -9.9796e-03, -6.5810e-01,\n",
      "           2.6479e-01,  1.8592e-02,  1.5770e-01, -6.9948e-01,  3.7104e-01,\n",
      "          -4.4118e-01, -1.5498e-01, -3.7653e-01,  4.3191e-01,  2.6865e-01,\n",
      "          -5.3847e-01, -5.7238e-01,  9.6250e-02,  3.7360e-01,  3.8330e-01,\n",
      "          -2.2325e-01,  3.0200e-02,  4.0257e-01, -2.9002e-01, -6.5591e-01,\n",
      "           4.5732e-02, -6.3011e-01, -2.2527e-01,  4.8350e-01, -3.8227e-01,\n",
      "          -1.2024e-01,  8.4301e-02, -1.5576e-01, -1.5699e-01, -1.9323e-01,\n",
      "           5.7661e-01, -3.9422e-01,  8.5288e-04,  5.4367e-01, -1.4868e-01,\n",
      "          -1.1992e-01, -8.6484e-02, -2.3480e-01,  6.9237e-01, -2.5061e-01,\n",
      "           3.7496e-01,  1.2718e-01,  1.6758e-01,  2.6665e-01, -9.3172e-02,\n",
      "          -1.0470e+00,  1.3756e-01,  5.5048e-03, -7.6269e-01, -1.6496e-01,\n",
      "           1.4641e-01, -1.5804e-01, -5.8320e-02]],\n",
      "\n",
      "        [[ 3.6722e-01, -6.6848e-01, -1.2310e-01,  5.4792e-01,  6.7847e-01,\n",
      "          -5.2661e-02, -1.3652e-01,  2.9460e-01,  4.4716e-01,  1.5983e-01,\n",
      "          -3.1854e-01, -5.6347e-02,  2.2991e-02, -5.2051e-02,  3.7278e-01,\n",
      "           7.9209e-01,  2.6723e-01, -6.0766e-01, -4.9658e-01, -4.0064e-01,\n",
      "           3.6910e-01, -6.6516e-01, -3.2095e-01,  3.1053e-01, -1.8624e-01,\n",
      "          -3.7913e-01,  1.5845e-01,  1.3986e-01, -1.0880e-01, -6.8445e-01,\n",
      "          -2.2284e-01, -2.8516e-01,  2.5633e-01,  2.3369e-01,  4.7740e-01,\n",
      "           3.1768e-01,  4.7933e-01, -4.0883e-01, -4.2207e-02, -1.1998e-01,\n",
      "          -6.1413e-02,  2.7154e-01,  2.9672e-01,  6.7955e-01, -5.6821e-01,\n",
      "          -7.6218e-01,  1.0810e-01, -3.1945e-01,  1.1937e-01,  3.7551e-01,\n",
      "          -7.0535e-02, -2.7942e-01,  7.1487e-01,  3.2540e-01, -4.0379e-01,\n",
      "          -3.1363e-01,  3.6910e-01,  5.6163e-02,  5.1705e-02, -4.2767e-01,\n",
      "          -5.2843e-01,  1.8715e-01, -4.1295e-01,  4.5298e-01, -4.3935e-01,\n",
      "           7.7952e-01,  5.3379e-02, -4.5353e-01, -1.8624e-01, -5.2831e-03,\n",
      "          -7.6378e-01, -6.0488e-01, -3.7934e-02, -1.0358e-01, -5.2632e-01,\n",
      "           1.3769e-01,  5.4200e-02,  3.6088e-01, -3.2483e-01,  4.5408e-01,\n",
      "          -3.5639e-01, -3.0107e-02, -7.9910e-01, -5.6907e-02,  1.3355e-01,\n",
      "          -2.2993e-01, -6.0258e-01, -3.4161e-01,  1.6751e-01,  2.1727e-01,\n",
      "          -2.1206e-01,  9.8710e-03,  9.5295e-01,  1.3943e-01, -8.6161e-01,\n",
      "          -3.7576e-01, -2.0159e-01,  7.1545e-02,  2.2997e-01, -6.4060e-02,\n",
      "          -3.3521e-01,  1.9517e-01,  3.3199e-01,  1.4555e-02,  6.6706e-02,\n",
      "           1.0989e+00, -5.4308e-01, -5.3115e-01,  3.8804e-01, -9.6087e-03,\n",
      "           3.1830e-01, -2.8191e-01, -1.9687e-01,  1.5639e-01,  5.9029e-02,\n",
      "           2.4386e-01,  5.1135e-01,  2.3665e-01, -6.2608e-03, -2.5569e-01,\n",
      "          -1.2862e+00,  5.6503e-01, -1.6145e-01, -8.4986e-01,  3.2235e-02,\n",
      "          -6.2117e-02, -2.8745e-01, -2.3288e-01]],\n",
      "\n",
      "        [[ 4.4740e-01, -1.0338e+00, -9.4229e-02,  7.4558e-01,  3.7710e-01,\n",
      "           2.0738e-01, -7.0788e-02, -1.2744e-02,  2.3915e-01,  3.1965e-01,\n",
      "           3.2879e-02,  1.0670e-01,  4.4175e-02, -5.2713e-02, -2.2962e-01,\n",
      "           6.5545e-01,  2.7439e-01, -3.3494e-01, -1.9954e-01, -3.4160e-01,\n",
      "           4.7329e-01, -1.1405e-01,  1.2775e-01, -4.9231e-01, -2.9982e-01,\n",
      "          -5.9769e-01, -1.7167e-01,  4.1334e-01,  2.8408e-02, -6.2954e-01,\n",
      "          -7.8878e-01, -2.4437e-01,  1.9261e-01,  5.1351e-01,  5.7207e-01,\n",
      "           1.2324e+00,  5.5276e-01, -3.1600e-01, -3.8417e-02,  3.5872e-01,\n",
      "          -3.0502e-01, -4.0304e-01,  6.5809e-01,  1.0001e+00, -5.6752e-01,\n",
      "          -1.1003e+00, -3.8761e-02, -9.5386e-02,  1.8850e-01,  2.4271e-01,\n",
      "          -1.8020e-01, -3.5673e-01,  4.6093e-01,  3.0903e-01, -2.6431e-02,\n",
      "          -3.9609e-01,  6.1346e-01,  1.7187e-01,  2.1020e-01, -7.2131e-01,\n",
      "          -5.8947e-01,  3.5558e-01, -3.8866e-01,  3.7733e-01, -3.7499e-01,\n",
      "           5.5668e-01,  3.6931e-01, -2.5479e-01, -6.0616e-02,  1.9189e-01,\n",
      "          -2.9054e-01, -9.1832e-01, -4.8309e-01,  1.6641e-01, -4.1358e-01,\n",
      "           3.5655e-01, -9.8252e-02,  6.9599e-01, -6.3530e-01,  6.7753e-01,\n",
      "          -1.3357e-01, -1.2430e-01, -8.6036e-01,  8.8160e-01, -1.1603e-01,\n",
      "          -6.1390e-02, -2.6781e-01,  1.5490e-01,  4.7111e-01,  1.5880e-01,\n",
      "          -6.7940e-02, -2.7959e-01,  6.0422e-01, -5.7823e-01, -4.5506e-01,\n",
      "          -3.9082e-01, -4.0704e-01, -5.0302e-01,  7.6593e-02, -5.5621e-01,\n",
      "          -6.1549e-01,  2.6488e-01,  2.9823e-02,  1.4246e-02, -1.8515e-01,\n",
      "           8.9900e-01, -5.8079e-01, -1.5458e-01,  9.0702e-01, -1.8314e-01,\n",
      "          -1.3822e-01,  7.1986e-02, -4.7939e-01,  3.4747e-01,  3.0742e-01,\n",
      "          -6.3926e-02,  8.2997e-01,  8.5535e-02,  1.3354e-01, -5.6940e-03,\n",
      "          -1.3433e+00,  3.3123e-01, -5.4767e-01, -7.8575e-01, -3.6999e-01,\n",
      "           1.3510e-01, -6.6557e-01, -1.1282e-01]]], grad_fn=<SplitBackward>)\n",
      "presoftmax tensor([[[ 0.1301, -0.0687, -0.0985,  0.1185,  0.0090],\n",
      "         [-0.0384, -0.1829, -0.1790,  0.0415, -0.1277],\n",
      "         [ 0.0604, -0.0921, -0.0756,  0.1221,  0.0139],\n",
      "         [ 0.0666, -0.1451, -0.0140,  0.1294,  0.0568],\n",
      "         [ 0.1160, -0.1834, -0.1188,  0.1049, -0.0468]]],\n",
      "       grad_fn=<BmmBackward0>)\n",
      "post softmax tensor([[[0.2227, 0.1826, 0.1772, 0.2202, 0.1973],\n",
      "         [0.2113, 0.1829, 0.1836, 0.2289, 0.1933],\n",
      "         [0.2105, 0.1808, 0.1838, 0.2239, 0.2010],\n",
      "         [0.2089, 0.1690, 0.1927, 0.2225, 0.2069],\n",
      "         [0.2288, 0.1696, 0.1809, 0.2263, 0.1944]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[ 0.3120, -0.6934, -0.1550,  0.5276,  0.3751,  0.0090, -0.0206,\n",
      "           0.0151,  0.4212,  0.2405, -0.3164,  0.1245,  0.1077, -0.1815,\n",
      "           0.0649,  0.6574,  0.3332, -0.4537, -0.2671, -0.3551,  0.3498,\n",
      "          -0.4135,  0.0606, -0.0345, -0.2147, -0.3250,  0.1074,  0.2064,\n",
      "          -0.1455, -0.5533, -0.1944, -0.1408,  0.2424,  0.3524,  0.4454,\n",
      "           0.7375,  0.2824, -0.1859,  0.0498,  0.0266, -0.0549,  0.0883,\n",
      "           0.3715,  0.8993, -0.5181, -0.7608,  0.0877, -0.1898,  0.1012,\n",
      "           0.2017, -0.1546, -0.2834,  0.5019,  0.1341, -0.1096, -0.2208,\n",
      "           0.3498,  0.0153,  0.0989, -0.5337, -0.5693,  0.1910, -0.2372,\n",
      "           0.3761, -0.4475,  0.5898,  0.2773, -0.2072, -0.0397,  0.0073,\n",
      "          -0.6011, -0.7600, -0.2177, -0.1317, -0.3894,  0.2090,  0.0102,\n",
      "           0.3547, -0.5772,  0.5103, -0.3430,  0.0139, -0.7197,  0.3854,\n",
      "           0.1171, -0.3695, -0.4558,  0.0373,  0.3039,  0.1830, -0.1376,\n",
      "          -0.1451,  0.7421, -0.2964, -0.6730, -0.2254, -0.3811, -0.1833,\n",
      "           0.1579, -0.3217, -0.3529,  0.1265,  0.1828,  0.0362, -0.1577,\n",
      "           1.0084, -0.4902, -0.3037,  0.4881, -0.0351,  0.1446, -0.2103,\n",
      "          -0.2442,  0.3155, -0.0287,  0.2267,  0.4189,  0.1983,  0.1371,\n",
      "          -0.1710, -1.0818,  0.4408, -0.3189, -0.7625, -0.1064,  0.1759,\n",
      "          -0.3409, -0.1359],\n",
      "         [ 0.3114, -0.6923, -0.1575,  0.5311,  0.3797,  0.0056, -0.0231,\n",
      "           0.0197,  0.4190,  0.2385, -0.3182,  0.1242,  0.1032, -0.1803,\n",
      "           0.0713,  0.6580,  0.3310, -0.4583, -0.2716, -0.3544,  0.3444,\n",
      "          -0.4157,  0.0573, -0.0362, -0.2155, -0.3247,  0.1097,  0.2069,\n",
      "          -0.1449, -0.5555, -0.1949, -0.1425,  0.2456,  0.3508,  0.4455,\n",
      "           0.7343,  0.2818, -0.1869,  0.0473,  0.0257, -0.0531,  0.0840,\n",
      "           0.3704,  0.8978, -0.5167, -0.7611,  0.0821, -0.1909,  0.0991,\n",
      "           0.2064, -0.1533, -0.2854,  0.5011,  0.1351, -0.1064, -0.2212,\n",
      "           0.3471,  0.0160,  0.0965, -0.5301, -0.5675,  0.1908, -0.2400,\n",
      "           0.3756, -0.4477,  0.5954,  0.2743, -0.2083, -0.0395,  0.0037,\n",
      "          -0.6035, -0.7543, -0.2145, -0.1276, -0.3949,  0.2105,  0.0099,\n",
      "           0.3515, -0.5764,  0.5094, -0.3455,  0.0086, -0.7184,  0.3801,\n",
      "           0.1164, -0.3692, -0.4588,  0.0334,  0.3023,  0.1857, -0.1397,\n",
      "          -0.1427,  0.7418, -0.2928, -0.6766, -0.2232, -0.3796, -0.1821,\n",
      "           0.1659, -0.3185, -0.3503,  0.1243,  0.1817,  0.0302, -0.1554,\n",
      "           1.0053, -0.4886, -0.3031,  0.4878, -0.0369,  0.1432, -0.2090,\n",
      "          -0.2435,  0.3181, -0.0309,  0.2267,  0.4200,  0.1973,  0.1393,\n",
      "          -0.1702, -1.0861,  0.4403, -0.3133, -0.7644, -0.1073,  0.1698,\n",
      "          -0.3386, -0.1372],\n",
      "         [ 0.3121, -0.6949, -0.1573,  0.5327,  0.3787,  0.0072, -0.0229,\n",
      "           0.0186,  0.4170,  0.2397, -0.3146,  0.1249,  0.1032, -0.1794,\n",
      "           0.0677,  0.6571,  0.3309, -0.4565, -0.2703, -0.3540,  0.3454,\n",
      "          -0.4121,  0.0596, -0.0418, -0.2164, -0.3271,  0.1080,  0.2086,\n",
      "          -0.1436, -0.5558, -0.1999, -0.1426,  0.2452,  0.3527,  0.4467,\n",
      "           0.7402,  0.2833, -0.1878,  0.0470,  0.0293, -0.0558,  0.0790,\n",
      "           0.3734,  0.8994, -0.5169, -0.7641,  0.0810, -0.1894,  0.0996,\n",
      "           0.2064, -0.1541, -0.2864,  0.4995,  0.1355, -0.1044, -0.2224,\n",
      "           0.3486,  0.0171,  0.0976, -0.5316, -0.5681,  0.1923, -0.2408,\n",
      "           0.3754, -0.4469,  0.5941,  0.2759, -0.2074, -0.0389,  0.0049,\n",
      "          -0.6007, -0.7558, -0.2175, -0.1251, -0.3950,  0.2119,  0.0089,\n",
      "           0.3545, -0.5778,  0.5107, -0.3435,  0.0076, -0.7187,  0.3861,\n",
      "           0.1148, -0.3671, -0.4568,  0.0357,  0.3045,  0.1856, -0.1389,\n",
      "          -0.1440,  0.7392, -0.2964, -0.6738, -0.2241, -0.3811, -0.1857,\n",
      "           0.1649, -0.3216, -0.3525,  0.1257,  0.1792,  0.0303, -0.1563,\n",
      "           1.0033, -0.4893, -0.3006,  0.4920, -0.0384,  0.1398, -0.2061,\n",
      "          -0.2458,  0.3194, -0.0283,  0.2244,  0.4226,  0.1963,  0.1395,\n",
      "          -0.1684, -1.0875,  0.4383, -0.3155, -0.7642, -0.1100,  0.1705,\n",
      "          -0.3415, -0.1364],\n",
      "         [ 0.3103, -0.6934, -0.1590,  0.5329,  0.3805,  0.0049, -0.0225,\n",
      "           0.0207,  0.4126,  0.2418, -0.3073,  0.1269,  0.1039, -0.1759,\n",
      "           0.0674,  0.6539,  0.3308, -0.4546, -0.2714, -0.3530,  0.3454,\n",
      "          -0.4096,  0.0619, -0.0494, -0.2179, -0.3313,  0.1142,  0.2074,\n",
      "          -0.1431, -0.5586, -0.2050, -0.1414,  0.2460,  0.3544,  0.4491,\n",
      "           0.7444,  0.2844, -0.1937,  0.0462,  0.0339, -0.0628,  0.0735,\n",
      "           0.3778,  0.8991, -0.5169, -0.7677,  0.0790, -0.1871,  0.0985,\n",
      "           0.2101, -0.1564, -0.2901,  0.4956,  0.1336, -0.1013, -0.2244,\n",
      "           0.3440,  0.0180,  0.0981, -0.5282, -0.5700,  0.1943, -0.2438,\n",
      "           0.3758, -0.4447,  0.5923,  0.2757, -0.2044, -0.0364,  0.0016,\n",
      "          -0.6023, -0.7523, -0.2187, -0.1215, -0.3996,  0.2123,  0.0089,\n",
      "           0.3575, -0.5773,  0.5091, -0.3408,  0.0059, -0.7145,  0.3904,\n",
      "           0.1167, -0.3646, -0.4573,  0.0349,  0.3074,  0.1885, -0.1397,\n",
      "          -0.1404,  0.7331, -0.2946, -0.6705, -0.2246, -0.3859, -0.1870,\n",
      "           0.1655, -0.3244, -0.3523,  0.1304,  0.1724,  0.0308, -0.1553,\n",
      "           0.9955, -0.4909, -0.2960,  0.4975, -0.0411,  0.1340, -0.2018,\n",
      "          -0.2486,  0.3237, -0.0253,  0.2248,  0.4213,  0.1958,  0.1375,\n",
      "          -0.1667, -1.0900,  0.4318, -0.3126, -0.7644, -0.1112,  0.1705,\n",
      "          -0.3424, -0.1348],\n",
      "         [ 0.3099, -0.6886, -0.1551,  0.5231,  0.3780,  0.0062, -0.0195,\n",
      "           0.0184,  0.4207,  0.2421, -0.3114,  0.1243,  0.1112, -0.1783,\n",
      "           0.0678,  0.6558,  0.3345, -0.4520, -0.2690, -0.3554,  0.3529,\n",
      "          -0.4165,  0.0591, -0.0308, -0.2144, -0.3273,  0.1159,  0.2015,\n",
      "          -0.1468, -0.5556, -0.1931, -0.1394,  0.2416,  0.3517,  0.4469,\n",
      "           0.7324,  0.2838, -0.1932,  0.0505,  0.0261, -0.0608,  0.0941,\n",
      "           0.3725,  0.8957, -0.5195, -0.7605,  0.0915, -0.1897,  0.1010,\n",
      "           0.2042, -0.1564, -0.2847,  0.5021,  0.1323, -0.1142, -0.2220,\n",
      "           0.3445,  0.0145,  0.0992, -0.5290, -0.5717,  0.1914, -0.2392,\n",
      "           0.3779, -0.4458,  0.5871,  0.2749, -0.2064, -0.0393,  0.0044,\n",
      "          -0.6068, -0.7566, -0.2156, -0.1341, -0.3918,  0.2054,  0.0123,\n",
      "           0.3564, -0.5729,  0.5067, -0.3409,  0.0170, -0.7159,  0.3814,\n",
      "           0.1224, -0.3684, -0.4585,  0.0322,  0.3042,  0.1847, -0.1388,\n",
      "          -0.1394,  0.7409, -0.2876, -0.6722, -0.2279, -0.3842, -0.1779,\n",
      "           0.1546, -0.3204, -0.3515,  0.1325,  0.1806,  0.0410, -0.1548,\n",
      "           1.0052, -0.4928, -0.3046,  0.4878, -0.0346,  0.1455, -0.2111,\n",
      "          -0.2443,  0.3151, -0.0265,  0.2307,  0.4134,  0.2003,  0.1312,\n",
      "          -0.1729, -1.0812,  0.4376, -0.3153, -0.7623, -0.1019,  0.1774,\n",
      "          -0.3392, -0.1350]]], grad_fn=<BmmBackward0>)\n",
      "postlinear tensor([[[-1.3660e-01,  6.6087e-02, -5.0627e-02,  1.2639e-01,  1.9532e-01,\n",
      "          -3.2134e-01, -2.7343e-01,  2.0153e-03, -3.2442e-01,  5.6851e-04,\n",
      "          -3.4001e-02,  9.2845e-02, -1.6106e-01, -1.3433e-01, -1.0303e-01,\n",
      "           2.3235e-01, -3.2212e-01,  3.6679e-01,  2.6246e-01,  1.0902e-01,\n",
      "          -1.4785e-01, -5.7198e-02, -4.8061e-02, -1.2659e-01,  1.3820e-01,\n",
      "           1.3805e-01, -8.7764e-03, -1.4642e-01, -1.4199e-01,  6.4847e-02,\n",
      "           5.1739e-01,  1.7461e-01,  1.0978e-01,  1.6533e-01, -3.3223e-01,\n",
      "           1.0636e-02,  1.3526e-01, -3.4203e-01, -1.7641e-01, -7.0847e-02,\n",
      "          -2.3090e-01, -1.9169e-01,  2.2647e-02, -1.4761e-01, -1.7998e-01,\n",
      "           4.1073e-01,  2.0616e-01,  6.9799e-02,  2.3915e-01,  1.7935e-01,\n",
      "          -1.9842e-01,  1.4602e-01, -1.8208e-01,  4.8545e-03,  4.2007e-01,\n",
      "           1.6484e-01, -6.2551e-03,  2.0847e-01,  1.1782e-02, -2.5447e-01,\n",
      "           1.8329e-01,  1.4128e-01,  5.1748e-01,  1.0040e-02,  8.6098e-02,\n",
      "           3.5674e-01, -9.3306e-02,  1.3212e-01, -3.3554e-01,  2.6614e-03,\n",
      "          -2.3813e-01, -1.5223e-01, -2.4392e-01,  1.1212e-01, -6.3059e-01,\n",
      "          -1.0907e-01,  6.3677e-02,  6.5187e-02, -1.0750e-01,  1.5629e-01,\n",
      "          -7.9210e-03,  1.6473e-01, -1.7678e-01,  3.9184e-02,  1.6106e-02,\n",
      "           2.5946e-02,  9.4928e-02, -1.7164e-01,  4.8088e-01,  3.9728e-01,\n",
      "          -2.0153e-01, -1.5492e-01, -6.3419e-02, -5.0407e-03,  7.5270e-02,\n",
      "           5.0027e-01, -1.6491e-01, -4.5283e-01,  1.1642e-01,  6.8036e-03,\n",
      "          -2.7294e-01,  5.8738e-02, -4.3244e-01,  1.0356e-01, -6.6180e-02,\n",
      "           1.4505e-01, -6.0490e-02, -1.1998e-01,  4.5172e-02, -3.2491e-01,\n",
      "           1.2912e-01, -2.5200e-01,  8.0196e-02, -3.4363e-02,  3.5363e-01,\n",
      "           1.7464e-01,  9.6368e-02,  4.4882e-01,  4.4896e-01,  3.9716e-02,\n",
      "          -7.9687e-02,  1.5968e-01, -1.5125e-01, -2.8583e-01, -2.7855e-01,\n",
      "          -6.5042e-02,  5.7501e-02, -2.7408e-01]],\n",
      "\n",
      "        [[-1.3957e-01,  6.6965e-02, -4.9062e-02,  1.2663e-01,  1.9383e-01,\n",
      "          -3.1976e-01, -2.7460e-01,  2.8361e-03, -3.2307e-01, -1.7677e-03,\n",
      "          -3.4727e-02,  9.4332e-02, -1.6279e-01, -1.3512e-01, -1.0233e-01,\n",
      "           2.3515e-01, -3.2009e-01,  3.6787e-01,  2.6304e-01,  1.0677e-01,\n",
      "          -1.4825e-01, -5.9035e-02, -5.0387e-02, -1.2938e-01,  1.3860e-01,\n",
      "           1.4128e-01, -8.1266e-03, -1.4644e-01, -1.4313e-01,  6.7518e-02,\n",
      "           5.1856e-01,  1.7591e-01,  1.0845e-01,  1.6782e-01, -3.3105e-01,\n",
      "           1.2160e-02,  1.3615e-01, -3.4403e-01, -1.7559e-01, -7.1952e-02,\n",
      "          -2.3001e-01, -1.9023e-01,  2.1512e-02, -1.4836e-01, -1.7839e-01,\n",
      "           4.1123e-01,  2.0620e-01,  7.0413e-02,  2.3959e-01,  1.7986e-01,\n",
      "          -2.0009e-01,  1.4435e-01, -1.8221e-01,  7.1258e-03,  4.2253e-01,\n",
      "           1.6600e-01, -5.6152e-03,  2.1025e-01,  1.2055e-02, -2.5575e-01,\n",
      "           1.8344e-01,  1.4054e-01,  5.1505e-01,  9.3630e-03,  8.5386e-02,\n",
      "           3.5747e-01, -9.0333e-02,  1.2949e-01, -3.3394e-01,  5.8391e-04,\n",
      "          -2.3791e-01, -1.5024e-01, -2.4317e-01,  1.1114e-01, -6.3144e-01,\n",
      "          -1.1073e-01,  6.2529e-02,  6.5787e-02, -1.0553e-01,  1.5498e-01,\n",
      "          -9.3952e-03,  1.6370e-01, -1.7840e-01,  3.8986e-02,  1.5676e-02,\n",
      "           2.6398e-02,  9.4221e-02, -1.7369e-01,  4.8098e-01,  3.9786e-01,\n",
      "          -2.0063e-01, -1.5240e-01, -6.9423e-02, -5.0040e-03,  7.3465e-02,\n",
      "           5.0004e-01, -1.6405e-01, -4.5196e-01,  1.1571e-01,  7.9269e-03,\n",
      "          -2.7256e-01,  5.7143e-02, -4.3468e-01,  1.0625e-01, -6.9402e-02,\n",
      "           1.4658e-01, -6.0923e-02, -1.2005e-01,  4.3258e-02, -3.2335e-01,\n",
      "           1.3039e-01, -2.5391e-01,  7.7050e-02, -3.5212e-02,  3.5214e-01,\n",
      "           1.7216e-01,  9.7881e-02,  4.4720e-01,  4.4769e-01,  4.0235e-02,\n",
      "          -7.7293e-02,  1.5816e-01, -1.5312e-01, -2.8693e-01, -2.7811e-01,\n",
      "          -6.3207e-02,  6.1003e-02, -2.7202e-01]],\n",
      "\n",
      "        [[-1.3806e-01,  6.8081e-02, -4.9596e-02,  1.2769e-01,  1.9527e-01,\n",
      "          -3.1916e-01, -2.7339e-01,  4.9000e-03, -3.2420e-01,  1.5972e-03,\n",
      "          -3.4120e-02,  9.3736e-02, -1.6265e-01, -1.3629e-01, -1.0141e-01,\n",
      "           2.3725e-01, -3.2072e-01,  3.6754e-01,  2.6351e-01,  1.0713e-01,\n",
      "          -1.4680e-01, -5.9598e-02, -5.0439e-02, -1.2972e-01,  1.3820e-01,\n",
      "           1.4298e-01, -6.6650e-03, -1.4488e-01, -1.4477e-01,  6.7091e-02,\n",
      "           5.2035e-01,  1.7455e-01,  1.0754e-01,  1.6867e-01, -3.3355e-01,\n",
      "           1.3741e-02,  1.3731e-01, -3.4413e-01, -1.7527e-01, -7.3142e-02,\n",
      "          -2.2925e-01, -1.9021e-01,  2.1522e-02, -1.4950e-01, -1.7852e-01,\n",
      "           4.0985e-01,  2.0790e-01,  6.9119e-02,  2.4159e-01,  1.7779e-01,\n",
      "          -2.0075e-01,  1.4514e-01, -1.8094e-01,  6.6302e-03,  4.2478e-01,\n",
      "           1.6516e-01, -4.6354e-03,  2.0981e-01,  1.2206e-02, -2.5492e-01,\n",
      "           1.8235e-01,  1.4293e-01,  5.1464e-01,  1.0051e-02,  8.5683e-02,\n",
      "           3.5876e-01, -9.2658e-02,  1.3059e-01, -3.3648e-01, -7.8429e-04,\n",
      "          -2.3728e-01, -1.5001e-01, -2.4329e-01,  1.1201e-01, -6.3366e-01,\n",
      "          -1.0946e-01,  6.3935e-02,  6.4402e-02, -1.0548e-01,  1.5449e-01,\n",
      "          -9.8260e-03,  1.6522e-01, -1.7672e-01,  3.9264e-02,  1.4398e-02,\n",
      "           2.7097e-02,  9.6198e-02, -1.7313e-01,  4.8141e-01,  3.9554e-01,\n",
      "          -2.0180e-01, -1.5299e-01, -6.9415e-02, -5.4690e-03,  7.4589e-02,\n",
      "           4.9904e-01, -1.6368e-01, -4.5197e-01,  1.1508e-01,  7.3816e-03,\n",
      "          -2.7377e-01,  5.7019e-02, -4.3410e-01,  1.0619e-01, -6.8370e-02,\n",
      "           1.4690e-01, -6.2871e-02, -1.2238e-01,  4.3168e-02, -3.2315e-01,\n",
      "           1.2877e-01, -2.5592e-01,  7.7193e-02, -3.3558e-02,  3.5453e-01,\n",
      "           1.7273e-01,  9.8341e-02,  4.4792e-01,  4.4970e-01,  4.0068e-02,\n",
      "          -7.6622e-02,  1.6167e-01, -1.5354e-01, -2.8752e-01, -2.7855e-01,\n",
      "          -6.4019e-02,  6.2570e-02, -2.7324e-01]],\n",
      "\n",
      "        [[-1.3802e-01,  6.8996e-02, -4.8467e-02,  1.2980e-01,  1.9246e-01,\n",
      "          -3.1710e-01, -2.7329e-01,  7.0101e-03, -3.2536e-01,  3.7709e-03,\n",
      "          -3.5307e-02,  9.2910e-02, -1.6135e-01, -1.3853e-01, -9.9079e-02,\n",
      "           2.4120e-01, -3.2007e-01,  3.6499e-01,  2.6381e-01,  1.0466e-01,\n",
      "          -1.4418e-01, -6.1753e-02, -5.1352e-02, -1.3248e-01,  1.3810e-01,\n",
      "           1.4872e-01, -5.3805e-03, -1.4348e-01, -1.4635e-01,  6.8639e-02,\n",
      "           5.2314e-01,  1.7297e-01,  1.0612e-01,  1.7237e-01, -3.3342e-01,\n",
      "           1.5620e-02,  1.3786e-01, -3.4478e-01, -1.7357e-01, -7.3817e-02,\n",
      "          -2.2839e-01, -1.9028e-01,  2.0833e-02, -1.5262e-01, -1.7786e-01,\n",
      "           4.0782e-01,  2.1033e-01,  6.9667e-02,  2.4333e-01,  1.7319e-01,\n",
      "          -2.0348e-01,  1.4591e-01, -1.8047e-01,  8.2236e-03,  4.2658e-01,\n",
      "           1.6416e-01, -1.6033e-03,  2.0924e-01,  1.2999e-02, -2.5437e-01,\n",
      "           1.8147e-01,  1.4655e-01,  5.1305e-01,  9.6218e-03,  8.6919e-02,\n",
      "           3.6042e-01, -9.4399e-02,  1.3081e-01, -3.3938e-01, -2.0864e-03,\n",
      "          -2.3332e-01, -1.5096e-01, -2.4091e-01,  1.1107e-01, -6.3580e-01,\n",
      "          -1.0682e-01,  6.6432e-02,  6.3270e-02, -1.0169e-01,  1.5473e-01,\n",
      "          -1.3626e-02,  1.6639e-01, -1.7628e-01,  3.8198e-02,  1.2391e-02,\n",
      "           2.7772e-02,  9.7925e-02, -1.7132e-01,  4.8100e-01,  3.9219e-01,\n",
      "          -2.0307e-01, -1.5300e-01, -6.9825e-02, -5.7512e-03,  7.5003e-02,\n",
      "           4.9456e-01, -1.6146e-01, -4.5027e-01,  1.1379e-01,  4.5959e-03,\n",
      "          -2.7730e-01,  5.8838e-02, -4.3397e-01,  1.0604e-01, -6.7307e-02,\n",
      "           1.4476e-01, -6.4900e-02, -1.2446e-01,  4.4097e-02, -3.2353e-01,\n",
      "           1.2729e-01, -2.5929e-01,  7.4698e-02, -3.1794e-02,  3.5453e-01,\n",
      "           1.7162e-01,  9.9255e-02,  4.4841e-01,  4.4888e-01,  4.0106e-02,\n",
      "          -7.3954e-02,  1.6550e-01, -1.5382e-01, -2.8921e-01, -2.7921e-01,\n",
      "          -6.3129e-02,  6.5924e-02, -2.7213e-01]],\n",
      "\n",
      "        [[-1.3741e-01,  6.4212e-02, -4.8732e-02,  1.2821e-01,  1.8990e-01,\n",
      "          -3.2122e-01, -2.7482e-01, -9.0392e-04, -3.2551e-01, -1.1750e-03,\n",
      "          -3.6147e-02,  9.2270e-02, -1.5911e-01, -1.3533e-01, -1.0179e-01,\n",
      "           2.3219e-01, -3.2289e-01,  3.6300e-01,  2.6190e-01,  1.0636e-01,\n",
      "          -1.4771e-01, -5.7692e-02, -4.8142e-02, -1.2766e-01,  1.3941e-01,\n",
      "           1.4143e-01, -1.1354e-02, -1.4865e-01, -1.4011e-01,  6.6210e-02,\n",
      "           5.1726e-01,  1.7401e-01,  1.1037e-01,  1.6718e-01, -3.2835e-01,\n",
      "           8.5755e-03,  1.3312e-01, -3.4145e-01, -1.7507e-01, -6.9192e-02,\n",
      "          -2.3194e-01, -1.9306e-01,  2.2216e-02, -1.4858e-01, -1.7975e-01,\n",
      "           4.1063e-01,  2.0683e-01,  7.2885e-02,  2.3726e-01,  1.7739e-01,\n",
      "          -2.0151e-01,  1.4668e-01, -1.8449e-01,  7.2988e-03,  4.1603e-01,\n",
      "           1.6497e-01, -5.2181e-03,  2.0786e-01,  1.3022e-02, -2.5537e-01,\n",
      "           1.8475e-01,  1.4099e-01,  5.1805e-01,  9.2345e-03,  8.7512e-02,\n",
      "           3.5736e-01, -9.3645e-02,  1.3247e-01, -3.3557e-01,  6.0268e-03,\n",
      "          -2.3476e-01, -1.5475e-01, -2.4060e-01,  1.0972e-01, -6.2981e-01,\n",
      "          -1.0793e-01,  6.5430e-02,  6.5787e-02, -1.0335e-01,  1.5813e-01,\n",
      "          -1.0631e-02,  1.6467e-01, -1.7873e-01,  3.7099e-02,  1.5700e-02,\n",
      "           2.4873e-02,  9.4294e-02, -1.6939e-01,  4.7941e-01,  3.9829e-01,\n",
      "          -2.0192e-01, -1.5594e-01, -6.1145e-02, -4.4052e-03,  7.4273e-02,\n",
      "           4.9759e-01, -1.6352e-01, -4.5022e-01,  1.1777e-01,  3.9401e-03,\n",
      "          -2.7689e-01,  6.2708e-02, -4.3244e-01,  1.0249e-01, -6.5643e-02,\n",
      "           1.4148e-01, -5.9646e-02, -1.1891e-01,  4.7602e-02, -3.2761e-01,\n",
      "           1.3074e-01, -2.5148e-01,  7.9227e-02, -3.4423e-02,  3.4970e-01,\n",
      "           1.7448e-01,  9.5992e-02,  4.4949e-01,  4.4492e-01,  4.0559e-02,\n",
      "          -7.9881e-02,  1.5851e-01, -1.5065e-01, -2.8713e-01, -2.7913e-01,\n",
      "          -6.3771e-02,  5.6897e-02, -2.7192e-01]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = model.forward(src, src, src)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1, 5)\n",
      "(128, 128)\n",
      "in_proj_bias\n",
      "here!\n",
      "0q_0in_proj_weight 1\n",
      "(128,)\n",
      "in_proj_bias\n",
      "0q_1in_proj_bias 2\n",
      "(128, 128)\n",
      "in_proj_bias\n",
      "here!\n",
      "1k_0in_proj_weight 3\n",
      "(128,)\n",
      "in_proj_bias\n",
      "1k_1in_proj_bias 4\n",
      "(128, 128)\n",
      "in_proj_bias\n",
      "here!\n",
      "2v_0in_proj_weight 5\n",
      "(128,)\n",
      "in_proj_bias\n",
      "2v_1in_proj_bias 6\n",
      "(128, 128)\n",
      "out_proj.weight 7\n",
      "(128,)\n",
      "out_proj.bias 8\n",
      "(128, 1, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = '/private/home/padentomasello/scratch/pytorch_testing/multi_headed_attention.array'\n",
    "af.array.save_array('input', toArrayFire(src), filepath, False)\n",
    "i = 1\n",
    "params = {}\n",
    "for (name, param) in model.named_parameters():\n",
    "        if 'in_proj' in name:\n",
    "            q, k, v = param.chunk(3, dim=0)\n",
    "            hack = '0'\n",
    "            if 'in_proj_bias' in name: hack = '1'\n",
    "            params['0q_' + hack + name] = q\n",
    "            params['1k_' + hack + name] = k\n",
    "            params['2v_' + hack + name] = v\n",
    "            if 'in_proj_bias' in name:\n",
    "                for key in sorted(params.keys()):\n",
    "                    af_array = toArrayFire(params[key])\n",
    "                    print(name)\n",
    "                    if 'weight' in key:\n",
    "                        print('here!')\n",
    "                        af_array = af.array.transpose(af_array)\n",
    "                    print(key, i)\n",
    "                    af.array.save_array(key, af_array, filepath, True)  \n",
    "                    i = i + 1\n",
    "            continue\n",
    "        elif len(param.size()) > 0:\n",
    "            af_array = toArrayFire(param)\n",
    "            if 'fc' in name and 'weight' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            if 'weight' in name and 'proj' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            print(name, i)\n",
    "            af.array.save_array(name, af_array, filepath, True)\n",
    "            i = i + 1\n",
    "af.array.save_array('output', toArrayFire(output), filepath, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3660e-01,  6.6087e-02, -5.0627e-02,  1.2639e-01,  1.9532e-01,\n",
       "          -3.2134e-01, -2.7343e-01,  2.0153e-03, -3.2442e-01,  5.6851e-04,\n",
       "          -3.4001e-02,  9.2845e-02, -1.6106e-01, -1.3433e-01, -1.0303e-01,\n",
       "           2.3235e-01, -3.2212e-01,  3.6679e-01,  2.6246e-01,  1.0902e-01,\n",
       "          -1.4785e-01, -5.7198e-02, -4.8061e-02, -1.2659e-01,  1.3820e-01,\n",
       "           1.3805e-01, -8.7764e-03, -1.4642e-01, -1.4199e-01,  6.4847e-02,\n",
       "           5.1739e-01,  1.7461e-01,  1.0978e-01,  1.6533e-01, -3.3223e-01,\n",
       "           1.0636e-02,  1.3526e-01, -3.4203e-01, -1.7641e-01, -7.0847e-02,\n",
       "          -2.3090e-01, -1.9169e-01,  2.2647e-02, -1.4761e-01, -1.7998e-01,\n",
       "           4.1073e-01,  2.0616e-01,  6.9799e-02,  2.3915e-01,  1.7935e-01,\n",
       "          -1.9842e-01,  1.4602e-01, -1.8208e-01,  4.8545e-03,  4.2007e-01,\n",
       "           1.6484e-01, -6.2551e-03,  2.0847e-01,  1.1782e-02, -2.5447e-01,\n",
       "           1.8329e-01,  1.4128e-01,  5.1748e-01,  1.0040e-02,  8.6098e-02,\n",
       "           3.5674e-01, -9.3306e-02,  1.3212e-01, -3.3554e-01,  2.6614e-03,\n",
       "          -2.3813e-01, -1.5223e-01, -2.4392e-01,  1.1212e-01, -6.3059e-01,\n",
       "          -1.0907e-01,  6.3677e-02,  6.5187e-02, -1.0750e-01,  1.5629e-01,\n",
       "          -7.9210e-03,  1.6473e-01, -1.7678e-01,  3.9184e-02,  1.6106e-02,\n",
       "           2.5946e-02,  9.4928e-02, -1.7164e-01,  4.8088e-01,  3.9728e-01,\n",
       "          -2.0153e-01, -1.5492e-01, -6.3419e-02, -5.0407e-03,  7.5270e-02,\n",
       "           5.0027e-01, -1.6491e-01, -4.5283e-01,  1.1642e-01,  6.8036e-03,\n",
       "          -2.7294e-01,  5.8738e-02, -4.3244e-01,  1.0356e-01, -6.6180e-02,\n",
       "           1.4505e-01, -6.0490e-02, -1.1998e-01,  4.5172e-02, -3.2491e-01,\n",
       "           1.2912e-01, -2.5200e-01,  8.0196e-02, -3.4363e-02,  3.5363e-01,\n",
       "           1.7464e-01,  9.6368e-02,  4.4882e-01,  4.4896e-01,  3.9716e-02,\n",
       "          -7.9687e-02,  1.5968e-01, -1.5125e-01, -2.8583e-01, -2.7855e-01,\n",
       "          -6.5042e-02,  5.7501e-02, -2.7408e-01]],\n",
       "\n",
       "        [[-1.3957e-01,  6.6965e-02, -4.9062e-02,  1.2663e-01,  1.9383e-01,\n",
       "          -3.1976e-01, -2.7460e-01,  2.8361e-03, -3.2307e-01, -1.7677e-03,\n",
       "          -3.4727e-02,  9.4332e-02, -1.6279e-01, -1.3512e-01, -1.0233e-01,\n",
       "           2.3515e-01, -3.2009e-01,  3.6787e-01,  2.6304e-01,  1.0677e-01,\n",
       "          -1.4825e-01, -5.9035e-02, -5.0387e-02, -1.2938e-01,  1.3860e-01,\n",
       "           1.4128e-01, -8.1266e-03, -1.4644e-01, -1.4313e-01,  6.7518e-02,\n",
       "           5.1856e-01,  1.7591e-01,  1.0845e-01,  1.6782e-01, -3.3105e-01,\n",
       "           1.2160e-02,  1.3615e-01, -3.4403e-01, -1.7559e-01, -7.1952e-02,\n",
       "          -2.3001e-01, -1.9023e-01,  2.1512e-02, -1.4836e-01, -1.7839e-01,\n",
       "           4.1123e-01,  2.0620e-01,  7.0413e-02,  2.3959e-01,  1.7986e-01,\n",
       "          -2.0009e-01,  1.4435e-01, -1.8221e-01,  7.1258e-03,  4.2253e-01,\n",
       "           1.6600e-01, -5.6152e-03,  2.1025e-01,  1.2055e-02, -2.5575e-01,\n",
       "           1.8344e-01,  1.4054e-01,  5.1505e-01,  9.3630e-03,  8.5386e-02,\n",
       "           3.5747e-01, -9.0333e-02,  1.2949e-01, -3.3394e-01,  5.8391e-04,\n",
       "          -2.3791e-01, -1.5024e-01, -2.4317e-01,  1.1114e-01, -6.3144e-01,\n",
       "          -1.1073e-01,  6.2529e-02,  6.5787e-02, -1.0553e-01,  1.5498e-01,\n",
       "          -9.3952e-03,  1.6370e-01, -1.7840e-01,  3.8986e-02,  1.5676e-02,\n",
       "           2.6398e-02,  9.4221e-02, -1.7369e-01,  4.8098e-01,  3.9786e-01,\n",
       "          -2.0063e-01, -1.5240e-01, -6.9423e-02, -5.0040e-03,  7.3465e-02,\n",
       "           5.0004e-01, -1.6405e-01, -4.5196e-01,  1.1571e-01,  7.9269e-03,\n",
       "          -2.7256e-01,  5.7143e-02, -4.3468e-01,  1.0625e-01, -6.9402e-02,\n",
       "           1.4658e-01, -6.0923e-02, -1.2005e-01,  4.3258e-02, -3.2335e-01,\n",
       "           1.3039e-01, -2.5391e-01,  7.7050e-02, -3.5212e-02,  3.5214e-01,\n",
       "           1.7216e-01,  9.7881e-02,  4.4720e-01,  4.4769e-01,  4.0235e-02,\n",
       "          -7.7293e-02,  1.5816e-01, -1.5312e-01, -2.8693e-01, -2.7811e-01,\n",
       "          -6.3207e-02,  6.1003e-02, -2.7202e-01]],\n",
       "\n",
       "        [[-1.3806e-01,  6.8081e-02, -4.9596e-02,  1.2769e-01,  1.9527e-01,\n",
       "          -3.1916e-01, -2.7339e-01,  4.9000e-03, -3.2420e-01,  1.5972e-03,\n",
       "          -3.4120e-02,  9.3736e-02, -1.6265e-01, -1.3629e-01, -1.0141e-01,\n",
       "           2.3725e-01, -3.2072e-01,  3.6754e-01,  2.6351e-01,  1.0713e-01,\n",
       "          -1.4680e-01, -5.9598e-02, -5.0439e-02, -1.2972e-01,  1.3820e-01,\n",
       "           1.4298e-01, -6.6650e-03, -1.4488e-01, -1.4477e-01,  6.7091e-02,\n",
       "           5.2035e-01,  1.7455e-01,  1.0754e-01,  1.6867e-01, -3.3355e-01,\n",
       "           1.3741e-02,  1.3731e-01, -3.4413e-01, -1.7527e-01, -7.3142e-02,\n",
       "          -2.2925e-01, -1.9021e-01,  2.1522e-02, -1.4950e-01, -1.7852e-01,\n",
       "           4.0985e-01,  2.0790e-01,  6.9119e-02,  2.4159e-01,  1.7779e-01,\n",
       "          -2.0075e-01,  1.4514e-01, -1.8094e-01,  6.6302e-03,  4.2478e-01,\n",
       "           1.6516e-01, -4.6354e-03,  2.0981e-01,  1.2206e-02, -2.5492e-01,\n",
       "           1.8235e-01,  1.4293e-01,  5.1464e-01,  1.0051e-02,  8.5683e-02,\n",
       "           3.5876e-01, -9.2658e-02,  1.3059e-01, -3.3648e-01, -7.8429e-04,\n",
       "          -2.3728e-01, -1.5001e-01, -2.4329e-01,  1.1201e-01, -6.3366e-01,\n",
       "          -1.0946e-01,  6.3935e-02,  6.4402e-02, -1.0548e-01,  1.5449e-01,\n",
       "          -9.8260e-03,  1.6522e-01, -1.7672e-01,  3.9264e-02,  1.4398e-02,\n",
       "           2.7097e-02,  9.6198e-02, -1.7313e-01,  4.8141e-01,  3.9554e-01,\n",
       "          -2.0180e-01, -1.5299e-01, -6.9415e-02, -5.4690e-03,  7.4589e-02,\n",
       "           4.9904e-01, -1.6368e-01, -4.5197e-01,  1.1508e-01,  7.3816e-03,\n",
       "          -2.7377e-01,  5.7019e-02, -4.3410e-01,  1.0619e-01, -6.8370e-02,\n",
       "           1.4690e-01, -6.2871e-02, -1.2238e-01,  4.3168e-02, -3.2315e-01,\n",
       "           1.2877e-01, -2.5592e-01,  7.7193e-02, -3.3558e-02,  3.5453e-01,\n",
       "           1.7273e-01,  9.8341e-02,  4.4792e-01,  4.4970e-01,  4.0068e-02,\n",
       "          -7.6622e-02,  1.6167e-01, -1.5354e-01, -2.8752e-01, -2.7855e-01,\n",
       "          -6.4019e-02,  6.2570e-02, -2.7324e-01]],\n",
       "\n",
       "        [[-1.3802e-01,  6.8996e-02, -4.8467e-02,  1.2980e-01,  1.9246e-01,\n",
       "          -3.1710e-01, -2.7329e-01,  7.0101e-03, -3.2536e-01,  3.7709e-03,\n",
       "          -3.5307e-02,  9.2910e-02, -1.6135e-01, -1.3853e-01, -9.9079e-02,\n",
       "           2.4120e-01, -3.2007e-01,  3.6499e-01,  2.6381e-01,  1.0466e-01,\n",
       "          -1.4418e-01, -6.1753e-02, -5.1352e-02, -1.3248e-01,  1.3810e-01,\n",
       "           1.4872e-01, -5.3805e-03, -1.4348e-01, -1.4635e-01,  6.8639e-02,\n",
       "           5.2314e-01,  1.7297e-01,  1.0612e-01,  1.7237e-01, -3.3342e-01,\n",
       "           1.5620e-02,  1.3786e-01, -3.4478e-01, -1.7357e-01, -7.3817e-02,\n",
       "          -2.2839e-01, -1.9028e-01,  2.0833e-02, -1.5262e-01, -1.7786e-01,\n",
       "           4.0782e-01,  2.1033e-01,  6.9667e-02,  2.4333e-01,  1.7319e-01,\n",
       "          -2.0348e-01,  1.4591e-01, -1.8047e-01,  8.2236e-03,  4.2658e-01,\n",
       "           1.6416e-01, -1.6033e-03,  2.0924e-01,  1.2999e-02, -2.5437e-01,\n",
       "           1.8147e-01,  1.4655e-01,  5.1305e-01,  9.6218e-03,  8.6919e-02,\n",
       "           3.6042e-01, -9.4399e-02,  1.3081e-01, -3.3938e-01, -2.0864e-03,\n",
       "          -2.3332e-01, -1.5096e-01, -2.4091e-01,  1.1107e-01, -6.3580e-01,\n",
       "          -1.0682e-01,  6.6432e-02,  6.3270e-02, -1.0169e-01,  1.5473e-01,\n",
       "          -1.3626e-02,  1.6639e-01, -1.7628e-01,  3.8198e-02,  1.2391e-02,\n",
       "           2.7772e-02,  9.7925e-02, -1.7132e-01,  4.8100e-01,  3.9219e-01,\n",
       "          -2.0307e-01, -1.5300e-01, -6.9825e-02, -5.7512e-03,  7.5003e-02,\n",
       "           4.9456e-01, -1.6146e-01, -4.5027e-01,  1.1379e-01,  4.5959e-03,\n",
       "          -2.7730e-01,  5.8838e-02, -4.3397e-01,  1.0604e-01, -6.7307e-02,\n",
       "           1.4476e-01, -6.4900e-02, -1.2446e-01,  4.4097e-02, -3.2353e-01,\n",
       "           1.2729e-01, -2.5929e-01,  7.4698e-02, -3.1794e-02,  3.5453e-01,\n",
       "           1.7162e-01,  9.9255e-02,  4.4841e-01,  4.4888e-01,  4.0106e-02,\n",
       "          -7.3954e-02,  1.6550e-01, -1.5382e-01, -2.8921e-01, -2.7921e-01,\n",
       "          -6.3129e-02,  6.5924e-02, -2.7213e-01]],\n",
       "\n",
       "        [[-1.3741e-01,  6.4212e-02, -4.8732e-02,  1.2821e-01,  1.8990e-01,\n",
       "          -3.2122e-01, -2.7482e-01, -9.0392e-04, -3.2551e-01, -1.1750e-03,\n",
       "          -3.6147e-02,  9.2270e-02, -1.5911e-01, -1.3533e-01, -1.0179e-01,\n",
       "           2.3219e-01, -3.2289e-01,  3.6300e-01,  2.6190e-01,  1.0636e-01,\n",
       "          -1.4771e-01, -5.7692e-02, -4.8142e-02, -1.2766e-01,  1.3941e-01,\n",
       "           1.4143e-01, -1.1354e-02, -1.4865e-01, -1.4011e-01,  6.6210e-02,\n",
       "           5.1726e-01,  1.7401e-01,  1.1037e-01,  1.6718e-01, -3.2835e-01,\n",
       "           8.5755e-03,  1.3312e-01, -3.4145e-01, -1.7507e-01, -6.9192e-02,\n",
       "          -2.3194e-01, -1.9306e-01,  2.2216e-02, -1.4858e-01, -1.7975e-01,\n",
       "           4.1063e-01,  2.0683e-01,  7.2885e-02,  2.3726e-01,  1.7739e-01,\n",
       "          -2.0151e-01,  1.4668e-01, -1.8449e-01,  7.2988e-03,  4.1603e-01,\n",
       "           1.6497e-01, -5.2181e-03,  2.0786e-01,  1.3022e-02, -2.5537e-01,\n",
       "           1.8475e-01,  1.4099e-01,  5.1805e-01,  9.2345e-03,  8.7512e-02,\n",
       "           3.5736e-01, -9.3645e-02,  1.3247e-01, -3.3557e-01,  6.0268e-03,\n",
       "          -2.3476e-01, -1.5475e-01, -2.4060e-01,  1.0972e-01, -6.2981e-01,\n",
       "          -1.0793e-01,  6.5430e-02,  6.5787e-02, -1.0335e-01,  1.5813e-01,\n",
       "          -1.0631e-02,  1.6467e-01, -1.7873e-01,  3.7099e-02,  1.5700e-02,\n",
       "           2.4873e-02,  9.4294e-02, -1.6939e-01,  4.7941e-01,  3.9829e-01,\n",
       "          -2.0192e-01, -1.5594e-01, -6.1145e-02, -4.4052e-03,  7.4273e-02,\n",
       "           4.9759e-01, -1.6352e-01, -4.5022e-01,  1.1777e-01,  3.9401e-03,\n",
       "          -2.7689e-01,  6.2708e-02, -4.3244e-01,  1.0249e-01, -6.5643e-02,\n",
       "           1.4148e-01, -5.9646e-02, -1.1891e-01,  4.7602e-02, -3.2761e-01,\n",
       "           1.3074e-01, -2.5148e-01,  7.9227e-02, -3.4423e-02,  3.4970e-01,\n",
       "           1.7448e-01,  9.5992e-02,  4.4949e-01,  4.4492e-01,  4.0559e-02,\n",
       "          -7.9881e-02,  1.5851e-01, -1.5065e-01, -2.8713e-01, -2.7913e-01,\n",
       "          -6.3771e-02,  5.6897e-02, -2.7192e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
