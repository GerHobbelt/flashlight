{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arrayfire as af\n",
    "import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toArrayFire(x):\n",
    "    x_np = x.detach().contiguous().numpy()\n",
    "    shape = 1\n",
    "    if len(x_np.shape) == 0:\n",
    "        shape = (1,)\n",
    "    else:\n",
    "        shape = x_np.shape[::-1]\n",
    "    afArray = af.Array(x_np.ctypes.data, shape, x_np.dtype.char)\n",
    "    return afArray\n",
    "\n",
    "def saveStateDict(module, filepath):\n",
    "    i = 0\n",
    "    for (name, param) in module.named_parameters():\n",
    "        #param = module.state_dict()[name]\n",
    "        print(name, \"\\t\", param.size())\n",
    "        if 'in_proj' in name:\n",
    "            print(param.shape)\n",
    "            q, k, v = param.chunk(3, dim=0)\n",
    "            print('in_proj!')\n",
    "            af.array.save_array(name + 'q', toArrayFire(q), filepath, True)\n",
    "            af.array.save_array(name + 'k', toArrayFire(k), filepath, True)\n",
    "            af.array.save_array(name + 'v', toArrayFire(k), filepath, True)\n",
    "            continue\n",
    "        if len(param.size()) > 0:\n",
    "            af_array = toArrayFire(param)\n",
    "            if 'fc' in name and 'weight' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            af.array.save_array(name, af_array, filepath, True)\n",
    "            i = i + 1\n",
    "    print(i)\n",
    "    for name in module.state_dict():\n",
    "        if 'running' in name:\n",
    "            print(name)\n",
    "            af_array = toArrayFire(module.state_dict()[name])\n",
    "            af.array.save_array(name, af_array, filepath + 'running', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.backbone import *\n",
    "from models.position_encoding import *\n",
    "from models.matcher import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.4640,  0.5245, -0.4588,  ..., -0.0227, -0.3933,  0.8547]],\n",
      "\n",
      "        [[ 0.8655,  0.2807,  1.1985,  ...,  0.9851,  0.2143, -0.5303]],\n",
      "\n",
      "        [[ 0.6088,  0.8931, -0.2293,  ..., -0.9108, -0.1970,  1.4215]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0110,  1.6418,  0.6668,  ...,  0.7015, -0.1773, -0.9895]],\n",
      "\n",
      "        [[-1.3038,  1.0231,  0.1501,  ..., -0.7441,  1.0080,  0.3091]],\n",
      "\n",
      "        [[-1.1998, -0.3417, -0.0349,  ..., -0.6445,  0.3683, -0.7311]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "embedding_size = 128\n",
    "src_len = 5\n",
    "tgt_len = 10\n",
    "queries = torch.rand(tgt_len, batch_size, embedding_size)\n",
    "memory = torch.rand(src_len, batch_size, embedding_size)\n",
    "model = TransformerDecoderLayer(embedding_size, 1, dropout=0.0, dim_feedforward=128)\n",
    "output = model.forward(queries, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0q_0self_attn.in_proj_weight 2 torch.Size([128, 128])\n",
      "2\n",
      "0q_1self_attn.in_proj_bias 3 torch.Size([128])\n",
      "3\n",
      "1k_0self_attn.in_proj_weight 4 torch.Size([128, 128])\n",
      "4\n",
      "1k_1self_attn.in_proj_bias 5 torch.Size([128])\n",
      "5\n",
      "2v_0self_attn.in_proj_weight 6 torch.Size([128, 128])\n",
      "6\n",
      "2v_1self_attn.in_proj_bias 7 torch.Size([128])\n",
      "7\n",
      "self_attn.out_proj.weight 8 torch.Size([128, 128])\n",
      "8\n",
      "self_attn.out_proj.bias 9 torch.Size([128])\n",
      "9\n",
      "0q_0multihead_attn.in_proj_weight 10 torch.Size([128, 128])\n",
      "10\n",
      "0q_1multihead_attn.in_proj_bias 11 torch.Size([128])\n",
      "11\n",
      "1k_0multihead_attn.in_proj_weight 12 torch.Size([128, 128])\n",
      "12\n",
      "1k_1multihead_attn.in_proj_bias 13 torch.Size([128])\n",
      "13\n",
      "2v_0multihead_attn.in_proj_weight 14 torch.Size([128, 128])\n",
      "14\n",
      "2v_1multihead_attn.in_proj_bias 15 torch.Size([128])\n",
      "15\n",
      "multihead_attn.out_proj.weight 16 torch.Size([128, 128])\n",
      "16\n",
      "multihead_attn.out_proj.bias 17 torch.Size([128])\n",
      "17\n",
      "linear1.weight 18 torch.Size([128, 128])\n",
      "18\n",
      "linear1.bias 19 torch.Size([128])\n",
      "19\n",
      "linear2.weight 20 torch.Size([128, 128])\n",
      "20\n",
      "linear2.bias 21 torch.Size([128])\n",
      "21\n",
      "norm1.weight 22 torch.Size([128])\n",
      "22\n",
      "norm1.bias 23 torch.Size([128])\n",
      "23\n",
      "norm2.weight 24 torch.Size([128])\n",
      "24\n",
      "norm2.bias 25 torch.Size([128])\n",
      "25\n",
      "norm3.weight 26 torch.Size([128])\n",
      "26\n",
      "norm3.bias 27 torch.Size([128])\n",
      "27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = '/private/home/padentomasello/scratch/pytorch_testing/transformer_decoder_layer.array'\n",
    "af.array.save_array('queries', toArrayFire(queries), filepath, False)\n",
    "af.array.save_array('memory', toArrayFire(memory), filepath, True)\n",
    "i = 2\n",
    "params = {}\n",
    "for (name, param) in model.named_parameters():\n",
    "        if 'in_proj' in name:\n",
    "            q, k, v = param.chunk(3, dim=0)\n",
    "            hack = '0'\n",
    "            if 'in_proj_bias' in name: hack = '1'\n",
    "            params['0q_' + hack + name] = q\n",
    "            params['1k_' + hack + name] = k\n",
    "            params['2v_' + hack + name] = v\n",
    "            if 'in_proj_bias' in name:\n",
    "                for key in sorted(params.keys()):\n",
    "                    af_array = toArrayFire(params[key])\n",
    "                    if 'weight' in key:\n",
    "                        af_array = af.array.transpose(af_array)\n",
    "                    print(key, i, params[key].shape)\n",
    "                    print(af.array.save_array(key, af_array, filepath, True))\n",
    "                    i = i + 1\n",
    "                params = {}\n",
    "            continue\n",
    "        elif len(param.size()) > 0:\n",
    "            af_array = toArrayFire(param)\n",
    "            if 'fc' in name and 'weight' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            if 'weight' in name and 'proj' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            if 'weight' in name and 'linear' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            print(name, i, param.shape)\n",
    "            print(af.array.save_array(name, af_array, filepath, True))\n",
    "            i = i + 1\n",
    "#af.array.save_array('output', toArrayFire(output), filepath, True)\n",
    "af.array.save_array('output', toArrayFire(output), filepath, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.2040e+00,  2.4613e-01, -5.2788e-01,  ...,  8.7406e-01,\n",
       "          -8.1491e-01,  6.8629e-01]],\n",
       "\n",
       "        [[ 1.0051e+00,  3.8374e-01,  1.2483e+00,  ...,  1.6616e+00,\n",
       "          -9.4458e-02, -6.5259e-01]],\n",
       "\n",
       "        [[ 6.3987e-01,  7.4390e-01, -3.5206e-01,  ..., -2.2591e-01,\n",
       "          -5.5494e-01,  1.1156e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-8.5121e-02,  1.4491e+00,  3.2883e-01,  ...,  1.2137e+00,\n",
       "          -2.5187e-01, -1.0812e+00]],\n",
       "\n",
       "        [[-1.3683e+00,  1.1406e+00,  2.9048e-01,  ..., -1.3256e-01,\n",
       "           7.7135e-01,  2.9597e-04]],\n",
       "\n",
       "        [[-1.4317e+00, -5.8421e-01, -4.9685e-01,  ...,  7.8811e-02,\n",
       "          -1.9064e-01, -8.7452e-01]]], grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
