{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arrayfire as af\n",
    "import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toArrayFire(x):\n",
    "    x_np = x.detach().contiguous().numpy()\n",
    "    shape = 1\n",
    "    if len(x_np.shape) == 0:\n",
    "        shape = (1,)\n",
    "    else:\n",
    "        shape = x_np.shape[::-1]\n",
    "    afArray = af.Array(x_np.ctypes.data, shape, x_np.dtype.char)\n",
    "    return afArray\n",
    "\n",
    "def saveStateDict(model, filepath):\n",
    "    params = {}\n",
    "    i = 0\n",
    "    for (name, param) in model.named_parameters():\n",
    "        if 'in_proj' in name:\n",
    "            q, k, v = param.chunk(3, dim=0)\n",
    "            hack = '0'\n",
    "            if 'in_proj_bias' in name: hack = '1'\n",
    "            params['0q_' + hack + name] = q\n",
    "            params['1k_' + hack + name] = k\n",
    "            params['2v_' + hack + name] = v\n",
    "            if 'in_proj_bias' in name:\n",
    "                for key in sorted(params.keys()):\n",
    "                    af_array = toArrayFire(params[key])\n",
    "                    if 'weight' in key:\n",
    "                        af_array = af.array.transpose(af_array)\n",
    "                    print(key, i, params[key].shape)\n",
    "                    print(af.array.save_array(key, af_array, filepath, True))\n",
    "                    i = i + 1\n",
    "                params = {}\n",
    "            continue\n",
    "        elif len(param.size()) > 0:\n",
    "            af_array = toArrayFire(param)\n",
    "            if 'fc' in name and 'weight' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            if 'weight' in name and 'proj' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            if 'weight' in name and 'linear' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            print(name, i, param.shape)\n",
    "            print(af.array.save_array(name, af_array, filepath, True))\n",
    "            i = i + 1\n",
    "    \n",
    "def create_parser():\n",
    "    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n",
    "    parser.add_argument('--lr', default=1e-4, type=float)\n",
    "    parser.add_argument('--lr_backbone', default=1e-5, type=float)\n",
    "    parser.add_argument('--batch_size', default=2, type=int)\n",
    "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
    "    parser.add_argument('--epochs', default=300, type=int)\n",
    "    parser.add_argument('--lr_drop', default=200, type=int)\n",
    "    parser.add_argument('--optimizer', default=\"adam\", type=str)\n",
    "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
    "                        help='gradient clipping max norm')\n",
    "    parser.add_argument('--eval_skip', default=1, type=int,\n",
    "                        help='do evaluation every \"eval_skip\" frames')\n",
    "    parser.add_argument('--schedule', default='step', type=str,\n",
    "                        choices=('step', 'multistep'))\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--frozen_weights', type=str, default=None,\n",
    "                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n",
    "    # * Backbone\n",
    "    parser.add_argument('--backbone', default='resnet50', type=str,\n",
    "                        help=\"Name of the convolutional backbone to use\")\n",
    "    parser.add_argument('--dilation', action='store_true',\n",
    "                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
    "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n",
    "                        help=\"Type of positional embedding to use on top of the image features\")\n",
    "\n",
    "    # * Transformer\n",
    "    parser.add_argument('--enc_layers', default=6, type=int,\n",
    "                        help=\"Number of encoding layers in the transformer\")\n",
    "    parser.add_argument('--dec_layers', default=6, type=int,\n",
    "                        help=\"Number of decoding layers in the transformer\")\n",
    "    parser.add_argument('--dim_feedforward', default=2048, type=int,\n",
    "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
    "    parser.add_argument('--hidden_dim', default=256, type=int,\n",
    "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
    "    parser.add_argument('--dropout', default=0.1, type=float,\n",
    "                        help=\"Dropout applied in the transformer\")\n",
    "    parser.add_argument('--nheads', default=8, type=int,\n",
    "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
    "    parser.add_argument('--num_queries', default=100, type=int,\n",
    "                        help=\"Number of query slots\")\n",
    "    parser.add_argument('--pre_norm', action='store_true')\n",
    "    parser.add_argument('--no_pass_pos_and_query', dest='pass_pos_and_query', action='store_false',\n",
    "                        help=\"Disables passing the positional encodings to each attention layers\")\n",
    "\n",
    "    # * Segmentation\n",
    "    parser.add_argument('--mask_model', default='none', type=str, choices=(\"none\", \"smallconv\", \"v2\"),\n",
    "                        help=\"Segmentation head to be used (if None, segmentation will not be trained)\")\n",
    "\n",
    "    # Loss\n",
    "    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',\n",
    "                        help=\"Disables auxiliary decoding losses (loss at each layer)\")\n",
    "    parser.add_argument('--set_loss', default='hungarian', type=str,\n",
    "                        choices=('sequential', 'hungarian', 'lexicographical'),\n",
    "                        help=\"Type of matching to perform in the loss\")\n",
    "    parser.add_argument('--bcl', dest='use_bcl', action='store_true',\n",
    "                        help=\"Use balanced classification loss\")\n",
    "    # * Matcher\n",
    "    parser.add_argument('--set_cost_class', default=1, type=float,\n",
    "                        help=\"Class coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_bbox', default=5, type=float,\n",
    "                        help=\"L1 box coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_giou', default=2, type=float,\n",
    "                        help=\"giou box coefficient in the matching cost\")\n",
    "    # * Loss coefficients\n",
    "    parser.add_argument('--mask_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--dice_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--bbox_loss_coef', default=5, type=float)\n",
    "    parser.add_argument('--giou_loss_coef', default=2, type=float)\n",
    "    parser.add_argument('--eos_coef', default=0.1, type=float,\n",
    "                        help=\"Relative classification weight of the no-object class\")\n",
    "\n",
    "    # dataset parameters\n",
    "    parser.add_argument('--dataset_file', default='coco')\n",
    "    parser.add_argument('--coco_path', type=str, default='/datasets01/COCO/022719')\n",
    "    parser.add_argument('--coco_panoptic_path', type=str, default='/datasets01/COCO/060419')\n",
    "    parser.add_argument('--remove_difficult', action='store_true')\n",
    "    parser.add_argument('--masks', action='store_true')\n",
    "\n",
    "    parser.add_argument('--output-dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=42, type=int)\n",
    "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "    parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--num_workers', default=2, type=int)\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world-size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--dist-url', default='env://', help='url used to set up distributed training')\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = create_parser()\n",
    "args = parser.parse_args([\"--hidden_dim=8\", \"--dropout=0.0\"])\n",
    "model = build_transformer(args)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0q_0encoder.layers.0.self_attn.in_proj_weight 0 torch.Size([8, 8])\n",
      "4\n",
      "0q_1encoder.layers.0.self_attn.in_proj_bias 1 torch.Size([8])\n",
      "5\n",
      "1k_0encoder.layers.0.self_attn.in_proj_weight 2 torch.Size([8, 8])\n",
      "6\n",
      "1k_1encoder.layers.0.self_attn.in_proj_bias 3 torch.Size([8])\n",
      "7\n",
      "2v_0encoder.layers.0.self_attn.in_proj_weight 4 torch.Size([8, 8])\n",
      "8\n",
      "2v_1encoder.layers.0.self_attn.in_proj_bias 5 torch.Size([8])\n",
      "9\n",
      "encoder.layers.0.self_attn.out_proj.weight 6 torch.Size([8, 8])\n",
      "10\n",
      "encoder.layers.0.self_attn.out_proj.bias 7 torch.Size([8])\n",
      "11\n",
      "encoder.layers.0.linear1.weight 8 torch.Size([2048, 8])\n",
      "12\n",
      "encoder.layers.0.linear1.bias 9 torch.Size([2048])\n",
      "13\n",
      "encoder.layers.0.linear2.weight 10 torch.Size([8, 2048])\n",
      "14\n",
      "encoder.layers.0.linear2.bias 11 torch.Size([8])\n",
      "15\n",
      "encoder.layers.0.norm1.weight 12 torch.Size([8])\n",
      "16\n",
      "encoder.layers.0.norm1.bias 13 torch.Size([8])\n",
      "17\n",
      "encoder.layers.0.norm2.weight 14 torch.Size([8])\n",
      "18\n",
      "encoder.layers.0.norm2.bias 15 torch.Size([8])\n",
      "19\n",
      "0q_0encoder.layers.1.self_attn.in_proj_weight 16 torch.Size([8, 8])\n",
      "20\n",
      "0q_1encoder.layers.1.self_attn.in_proj_bias 17 torch.Size([8])\n",
      "21\n",
      "1k_0encoder.layers.1.self_attn.in_proj_weight 18 torch.Size([8, 8])\n",
      "22\n",
      "1k_1encoder.layers.1.self_attn.in_proj_bias 19 torch.Size([8])\n",
      "23\n",
      "2v_0encoder.layers.1.self_attn.in_proj_weight 20 torch.Size([8, 8])\n",
      "24\n",
      "2v_1encoder.layers.1.self_attn.in_proj_bias 21 torch.Size([8])\n",
      "25\n",
      "encoder.layers.1.self_attn.out_proj.weight 22 torch.Size([8, 8])\n",
      "26\n",
      "encoder.layers.1.self_attn.out_proj.bias 23 torch.Size([8])\n",
      "27\n",
      "encoder.layers.1.linear1.weight 24 torch.Size([2048, 8])\n",
      "28\n",
      "encoder.layers.1.linear1.bias 25 torch.Size([2048])\n",
      "29\n",
      "encoder.layers.1.linear2.weight 26 torch.Size([8, 2048])\n",
      "30\n",
      "encoder.layers.1.linear2.bias 27 torch.Size([8])\n",
      "31\n",
      "encoder.layers.1.norm1.weight 28 torch.Size([8])\n",
      "32\n",
      "encoder.layers.1.norm1.bias 29 torch.Size([8])\n",
      "33\n",
      "encoder.layers.1.norm2.weight 30 torch.Size([8])\n",
      "34\n",
      "encoder.layers.1.norm2.bias 31 torch.Size([8])\n",
      "35\n",
      "0q_0encoder.layers.2.self_attn.in_proj_weight 32 torch.Size([8, 8])\n",
      "36\n",
      "0q_1encoder.layers.2.self_attn.in_proj_bias 33 torch.Size([8])\n",
      "37\n",
      "1k_0encoder.layers.2.self_attn.in_proj_weight 34 torch.Size([8, 8])\n",
      "38\n",
      "1k_1encoder.layers.2.self_attn.in_proj_bias 35 torch.Size([8])\n",
      "39\n",
      "2v_0encoder.layers.2.self_attn.in_proj_weight 36 torch.Size([8, 8])\n",
      "40\n",
      "2v_1encoder.layers.2.self_attn.in_proj_bias 37 torch.Size([8])\n",
      "41\n",
      "encoder.layers.2.self_attn.out_proj.weight 38 torch.Size([8, 8])\n",
      "42\n",
      "encoder.layers.2.self_attn.out_proj.bias 39 torch.Size([8])\n",
      "43\n",
      "encoder.layers.2.linear1.weight 40 torch.Size([2048, 8])\n",
      "44\n",
      "encoder.layers.2.linear1.bias 41 torch.Size([2048])\n",
      "45\n",
      "encoder.layers.2.linear2.weight 42 torch.Size([8, 2048])\n",
      "46\n",
      "encoder.layers.2.linear2.bias 43 torch.Size([8])\n",
      "47\n",
      "encoder.layers.2.norm1.weight 44 torch.Size([8])\n",
      "48\n",
      "encoder.layers.2.norm1.bias 45 torch.Size([8])\n",
      "49\n",
      "encoder.layers.2.norm2.weight 46 torch.Size([8])\n",
      "50\n",
      "encoder.layers.2.norm2.bias 47 torch.Size([8])\n",
      "51\n",
      "0q_0encoder.layers.3.self_attn.in_proj_weight 48 torch.Size([8, 8])\n",
      "52\n",
      "0q_1encoder.layers.3.self_attn.in_proj_bias 49 torch.Size([8])\n",
      "53\n",
      "1k_0encoder.layers.3.self_attn.in_proj_weight 50 torch.Size([8, 8])\n",
      "54\n",
      "1k_1encoder.layers.3.self_attn.in_proj_bias 51 torch.Size([8])\n",
      "55\n",
      "2v_0encoder.layers.3.self_attn.in_proj_weight 52 torch.Size([8, 8])\n",
      "56\n",
      "2v_1encoder.layers.3.self_attn.in_proj_bias 53 torch.Size([8])\n",
      "57\n",
      "encoder.layers.3.self_attn.out_proj.weight 54 torch.Size([8, 8])\n",
      "58\n",
      "encoder.layers.3.self_attn.out_proj.bias 55 torch.Size([8])\n",
      "59\n",
      "encoder.layers.3.linear1.weight 56 torch.Size([2048, 8])\n",
      "60\n",
      "encoder.layers.3.linear1.bias 57 torch.Size([2048])\n",
      "61\n",
      "encoder.layers.3.linear2.weight 58 torch.Size([8, 2048])\n",
      "62\n",
      "encoder.layers.3.linear2.bias 59 torch.Size([8])\n",
      "63\n",
      "encoder.layers.3.norm1.weight 60 torch.Size([8])\n",
      "64\n",
      "encoder.layers.3.norm1.bias 61 torch.Size([8])\n",
      "65\n",
      "encoder.layers.3.norm2.weight 62 torch.Size([8])\n",
      "66\n",
      "encoder.layers.3.norm2.bias 63 torch.Size([8])\n",
      "67\n",
      "0q_0encoder.layers.4.self_attn.in_proj_weight 64 torch.Size([8, 8])\n",
      "68\n",
      "0q_1encoder.layers.4.self_attn.in_proj_bias 65 torch.Size([8])\n",
      "69\n",
      "1k_0encoder.layers.4.self_attn.in_proj_weight 66 torch.Size([8, 8])\n",
      "70\n",
      "1k_1encoder.layers.4.self_attn.in_proj_bias 67 torch.Size([8])\n",
      "71\n",
      "2v_0encoder.layers.4.self_attn.in_proj_weight 68 torch.Size([8, 8])\n",
      "72\n",
      "2v_1encoder.layers.4.self_attn.in_proj_bias 69 torch.Size([8])\n",
      "73\n",
      "encoder.layers.4.self_attn.out_proj.weight 70 torch.Size([8, 8])\n",
      "74\n",
      "encoder.layers.4.self_attn.out_proj.bias 71 torch.Size([8])\n",
      "75\n",
      "encoder.layers.4.linear1.weight 72 torch.Size([2048, 8])\n",
      "76\n",
      "encoder.layers.4.linear1.bias 73 torch.Size([2048])\n",
      "77\n",
      "encoder.layers.4.linear2.weight 74 torch.Size([8, 2048])\n",
      "78\n",
      "encoder.layers.4.linear2.bias 75 torch.Size([8])\n",
      "79\n",
      "encoder.layers.4.norm1.weight 76 torch.Size([8])\n",
      "80\n",
      "encoder.layers.4.norm1.bias 77 torch.Size([8])\n",
      "81\n",
      "encoder.layers.4.norm2.weight 78 torch.Size([8])\n",
      "82\n",
      "encoder.layers.4.norm2.bias 79 torch.Size([8])\n",
      "83\n",
      "0q_0encoder.layers.5.self_attn.in_proj_weight 80 torch.Size([8, 8])\n",
      "84\n",
      "0q_1encoder.layers.5.self_attn.in_proj_bias 81 torch.Size([8])\n",
      "85\n",
      "1k_0encoder.layers.5.self_attn.in_proj_weight 82 torch.Size([8, 8])\n",
      "86\n",
      "1k_1encoder.layers.5.self_attn.in_proj_bias 83 torch.Size([8])\n",
      "87\n",
      "2v_0encoder.layers.5.self_attn.in_proj_weight 84 torch.Size([8, 8])\n",
      "88\n",
      "2v_1encoder.layers.5.self_attn.in_proj_bias 85 torch.Size([8])\n",
      "89\n",
      "encoder.layers.5.self_attn.out_proj.weight 86 torch.Size([8, 8])\n",
      "90\n",
      "encoder.layers.5.self_attn.out_proj.bias 87 torch.Size([8])\n",
      "91\n",
      "encoder.layers.5.linear1.weight 88 torch.Size([2048, 8])\n",
      "92\n",
      "encoder.layers.5.linear1.bias 89 torch.Size([2048])\n",
      "93\n",
      "encoder.layers.5.linear2.weight 90 torch.Size([8, 2048])\n",
      "94\n",
      "encoder.layers.5.linear2.bias 91 torch.Size([8])\n",
      "95\n",
      "encoder.layers.5.norm1.weight 92 torch.Size([8])\n",
      "96\n",
      "encoder.layers.5.norm1.bias 93 torch.Size([8])\n",
      "97\n",
      "encoder.layers.5.norm2.weight 94 torch.Size([8])\n",
      "98\n",
      "encoder.layers.5.norm2.bias 95 torch.Size([8])\n",
      "99\n",
      "0q_0decoder.layers.0.self_attn.in_proj_weight 96 torch.Size([8, 8])\n",
      "100\n",
      "0q_1decoder.layers.0.self_attn.in_proj_bias 97 torch.Size([8])\n",
      "101\n",
      "1k_0decoder.layers.0.self_attn.in_proj_weight 98 torch.Size([8, 8])\n",
      "102\n",
      "1k_1decoder.layers.0.self_attn.in_proj_bias 99 torch.Size([8])\n",
      "103\n",
      "2v_0decoder.layers.0.self_attn.in_proj_weight 100 torch.Size([8, 8])\n",
      "104\n",
      "2v_1decoder.layers.0.self_attn.in_proj_bias 101 torch.Size([8])\n",
      "105\n",
      "decoder.layers.0.self_attn.out_proj.weight 102 torch.Size([8, 8])\n",
      "106\n",
      "decoder.layers.0.self_attn.out_proj.bias 103 torch.Size([8])\n",
      "107\n",
      "0q_0decoder.layers.0.multihead_attn.in_proj_weight 104 torch.Size([8, 8])\n",
      "108\n",
      "0q_1decoder.layers.0.multihead_attn.in_proj_bias 105 torch.Size([8])\n",
      "109\n",
      "1k_0decoder.layers.0.multihead_attn.in_proj_weight 106 torch.Size([8, 8])\n",
      "110\n",
      "1k_1decoder.layers.0.multihead_attn.in_proj_bias 107 torch.Size([8])\n",
      "111\n",
      "2v_0decoder.layers.0.multihead_attn.in_proj_weight 108 torch.Size([8, 8])\n",
      "112\n",
      "2v_1decoder.layers.0.multihead_attn.in_proj_bias 109 torch.Size([8])\n",
      "113\n",
      "decoder.layers.0.multihead_attn.out_proj.weight 110 torch.Size([8, 8])\n",
      "114\n",
      "decoder.layers.0.multihead_attn.out_proj.bias 111 torch.Size([8])\n",
      "115\n",
      "decoder.layers.0.linear1.weight 112 torch.Size([2048, 8])\n",
      "116\n",
      "decoder.layers.0.linear1.bias 113 torch.Size([2048])\n",
      "117\n",
      "decoder.layers.0.linear2.weight 114 torch.Size([8, 2048])\n",
      "118\n",
      "decoder.layers.0.linear2.bias 115 torch.Size([8])\n",
      "119\n",
      "decoder.layers.0.norm1.weight 116 torch.Size([8])\n",
      "120\n",
      "decoder.layers.0.norm1.bias 117 torch.Size([8])\n",
      "121\n",
      "decoder.layers.0.norm2.weight 118 torch.Size([8])\n",
      "122\n",
      "decoder.layers.0.norm2.bias 119 torch.Size([8])\n",
      "123\n",
      "decoder.layers.0.norm3.weight 120 torch.Size([8])\n",
      "124\n",
      "decoder.layers.0.norm3.bias 121 torch.Size([8])\n",
      "125\n",
      "0q_0decoder.layers.1.self_attn.in_proj_weight 122 torch.Size([8, 8])\n",
      "126\n",
      "0q_1decoder.layers.1.self_attn.in_proj_bias 123 torch.Size([8])\n",
      "127\n",
      "1k_0decoder.layers.1.self_attn.in_proj_weight 124 torch.Size([8, 8])\n",
      "128\n",
      "1k_1decoder.layers.1.self_attn.in_proj_bias 125 torch.Size([8])\n",
      "129\n",
      "2v_0decoder.layers.1.self_attn.in_proj_weight 126 torch.Size([8, 8])\n",
      "130\n",
      "2v_1decoder.layers.1.self_attn.in_proj_bias 127 torch.Size([8])\n",
      "131\n",
      "decoder.layers.1.self_attn.out_proj.weight 128 torch.Size([8, 8])\n",
      "132\n",
      "decoder.layers.1.self_attn.out_proj.bias 129 torch.Size([8])\n",
      "133\n",
      "0q_0decoder.layers.1.multihead_attn.in_proj_weight 130 torch.Size([8, 8])\n",
      "134\n",
      "0q_1decoder.layers.1.multihead_attn.in_proj_bias 131 torch.Size([8])\n",
      "135\n",
      "1k_0decoder.layers.1.multihead_attn.in_proj_weight 132 torch.Size([8, 8])\n",
      "136\n",
      "1k_1decoder.layers.1.multihead_attn.in_proj_bias 133 torch.Size([8])\n",
      "137\n",
      "2v_0decoder.layers.1.multihead_attn.in_proj_weight 134 torch.Size([8, 8])\n",
      "138\n",
      "2v_1decoder.layers.1.multihead_attn.in_proj_bias 135 torch.Size([8])\n",
      "139\n",
      "decoder.layers.1.multihead_attn.out_proj.weight 136 torch.Size([8, 8])\n",
      "140\n",
      "decoder.layers.1.multihead_attn.out_proj.bias 137 torch.Size([8])\n",
      "141\n",
      "decoder.layers.1.linear1.weight 138 torch.Size([2048, 8])\n",
      "142\n",
      "decoder.layers.1.linear1.bias 139 torch.Size([2048])\n",
      "143\n",
      "decoder.layers.1.linear2.weight 140 torch.Size([8, 2048])\n",
      "144\n",
      "decoder.layers.1.linear2.bias 141 torch.Size([8])\n",
      "145\n",
      "decoder.layers.1.norm1.weight 142 torch.Size([8])\n",
      "146\n",
      "decoder.layers.1.norm1.bias 143 torch.Size([8])\n",
      "147\n",
      "decoder.layers.1.norm2.weight 144 torch.Size([8])\n",
      "148\n",
      "decoder.layers.1.norm2.bias 145 torch.Size([8])\n",
      "149\n",
      "decoder.layers.1.norm3.weight 146 torch.Size([8])\n",
      "150\n",
      "decoder.layers.1.norm3.bias 147 torch.Size([8])\n",
      "151\n",
      "0q_0decoder.layers.2.self_attn.in_proj_weight 148 torch.Size([8, 8])\n",
      "152\n",
      "0q_1decoder.layers.2.self_attn.in_proj_bias 149 torch.Size([8])\n",
      "153\n",
      "1k_0decoder.layers.2.self_attn.in_proj_weight 150 torch.Size([8, 8])\n",
      "154\n",
      "1k_1decoder.layers.2.self_attn.in_proj_bias 151 torch.Size([8])\n",
      "155\n",
      "2v_0decoder.layers.2.self_attn.in_proj_weight 152 torch.Size([8, 8])\n",
      "156\n",
      "2v_1decoder.layers.2.self_attn.in_proj_bias 153 torch.Size([8])\n",
      "157\n",
      "decoder.layers.2.self_attn.out_proj.weight 154 torch.Size([8, 8])\n",
      "158\n",
      "decoder.layers.2.self_attn.out_proj.bias 155 torch.Size([8])\n",
      "159\n",
      "0q_0decoder.layers.2.multihead_attn.in_proj_weight 156 torch.Size([8, 8])\n",
      "160\n",
      "0q_1decoder.layers.2.multihead_attn.in_proj_bias 157 torch.Size([8])\n",
      "161\n",
      "1k_0decoder.layers.2.multihead_attn.in_proj_weight 158 torch.Size([8, 8])\n",
      "162\n",
      "1k_1decoder.layers.2.multihead_attn.in_proj_bias 159 torch.Size([8])\n",
      "163\n",
      "2v_0decoder.layers.2.multihead_attn.in_proj_weight 160 torch.Size([8, 8])\n",
      "164\n",
      "2v_1decoder.layers.2.multihead_attn.in_proj_bias 161 torch.Size([8])\n",
      "165\n",
      "decoder.layers.2.multihead_attn.out_proj.weight 162 torch.Size([8, 8])\n",
      "166\n",
      "decoder.layers.2.multihead_attn.out_proj.bias 163 torch.Size([8])\n",
      "167\n",
      "decoder.layers.2.linear1.weight 164 torch.Size([2048, 8])\n",
      "168\n",
      "decoder.layers.2.linear1.bias 165 torch.Size([2048])\n",
      "169\n",
      "decoder.layers.2.linear2.weight 166 torch.Size([8, 2048])\n",
      "170\n",
      "decoder.layers.2.linear2.bias 167 torch.Size([8])\n",
      "171\n",
      "decoder.layers.2.norm1.weight 168 torch.Size([8])\n",
      "172\n",
      "decoder.layers.2.norm1.bias 169 torch.Size([8])\n",
      "173\n",
      "decoder.layers.2.norm2.weight 170 torch.Size([8])\n",
      "174\n",
      "decoder.layers.2.norm2.bias 171 torch.Size([8])\n",
      "175\n",
      "decoder.layers.2.norm3.weight 172 torch.Size([8])\n",
      "176\n",
      "decoder.layers.2.norm3.bias 173 torch.Size([8])\n",
      "177\n",
      "0q_0decoder.layers.3.self_attn.in_proj_weight 174 torch.Size([8, 8])\n",
      "178\n",
      "0q_1decoder.layers.3.self_attn.in_proj_bias 175 torch.Size([8])\n",
      "179\n",
      "1k_0decoder.layers.3.self_attn.in_proj_weight 176 torch.Size([8, 8])\n",
      "180\n",
      "1k_1decoder.layers.3.self_attn.in_proj_bias 177 torch.Size([8])\n",
      "181\n",
      "2v_0decoder.layers.3.self_attn.in_proj_weight 178 torch.Size([8, 8])\n",
      "182\n",
      "2v_1decoder.layers.3.self_attn.in_proj_bias 179 torch.Size([8])\n",
      "183\n",
      "decoder.layers.3.self_attn.out_proj.weight 180 torch.Size([8, 8])\n",
      "184\n",
      "decoder.layers.3.self_attn.out_proj.bias 181 torch.Size([8])\n",
      "185\n",
      "0q_0decoder.layers.3.multihead_attn.in_proj_weight 182 torch.Size([8, 8])\n",
      "186\n",
      "0q_1decoder.layers.3.multihead_attn.in_proj_bias 183 torch.Size([8])\n",
      "187\n",
      "1k_0decoder.layers.3.multihead_attn.in_proj_weight 184 torch.Size([8, 8])\n",
      "188\n",
      "1k_1decoder.layers.3.multihead_attn.in_proj_bias 185 torch.Size([8])\n",
      "189\n",
      "2v_0decoder.layers.3.multihead_attn.in_proj_weight 186 torch.Size([8, 8])\n",
      "190\n",
      "2v_1decoder.layers.3.multihead_attn.in_proj_bias 187 torch.Size([8])\n",
      "191\n",
      "decoder.layers.3.multihead_attn.out_proj.weight 188 torch.Size([8, 8])\n",
      "192\n",
      "decoder.layers.3.multihead_attn.out_proj.bias 189 torch.Size([8])\n",
      "193\n",
      "decoder.layers.3.linear1.weight 190 torch.Size([2048, 8])\n",
      "194\n",
      "decoder.layers.3.linear1.bias 191 torch.Size([2048])\n",
      "195\n",
      "decoder.layers.3.linear2.weight 192 torch.Size([8, 2048])\n",
      "196\n",
      "decoder.layers.3.linear2.bias 193 torch.Size([8])\n",
      "197\n",
      "decoder.layers.3.norm1.weight 194 torch.Size([8])\n",
      "198\n",
      "decoder.layers.3.norm1.bias 195 torch.Size([8])\n",
      "199\n",
      "decoder.layers.3.norm2.weight 196 torch.Size([8])\n",
      "200\n",
      "decoder.layers.3.norm2.bias 197 torch.Size([8])\n",
      "201\n",
      "decoder.layers.3.norm3.weight 198 torch.Size([8])\n",
      "202\n",
      "decoder.layers.3.norm3.bias 199 torch.Size([8])\n",
      "203\n",
      "0q_0decoder.layers.4.self_attn.in_proj_weight 200 torch.Size([8, 8])\n",
      "204\n",
      "0q_1decoder.layers.4.self_attn.in_proj_bias 201 torch.Size([8])\n",
      "205\n",
      "1k_0decoder.layers.4.self_attn.in_proj_weight 202 torch.Size([8, 8])\n",
      "206\n",
      "1k_1decoder.layers.4.self_attn.in_proj_bias 203 torch.Size([8])\n",
      "207\n",
      "2v_0decoder.layers.4.self_attn.in_proj_weight 204 torch.Size([8, 8])\n",
      "208\n",
      "2v_1decoder.layers.4.self_attn.in_proj_bias 205 torch.Size([8])\n",
      "209\n",
      "decoder.layers.4.self_attn.out_proj.weight 206 torch.Size([8, 8])\n",
      "210\n",
      "decoder.layers.4.self_attn.out_proj.bias 207 torch.Size([8])\n",
      "211\n",
      "0q_0decoder.layers.4.multihead_attn.in_proj_weight 208 torch.Size([8, 8])\n",
      "212\n",
      "0q_1decoder.layers.4.multihead_attn.in_proj_bias 209 torch.Size([8])\n",
      "213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1k_0decoder.layers.4.multihead_attn.in_proj_weight 210 torch.Size([8, 8])\n",
      "214\n",
      "1k_1decoder.layers.4.multihead_attn.in_proj_bias 211 torch.Size([8])\n",
      "215\n",
      "2v_0decoder.layers.4.multihead_attn.in_proj_weight 212 torch.Size([8, 8])\n",
      "216\n",
      "2v_1decoder.layers.4.multihead_attn.in_proj_bias 213 torch.Size([8])\n",
      "217\n",
      "decoder.layers.4.multihead_attn.out_proj.weight 214 torch.Size([8, 8])\n",
      "218\n",
      "decoder.layers.4.multihead_attn.out_proj.bias 215 torch.Size([8])\n",
      "219\n",
      "decoder.layers.4.linear1.weight 216 torch.Size([2048, 8])\n",
      "220\n",
      "decoder.layers.4.linear1.bias 217 torch.Size([2048])\n",
      "221\n",
      "decoder.layers.4.linear2.weight 218 torch.Size([8, 2048])\n",
      "222\n",
      "decoder.layers.4.linear2.bias 219 torch.Size([8])\n",
      "223\n",
      "decoder.layers.4.norm1.weight 220 torch.Size([8])\n",
      "224\n",
      "decoder.layers.4.norm1.bias 221 torch.Size([8])\n",
      "225\n",
      "decoder.layers.4.norm2.weight 222 torch.Size([8])\n",
      "226\n",
      "decoder.layers.4.norm2.bias 223 torch.Size([8])\n",
      "227\n",
      "decoder.layers.4.norm3.weight 224 torch.Size([8])\n",
      "228\n",
      "decoder.layers.4.norm3.bias 225 torch.Size([8])\n",
      "229\n",
      "0q_0decoder.layers.5.self_attn.in_proj_weight 226 torch.Size([8, 8])\n",
      "230\n",
      "0q_1decoder.layers.5.self_attn.in_proj_bias 227 torch.Size([8])\n",
      "231\n",
      "1k_0decoder.layers.5.self_attn.in_proj_weight 228 torch.Size([8, 8])\n",
      "232\n",
      "1k_1decoder.layers.5.self_attn.in_proj_bias 229 torch.Size([8])\n",
      "233\n",
      "2v_0decoder.layers.5.self_attn.in_proj_weight 230 torch.Size([8, 8])\n",
      "234\n",
      "2v_1decoder.layers.5.self_attn.in_proj_bias 231 torch.Size([8])\n",
      "235\n",
      "decoder.layers.5.self_attn.out_proj.weight 232 torch.Size([8, 8])\n",
      "236\n",
      "decoder.layers.5.self_attn.out_proj.bias 233 torch.Size([8])\n",
      "237\n",
      "0q_0decoder.layers.5.multihead_attn.in_proj_weight 234 torch.Size([8, 8])\n",
      "238\n",
      "0q_1decoder.layers.5.multihead_attn.in_proj_bias 235 torch.Size([8])\n",
      "239\n",
      "1k_0decoder.layers.5.multihead_attn.in_proj_weight 236 torch.Size([8, 8])\n",
      "240\n",
      "1k_1decoder.layers.5.multihead_attn.in_proj_bias 237 torch.Size([8])\n",
      "241\n",
      "2v_0decoder.layers.5.multihead_attn.in_proj_weight 238 torch.Size([8, 8])\n",
      "242\n",
      "2v_1decoder.layers.5.multihead_attn.in_proj_bias 239 torch.Size([8])\n",
      "243\n",
      "decoder.layers.5.multihead_attn.out_proj.weight 240 torch.Size([8, 8])\n",
      "244\n",
      "decoder.layers.5.multihead_attn.out_proj.bias 241 torch.Size([8])\n",
      "245\n",
      "decoder.layers.5.linear1.weight 242 torch.Size([2048, 8])\n",
      "246\n",
      "decoder.layers.5.linear1.bias 243 torch.Size([2048])\n",
      "247\n",
      "decoder.layers.5.linear2.weight 244 torch.Size([8, 2048])\n",
      "248\n",
      "decoder.layers.5.linear2.bias 245 torch.Size([8])\n",
      "249\n",
      "decoder.layers.5.norm1.weight 246 torch.Size([8])\n",
      "250\n",
      "decoder.layers.5.norm1.bias 247 torch.Size([8])\n",
      "251\n",
      "decoder.layers.5.norm2.weight 248 torch.Size([8])\n",
      "252\n",
      "decoder.layers.5.norm2.bias 249 torch.Size([8])\n",
      "253\n",
      "decoder.layers.5.norm3.weight 250 torch.Size([8])\n",
      "254\n",
      "decoder.layers.5.norm3.bias 251 torch.Size([8])\n",
      "255\n",
      "decoder.norm.weight 252 torch.Size([8])\n",
      "256\n",
      "decoder.norm.bias 253 torch.Size([8])\n",
      "257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.transformer import *\n",
    "\n",
    "\n",
    "from models.backbone import *\n",
    "filepath = '/private/home/padentomasello/scratch/pytorch_testing/transformer.array'\n",
    "\n",
    "N = 2\n",
    "C = 8\n",
    "H = 3\n",
    "W = 3\n",
    "\n",
    "embedding_size = 8\n",
    "tgt_len = 10\n",
    "\n",
    "queries = torch.rand(tgt_len, embedding_size)\n",
    "src = torch.rand(N, C, H, W)\n",
    "pos = torch.rand(N, C, H, W)\n",
    "mask = torch.zeros(N, H, W)\n",
    "#mask[0, :20, :20] = 1\n",
    "##mask[1, :4, :10] = 1\n",
    "\n",
    "\n",
    "\n",
    "af.array.save_array('src', toArrayFire(src), filepath, False)\n",
    "af.array.save_array('queries', toArrayFire(queries), filepath, True)\n",
    "af.array.save_array('mask', toArrayFire(mask), filepath, True)\n",
    "af.array.save_array('pos', toArrayFire(pos), filepath, True)\n",
    "       \n",
    "\n",
    "\n",
    "model.eval()\n",
    "output = model(src, mask.to(bool), queries, pos)[0]\n",
    "saveStateDict(model, filepath)\n",
    "af.array.save_array('output', toArrayFire(output), filepath, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.1497e+00, -1.8422e+00,  5.3584e-01, -4.2837e-01,  8.7465e-01,\n",
       "            2.3533e-01,  1.3450e+00,  4.2949e-01],\n",
       "          [-1.1422e+00, -1.8410e+00,  5.3973e-01, -4.3984e-01,  8.7021e-01,\n",
       "            2.3584e-01,  1.3518e+00,  4.2544e-01],\n",
       "          [-1.1505e+00, -1.8423e+00,  5.3595e-01, -4.2602e-01,  8.7637e-01,\n",
       "            2.3130e-01,  1.3438e+00,  4.3137e-01],\n",
       "          [-1.1422e+00, -1.8419e+00,  5.3946e-01, -4.3884e-01,  8.7005e-01,\n",
       "            2.3922e-01,  1.3514e+00,  4.2271e-01],\n",
       "          [-1.1407e+00, -1.8411e+00,  5.4150e-01, -4.4315e-01,  8.6841e-01,\n",
       "            2.3937e-01,  1.3524e+00,  4.2325e-01],\n",
       "          [-1.1475e+00, -1.8424e+00,  5.3804e-01, -4.3271e-01,  8.7312e-01,\n",
       "            2.3756e-01,  1.3453e+00,  4.2848e-01],\n",
       "          [-1.1424e+00, -1.8434e+00,  5.3477e-01, -4.3430e-01,  8.7063e-01,\n",
       "            2.3132e-01,  1.3502e+00,  4.3316e-01],\n",
       "          [-1.1493e+00, -1.8426e+00,  5.3871e-01, -4.3077e-01,  8.7433e-01,\n",
       "            2.3917e-01,  1.3429e+00,  4.2760e-01],\n",
       "          [-1.1410e+00, -1.8424e+00,  5.3536e-01, -4.3586e-01,  8.7007e-01,\n",
       "            2.3110e-01,  1.3538e+00,  4.2889e-01],\n",
       "          [-1.1389e+00, -1.8446e+00,  5.3863e-01, -4.4010e-01,  8.6799e-01,\n",
       "            2.3836e-01,  1.3499e+00,  4.2879e-01]],\n",
       "\n",
       "         [[-1.0766e+00, -1.9275e+00,  6.1462e-01, -3.3902e-01,  9.3849e-01,\n",
       "            1.6567e-01,  1.2625e+00,  3.6188e-01],\n",
       "          [-1.0692e+00, -1.9267e+00,  6.2689e-01, -3.5211e-01,  9.3733e-01,\n",
       "            1.6925e-01,  1.2639e+00,  3.5065e-01],\n",
       "          [-1.0770e+00, -1.9267e+00,  6.1777e-01, -3.3976e-01,  9.4079e-01,\n",
       "            1.6508e-01,  1.2607e+00,  3.5915e-01],\n",
       "          [-1.0733e+00, -1.9278e+00,  6.1492e-01, -3.4427e-01,  9.3510e-01,\n",
       "            1.6866e-01,  1.2656e+00,  3.6104e-01],\n",
       "          [-1.0714e+00, -1.9252e+00,  6.2892e-01, -3.5547e-01,  9.3699e-01,\n",
       "            1.7627e-01,  1.2625e+00,  3.4744e-01],\n",
       "          [-1.0737e+00, -1.9268e+00,  6.2128e-01, -3.4438e-01,  9.3807e-01,\n",
       "            1.6589e-01,  1.2630e+00,  3.5673e-01],\n",
       "          [-1.0664e+00, -1.9291e+00,  6.2096e-01, -3.4681e-01,  9.3656e-01,\n",
       "            1.6104e-01,  1.2670e+00,  3.5676e-01],\n",
       "          [-1.0779e+00, -1.9260e+00,  6.1861e-01, -3.4173e-01,  9.3891e-01,\n",
       "            1.6871e-01,  1.2613e+00,  3.5806e-01],\n",
       "          [-1.0696e+00, -1.9289e+00,  6.1515e-01, -3.4507e-01,  9.3622e-01,\n",
       "            1.6609e-01,  1.2665e+00,  3.5967e-01],\n",
       "          [-1.0693e+00, -1.9275e+00,  6.2066e-01, -3.4930e-01,  9.3509e-01,\n",
       "            1.6818e-01,  1.2669e+00,  3.5524e-01]]],\n",
       "\n",
       "\n",
       "        [[[-8.4279e-01, -1.2805e+00,  7.9232e-01, -3.0369e-02,  1.3060e+00,\n",
       "           -5.5145e-01,  1.4924e+00, -8.8561e-01],\n",
       "          [-8.2970e-01, -1.2878e+00,  7.8971e-01, -5.2299e-02,  1.2979e+00,\n",
       "           -5.4248e-01,  1.5066e+00, -8.8198e-01],\n",
       "          [-8.3488e-01, -1.2857e+00,  7.7999e-01, -3.3482e-02,  1.3080e+00,\n",
       "           -5.4762e-01,  1.4988e+00, -8.8505e-01],\n",
       "          [-8.3935e-01, -1.2756e+00,  8.0139e-01, -3.7020e-02,  1.3007e+00,\n",
       "           -5.5429e-01,  1.4943e+00, -8.9014e-01],\n",
       "          [-8.3826e-01, -1.2908e+00,  7.9910e-01, -5.5639e-02,  1.2997e+00,\n",
       "           -5.4233e-01,  1.4992e+00, -8.7089e-01],\n",
       "          [-8.4320e-01, -1.2831e+00,  7.9482e-01, -3.6655e-02,  1.3063e+00,\n",
       "           -5.4995e-01,  1.4920e+00, -8.8014e-01],\n",
       "          [-8.2724e-01, -1.2827e+00,  7.8505e-01, -3.9633e-02,  1.2986e+00,\n",
       "           -5.5161e-01,  1.5069e+00, -8.8942e-01],\n",
       "          [-8.4726e-01, -1.2850e+00,  7.9349e-01, -3.5119e-02,  1.3117e+00,\n",
       "           -5.4900e-01,  1.4870e+00, -8.7572e-01],\n",
       "          [-8.2105e-01, -1.2809e+00,  7.7996e-01, -4.1690e-02,  1.2975e+00,\n",
       "           -5.4964e-01,  1.5120e+00, -8.9617e-01],\n",
       "          [-8.3728e-01, -1.2868e+00,  7.9842e-01, -4.4481e-02,  1.3015e+00,\n",
       "           -5.5159e-01,  1.4961e+00, -8.7589e-01]],\n",
       "\n",
       "         [[-8.8541e-01, -1.4073e+00,  9.5507e-01,  2.5998e-01,  1.3532e+00,\n",
       "           -4.6087e-01,  1.1403e+00, -9.5499e-01],\n",
       "          [-8.8121e-01, -1.4095e+00,  9.6357e-01,  2.5353e-01,  1.3494e+00,\n",
       "           -4.6020e-01,  1.1396e+00, -9.5529e-01],\n",
       "          [-8.8520e-01, -1.4108e+00,  9.4890e-01,  2.5456e-01,  1.3605e+00,\n",
       "           -4.6226e-01,  1.1405e+00, -9.4620e-01],\n",
       "          [-8.7999e-01, -1.4064e+00,  9.5707e-01,  2.5559e-01,  1.3472e+00,\n",
       "           -4.5919e-01,  1.1472e+00, -9.6144e-01],\n",
       "          [-8.7729e-01, -1.4087e+00,  9.6373e-01,  2.4356e-01,  1.3472e+00,\n",
       "           -4.4992e-01,  1.1452e+00, -9.6380e-01],\n",
       "          [-8.7980e-01, -1.4078e+00,  9.5568e-01,  2.5325e-01,  1.3517e+00,\n",
       "           -4.6151e-01,  1.1451e+00, -9.5664e-01],\n",
       "          [-8.8220e-01, -1.4049e+00,  9.6659e-01,  2.6285e-01,  1.3474e+00,\n",
       "           -4.7128e-01,  1.1373e+00, -9.5570e-01],\n",
       "          [-8.7894e-01, -1.4110e+00,  9.4576e-01,  2.4714e-01,  1.3565e+00,\n",
       "           -4.5598e-01,  1.1503e+00, -9.5383e-01],\n",
       "          [-8.8550e-01, -1.4070e+00,  9.6193e-01,  2.6064e-01,  1.3507e+00,\n",
       "           -4.6463e-01,  1.1373e+00, -9.5351e-01],\n",
       "          [-8.7530e-01, -1.4086e+00,  9.5707e-01,  2.5106e-01,  1.3460e+00,\n",
       "           -4.6077e-01,  1.1509e+00, -9.6035e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.6640e-01, -5.6212e-01,  3.8612e-01, -6.4433e-01,  1.1594e+00,\n",
       "           -1.8003e+00,  1.5607e+00, -2.6590e-01],\n",
       "          [ 1.7870e-01, -5.6782e-01,  3.8656e-01, -6.6104e-01,  1.1457e+00,\n",
       "           -1.7912e+00,  1.5714e+00, -2.6227e-01],\n",
       "          [ 1.7516e-01, -5.6503e-01,  3.7879e-01, -6.4733e-01,  1.1593e+00,\n",
       "           -1.7962e+00,  1.5637e+00, -2.6837e-01],\n",
       "          [ 1.6854e-01, -5.5746e-01,  3.9146e-01, -6.4963e-01,  1.1566e+00,\n",
       "           -1.8007e+00,  1.5598e+00, -2.6868e-01],\n",
       "          [ 1.7317e-01, -5.7073e-01,  3.9348e-01, -6.6353e-01,  1.1477e+00,\n",
       "           -1.7922e+00,  1.5668e+00, -2.5468e-01],\n",
       "          [ 1.6777e-01, -5.6463e-01,  3.8983e-01, -6.4924e-01,  1.1552e+00,\n",
       "           -1.7994e+00,  1.5617e+00, -2.6132e-01],\n",
       "          [ 1.8092e-01, -5.6425e-01,  3.8367e-01, -6.5196e-01,  1.1467e+00,\n",
       "           -1.7972e+00,  1.5686e+00, -2.6646e-01],\n",
       "          [ 1.6499e-01, -5.6531e-01,  3.8832e-01, -6.4844e-01,  1.1615e+00,\n",
       "           -1.7993e+00,  1.5582e+00, -2.5998e-01],\n",
       "          [ 1.8488e-01, -5.6148e-01,  3.7827e-01, -6.5427e-01,  1.1508e+00,\n",
       "           -1.7942e+00,  1.5689e+00, -2.7293e-01],\n",
       "          [ 1.7338e-01, -5.6640e-01,  3.9206e-01, -6.5522e-01,  1.1504e+00,\n",
       "           -1.7984e+00,  1.5626e+00, -2.5835e-01]],\n",
       "\n",
       "         [[ 7.6854e-03, -7.6250e-01,  5.8698e-01, -1.7174e-01,  1.2018e+00,\n",
       "           -1.9264e+00,  1.3454e+00, -2.8125e-01],\n",
       "          [ 8.4218e-03, -7.6558e-01,  5.9407e-01, -1.7357e-01,  1.1979e+00,\n",
       "           -1.9256e+00,  1.3451e+00, -2.8073e-01],\n",
       "          [ 5.9455e-03, -7.6473e-01,  5.8288e-01, -1.7519e-01,  1.2029e+00,\n",
       "           -1.9263e+00,  1.3465e+00, -2.7207e-01],\n",
       "          [ 1.1325e-02, -7.5952e-01,  5.9086e-01, -1.7308e-01,  1.1966e+00,\n",
       "           -1.9247e+00,  1.3500e+00, -2.9158e-01],\n",
       "          [ 1.2014e-02, -7.6661e-01,  5.9684e-01, -1.8336e-01,  1.1974e+00,\n",
       "           -1.9186e+00,  1.3507e+00, -2.8827e-01],\n",
       "          [ 9.1735e-03, -7.6428e-01,  5.8408e-01, -1.7394e-01,  1.1991e+00,\n",
       "           -1.9250e+00,  1.3502e+00, -2.7925e-01],\n",
       "          [ 1.0124e-02, -7.5995e-01,  5.9560e-01, -1.6552e-01,  1.1939e+00,\n",
       "           -1.9327e+00,  1.3414e+00, -2.8283e-01],\n",
       "          [ 8.7501e-03, -7.6644e-01,  5.7722e-01, -1.7936e-01,  1.2019e+00,\n",
       "           -1.9206e+00,  1.3554e+00, -2.7681e-01],\n",
       "          [ 9.1154e-03, -7.5833e-01,  5.9971e-01, -1.6869e-01,  1.1960e+00,\n",
       "           -1.9298e+00,  1.3411e+00, -2.8908e-01],\n",
       "          [ 1.3897e-02, -7.6376e-01,  5.8731e-01, -1.7622e-01,  1.1941e+00,\n",
       "           -1.9240e+00,  1.3534e+00, -2.8466e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 4.6993e-01, -8.7626e-01,  1.4436e-01, -4.7653e-01,  1.6455e+00,\n",
       "           -1.8900e+00,  4.8232e-01,  5.0068e-01],\n",
       "          [ 4.8383e-01, -8.8735e-01,  1.4529e-01, -5.1252e-01,  1.6448e+00,\n",
       "           -1.8690e+00,  4.7356e-01,  5.2134e-01],\n",
       "          [ 4.8301e-01, -8.7329e-01,  1.2749e-01, -4.9641e-01,  1.6579e+00,\n",
       "           -1.8748e+00,  4.6288e-01,  5.1322e-01],\n",
       "          [ 4.6311e-01, -8.7908e-01,  1.5531e-01, -4.6721e-01,  1.6429e+00,\n",
       "           -1.8950e+00,  4.9227e-01,  4.8768e-01],\n",
       "          [ 4.7462e-01, -8.9559e-01,  1.5951e-01, -5.2148e-01,  1.6509e+00,\n",
       "           -1.8590e+00,  4.5709e-01,  5.3392e-01],\n",
       "          [ 4.6840e-01, -8.8262e-01,  1.5113e-01, -4.8975e-01,  1.6469e+00,\n",
       "           -1.8811e+00,  4.7295e-01,  5.1412e-01],\n",
       "          [ 4.8048e-01, -8.8809e-01,  1.4459e-01, -5.0834e-01,  1.6496e+00,\n",
       "           -1.8679e+00,  4.7026e-01,  5.1930e-01],\n",
       "          [ 4.6503e-01, -8.8123e-01,  1.4789e-01, -4.8525e-01,  1.6556e+00,\n",
       "           -1.8789e+00,  4.6378e-01,  5.1309e-01],\n",
       "          [ 4.8593e-01, -8.8316e-01,  1.3403e-01, -5.0332e-01,  1.6594e+00,\n",
       "           -1.8666e+00,  4.6831e-01,  5.0546e-01],\n",
       "          [ 4.6596e-01, -8.9640e-01,  1.6350e-01, -4.9582e-01,  1.6461e+00,\n",
       "           -1.8727e+00,  4.7292e-01,  5.1643e-01]],\n",
       "\n",
       "         [[-1.2743e-03, -1.1377e+00,  6.7835e-01,  6.2445e-01,  1.5926e+00,\n",
       "           -1.8085e+00, -1.2749e-01,  1.7956e-01],\n",
       "          [-3.9224e-03, -1.1401e+00,  6.7982e-01,  6.2561e-01,  1.5931e+00,\n",
       "           -1.8040e+00, -1.3751e-01,  1.8702e-01],\n",
       "          [-1.7773e-03, -1.1369e+00,  6.6871e-01,  6.1504e-01,  1.6018e+00,\n",
       "           -1.8054e+00, -1.3668e-01,  1.9518e-01],\n",
       "          [-3.1085e-03, -1.1358e+00,  6.8466e-01,  6.2727e-01,  1.5888e+00,\n",
       "           -1.8117e+00, -1.1599e-01,  1.6592e-01],\n",
       "          [-1.1152e-03, -1.1476e+00,  6.8925e-01,  6.1447e-01,  1.5954e+00,\n",
       "           -1.7978e+00, -1.3704e-01,  1.8447e-01],\n",
       "          [-3.3033e-03, -1.1427e+00,  6.7419e-01,  6.2840e-01,  1.5924e+00,\n",
       "           -1.8046e+00, -1.3171e-01,  1.8738e-01],\n",
       "          [-2.2407e-03, -1.1410e+00,  6.8629e-01,  6.2449e-01,  1.5873e+00,\n",
       "           -1.8060e+00, -1.3934e-01,  1.9048e-01],\n",
       "          [-2.9176e-03, -1.1443e+00,  6.6897e-01,  6.2250e-01,  1.5985e+00,\n",
       "           -1.8024e+00, -1.2849e-01,  1.8809e-01],\n",
       "          [-1.2106e-03, -1.1339e+00,  6.9060e-01,  6.1270e-01,  1.5939e+00,\n",
       "           -1.8088e+00, -1.3233e-01,  1.7911e-01],\n",
       "          [-9.7451e-04, -1.1500e+00,  6.8683e-01,  6.2402e-01,  1.5867e+00,\n",
       "           -1.8027e+00, -1.2583e-01,  1.8200e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0313e+00, -1.6263e+00,  4.8658e-01,  5.1664e-02,  1.2361e+00,\n",
       "           -1.5647e+00,  1.6383e-01,  2.2161e-01],\n",
       "          [ 1.0426e+00, -1.6279e+00,  5.0414e-01,  7.0661e-03,  1.2420e+00,\n",
       "           -1.5459e+00,  1.4040e-01,  2.3767e-01],\n",
       "          [ 1.0473e+00, -1.6289e+00,  4.7661e-01,  3.3562e-02,  1.2436e+00,\n",
       "           -1.5491e+00,  1.4291e-01,  2.3400e-01],\n",
       "          [ 1.0220e+00, -1.6262e+00,  4.9436e-01,  6.4725e-02,  1.2326e+00,\n",
       "           -1.5712e+00,  1.7395e-01,  2.0977e-01],\n",
       "          [ 1.0327e+00, -1.6339e+00,  5.0946e-01,  1.1012e-02,  1.2443e+00,\n",
       "           -1.5420e+00,  1.3214e-01,  2.4638e-01],\n",
       "          [ 1.0312e+00, -1.6378e+00,  4.9030e-01,  3.3551e-02,  1.2385e+00,\n",
       "           -1.5488e+00,  1.5920e-01,  2.3389e-01],\n",
       "          [ 1.0399e+00, -1.6342e+00,  4.9021e-01,  2.3806e-02,  1.2414e+00,\n",
       "           -1.5455e+00,  1.4918e-01,  2.3519e-01],\n",
       "          [ 1.0304e+00, -1.6388e+00,  4.8678e-01,  4.0704e-02,  1.2418e+00,\n",
       "           -1.5472e+00,  1.5191e-01,  2.3438e-01],\n",
       "          [ 1.0429e+00, -1.6213e+00,  4.8806e-01,  3.9917e-02,  1.2457e+00,\n",
       "           -1.5568e+00,  1.3933e-01,  2.2216e-01],\n",
       "          [ 1.0255e+00, -1.6407e+00,  5.0484e-01,  3.2871e-02,  1.2371e+00,\n",
       "           -1.5466e+00,  1.5354e-01,  2.3347e-01]],\n",
       "\n",
       "         [[ 3.7344e-01, -1.4107e+00,  1.6164e+00,  3.3425e-01,  9.4386e-01,\n",
       "           -1.4775e+00, -1.8424e-01, -1.9557e-01],\n",
       "          [ 3.7837e-01, -1.4151e+00,  1.6194e+00,  3.3863e-01,  9.3837e-01,\n",
       "           -1.4699e+00, -1.9530e-01, -1.9445e-01],\n",
       "          [ 3.8051e-01, -1.4126e+00,  1.6160e+00,  3.2814e-01,  9.4726e-01,\n",
       "           -1.4726e+00, -1.9540e-01, -1.9133e-01],\n",
       "          [ 3.7566e-01, -1.4091e+00,  1.6213e+00,  3.3388e-01,  9.3898e-01,\n",
       "           -1.4754e+00, -1.7801e-01, -2.0740e-01],\n",
       "          [ 3.8584e-01, -1.4203e+00,  1.6287e+00,  3.2702e-01,  9.3553e-01,\n",
       "           -1.4555e+00, -2.0113e-01, -2.0023e-01],\n",
       "          [ 3.7476e-01, -1.4138e+00,  1.6161e+00,  3.4067e-01,  9.4081e-01,\n",
       "           -1.4745e+00, -1.9064e-01, -1.9343e-01],\n",
       "          [ 3.8275e-01, -1.4149e+00,  1.6263e+00,  3.3462e-01,  9.3228e-01,\n",
       "           -1.4653e+00, -2.0098e-01, -1.9483e-01],\n",
       "          [ 3.7441e-01, -1.4146e+00,  1.6135e+00,  3.3527e-01,  9.4681e-01,\n",
       "           -1.4743e+00, -1.8718e-01, -1.9394e-01],\n",
       "          [ 3.8899e-01, -1.4126e+00,  1.6310e+00,  3.1969e-01,  9.3584e-01,\n",
       "           -1.4609e+00, -1.9676e-01, -2.0536e-01],\n",
       "          [ 3.7598e-01, -1.4170e+00,  1.6229e+00,  3.3504e-01,  9.3535e-01,\n",
       "           -1.4683e+00, -1.8644e-01, -1.9744e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 6.5358e-01, -5.6496e-01,  1.3986e+00,  4.3390e-02,  1.4879e+00,\n",
       "           -8.5036e-01, -1.1479e+00, -1.0203e+00],\n",
       "          [ 6.5587e-01, -5.5166e-01,  1.4055e+00,  2.0361e-02,  1.4841e+00,\n",
       "           -8.2810e-01, -1.1698e+00, -1.0163e+00],\n",
       "          [ 6.6671e-01, -5.7021e-01,  1.3938e+00,  3.3574e-02,  1.4875e+00,\n",
       "           -8.4130e-01, -1.1599e+00, -1.0102e+00],\n",
       "          [ 6.4278e-01, -5.6185e-01,  1.4012e+00,  4.9265e-02,  1.4900e+00,\n",
       "           -8.5204e-01, -1.1413e+00, -1.0280e+00],\n",
       "          [ 6.4735e-01, -5.5153e-01,  1.4079e+00,  2.2785e-02,  1.4857e+00,\n",
       "           -8.2206e-01, -1.1750e+00, -1.0151e+00],\n",
       "          [ 6.5300e-01, -5.7292e-01,  1.4022e+00,  3.3091e-02,  1.4894e+00,\n",
       "           -8.3720e-01, -1.1532e+00, -1.0143e+00],\n",
       "          [ 6.5541e-01, -5.5903e-01,  1.3990e+00,  2.9274e-02,  1.4888e+00,\n",
       "           -8.3037e-01, -1.1659e+00, -1.0172e+00],\n",
       "          [ 6.5417e-01, -5.7858e-01,  1.4008e+00,  3.5976e-02,  1.4898e+00,\n",
       "           -8.3858e-01, -1.1531e+00, -1.0105e+00],\n",
       "          [ 6.5505e-01, -5.4802e-01,  1.3942e+00,  3.7209e-02,  1.4886e+00,\n",
       "           -8.3771e-01, -1.1680e+00, -1.0214e+00],\n",
       "          [ 6.4491e-01, -5.6561e-01,  1.4080e+00,  3.3082e-02,  1.4870e+00,\n",
       "           -8.3055e-01, -1.1588e+00, -1.0181e+00]],\n",
       "\n",
       "         [[ 1.7025e-01, -1.2463e-02,  1.7583e+00,  1.9715e-01,  1.1096e+00,\n",
       "           -7.6261e-01, -1.2085e+00, -1.2518e+00],\n",
       "          [ 1.7644e-01, -1.3174e-02,  1.7545e+00,  2.0376e-01,  1.1055e+00,\n",
       "           -7.5431e-01, -1.2191e+00, -1.2536e+00],\n",
       "          [ 1.7069e-01, -1.3691e-02,  1.7581e+00,  1.8988e-01,  1.1138e+00,\n",
       "           -7.5767e-01, -1.2130e+00, -1.2481e+00],\n",
       "          [ 1.7050e-01, -1.3772e-02,  1.7610e+00,  1.9635e-01,  1.1072e+00,\n",
       "           -7.6254e-01, -1.2040e+00, -1.2547e+00],\n",
       "          [ 1.7990e-01, -1.5394e-02,  1.7582e+00,  1.9589e-01,  1.1027e+00,\n",
       "           -7.4463e-01, -1.2208e+00, -1.2558e+00],\n",
       "          [ 1.7064e-01, -1.7241e-02,  1.7589e+00,  2.0230e-01,  1.1071e+00,\n",
       "           -7.5782e-01, -1.2156e+00, -1.2483e+00],\n",
       "          [ 1.7916e-01, -1.3991e-02,  1.7579e+00,  2.0143e-01,  1.1006e+00,\n",
       "           -7.5054e-01, -1.2226e+00, -1.2520e+00],\n",
       "          [ 1.6684e-01, -1.9600e-02,  1.7608e+00,  1.9534e-01,  1.1126e+00,\n",
       "           -7.5900e-01, -1.2103e+00, -1.2467e+00],\n",
       "          [ 1.8211e-01, -1.1344e-02,  1.7585e+00,  1.8894e-01,  1.1047e+00,\n",
       "           -7.5254e-01, -1.2135e+00, -1.2568e+00],\n",
       "          [ 1.6977e-01, -1.7620e-02,  1.7623e+00,  1.9821e-01,  1.1050e+00,\n",
       "           -7.5344e-01, -1.2123e+00, -1.2519e+00]]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7, 7])\n",
      "torch.Size([2, 256, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "output = model(NestedTensor(x, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 7, 7])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2048, 7, 7])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0].tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
