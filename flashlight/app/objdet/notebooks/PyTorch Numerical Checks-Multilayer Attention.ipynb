{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arrayfire as af\n",
    "import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toArrayFire(x):\n",
    "    x_np = x.detach().contiguous().numpy()\n",
    "    shape = 1\n",
    "    if len(x_np.shape) == 0:\n",
    "        shape = (1,)\n",
    "    else:\n",
    "        shape = x_np.shape[::-1]\n",
    "    #print(shape)\n",
    "    afArray = af.Array(x_np.ctypes.data, shape, x_np.dtype.char)\n",
    "    return afArray\n",
    "\n",
    "def saveStateDict(module, filepath):\n",
    "    i = 0\n",
    "    for (name, param) in module.named_parameters():\n",
    "        #param = module.state_dict()[name]\n",
    "        print(name, \"\\t\", param.size())\n",
    "        if 'in_proj' in name:\n",
    "            print(param.shape)\n",
    "            q, k, v = param.chunk(3, dim=0)\n",
    "            print('in_proj!')\n",
    "            af.array.save_array(name + 'q', toArrayFire(q), filepath, True)\n",
    "            af.array.save_array(name + 'k', toArrayFire(k), filepath, True)\n",
    "            af.array.save_array(name + 'v', toArrayFire(k), filepath, True)\n",
    "            continue\n",
    "        if len(param.size()) > 0:\n",
    "            af_array = toArrayFire(param)\n",
    "            if 'fc' in name and 'weight' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            af.array.save_array(name, af_array, filepath, True)\n",
    "            i = i + 1\n",
    "    print(i)\n",
    "    for name in module.state_dict():\n",
    "        if 'running' in name:\n",
    "            print(name)\n",
    "            af_array = toArrayFire(module.state_dict()[name])\n",
    "            af.array.save_array(name, af_array, filepath + 'running', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.backbone import *\n",
    "from models.position_encoding import *\n",
    "from models.matcher import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.modules.linear import _LinearWithBias\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.init import constant_\n",
    "from torch.nn.init import xavier_normal_\n",
    "from torch.nn.parameter import Parameter\n",
    "#from .module import Module\n",
    "#from .. import functional as F\n",
    "\n",
    "class MultiheadAttention2(nn.Module):\n",
    "    r\"\"\"Allows the model to jointly attend to information\n",
    "    from different representation subspaces.\n",
    "    See reference: Attention Is All You Need\n",
    "\n",
    "    .. math::\n",
    "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
    "        \\text{where} head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "\n",
    "    Args:\n",
    "        embed_dim: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n",
    "        bias: add bias as module parameter. Default: True.\n",
    "        add_bias_kv: add bias to the key and value sequences at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        kdim: total number of features in key. Default: None.\n",
    "        vdim: total number of features in value. Default: None.\n",
    "\n",
    "        Note: if kdim and vdim are None, they will be set to embed_dim such that\n",
    "        query, key, and value have the same number of features.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "    \"\"\"\n",
    "    bias_k: Optional[torch.Tensor]\n",
    "    bias_v: Optional[torch.Tensor]\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None):\n",
    "        super(MultiheadAttention2, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        if self._qkv_same_embed_dim is False:\n",
    "            self.q_proj_weight = Parameter(torch.Tensor(embed_dim, embed_dim))\n",
    "            self.k_proj_weight = Parameter(torch.Tensor(embed_dim, self.kdim))\n",
    "            self.v_proj_weight = Parameter(torch.Tensor(embed_dim, self.vdim))\n",
    "            self.register_parameter('in_proj_weight', None)\n",
    "        else:\n",
    "            self.in_proj_weight = Parameter(torch.empty(3 * embed_dim, embed_dim))\n",
    "            self.register_parameter('q_proj_weight', None)\n",
    "            self.register_parameter('k_proj_weight', None)\n",
    "            self.register_parameter('v_proj_weight', None)\n",
    "\n",
    "        if bias:\n",
    "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim))\n",
    "        else:\n",
    "            self.register_parameter('in_proj_bias', None)\n",
    "        self.out_proj = _LinearWithBias(embed_dim, embed_dim)\n",
    "\n",
    "        if add_bias_kv:\n",
    "            self.bias_k = Parameter(torch.empty(1, 1, embed_dim))\n",
    "            self.bias_v = Parameter(torch.empty(1, 1, embed_dim))\n",
    "        else:\n",
    "            self.bias_k = self.bias_v = None\n",
    "\n",
    "        self.add_zero_attn = add_zero_attn\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        if self._qkv_same_embed_dim:\n",
    "            xavier_uniform_(self.in_proj_weight)\n",
    "        else:\n",
    "            xavier_uniform_(self.q_proj_weight)\n",
    "            xavier_uniform_(self.k_proj_weight)\n",
    "            xavier_uniform_(self.v_proj_weight)\n",
    "\n",
    "        if self.in_proj_bias is not None:\n",
    "            constant_(self.in_proj_bias, 0.)\n",
    "            constant_(self.out_proj.bias, 0.)\n",
    "        if self.bias_k is not None:\n",
    "            xavier_normal_(self.bias_k)\n",
    "        if self.bias_v is not None:\n",
    "            xavier_normal_(self.bias_v)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
    "        if '_qkv_same_embed_dim' not in state:\n",
    "            state['_qkv_same_embed_dim'] = True\n",
    "\n",
    "        super(MultiheadAttention, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None,\n",
    "                need_weights=True, attn_mask=None):\n",
    "        # type: (Tensor, Tensor, Tensor, Optional[Tensor], bool, Optional[Tensor]) -> Tuple[Tensor, Optional[Tensor]]\n",
    "        r\"\"\"\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. When given a binary mask and a value is True,\n",
    "            the corresponding value on the attention layer will be ignored. When given\n",
    "            a byte mask and a value is non-zero, the corresponding value on the attention\n",
    "            layer will be ignored\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
    "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
    "\n",
    "    Shape:\n",
    "        - Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
    "          If a ByteTensor is provided, the non-zero positions will be ignored while the position\n",
    "          with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n",
    "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked\n",
    "          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
    "          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
    "          is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "          is provided, it will be added to the attention weight.\n",
    "\n",
    "        - Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension.\n",
    "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
    "          L is the target sequence length, S is the source sequence length.\n",
    "        \"\"\"\n",
    "        if not self._qkv_same_embed_dim:\n",
    "            return multi_head_attention_forward2(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask, use_separate_proj_weight=True,\n",
    "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
    "                v_proj_weight=self.v_proj_weight)\n",
    "        else:\n",
    "            return multi_head_attention_forward2(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(input, p=0.5, training=True, inplace=False):\n",
    "    # type: (Tensor, float, bool, bool) -> Tensor\n",
    "    r\"\"\"\n",
    "    During training, randomly zeroes some of the elements of the input\n",
    "    tensor with probability :attr:`p` using samples from a Bernoulli\n",
    "    distribution.\n",
    "    See :class:`~torch.nn.Dropout` for details.\n",
    "    Args:\n",
    "        p: probability of an element to be zeroed. Default: 0.5\n",
    "        training: apply dropout if is ``True``. Default: ``True``\n",
    "        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n",
    "    \"\"\"\n",
    "    assert(p == 0.0)\n",
    "    return input\n",
    "    if p < 0. or p > 1.:\n",
    "        raise ValueError(\"dropout probability has to be between 0 and 1, \"\n",
    "                         \"but got {}\".format(p))\n",
    "    return (_VF.dropout_(input, p, training)\n",
    "            if inplace\n",
    "            else _VF.dropout(input, p, training))\n",
    "\n",
    "\n",
    "def softmax(input, dim=None, _stacklevel=3, dtype=None):\n",
    "    # type: (Tensor, Optional[int], int, Optional[int]) -> Tensor\n",
    "    r\"\"\"Applies a softmax function.\n",
    "    Softmax is defined as:\n",
    "    :math:`\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}`\n",
    "    It is applied to all slices along dim, and will re-scale them so that the elements\n",
    "    lie in the range `[0, 1]` and sum to 1.\n",
    "    See :class:`~torch.nn.Softmax` for more details.\n",
    "    Arguments:\n",
    "        input (Tensor): input\n",
    "        dim (int): A dimension along which softmax will be computed.\n",
    "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
    "          If specified, the input tensor is casted to :attr:`dtype` before the operation\n",
    "          is performed. This is useful for preventing data type overflows. Default: None.\n",
    "    .. note::\n",
    "        This function doesn't work directly with NLLLoss,\n",
    "        which expects the Log to be computed between the Softmax and itself.\n",
    "        Use log_softmax instead (it's faster and has better numerical properties).\n",
    "    \"\"\"\n",
    "    if dim is None:\n",
    "        dim = _get_softmax_dim('softmax', input.dim(), _stacklevel)\n",
    "    if dtype is None:\n",
    "        ret = input.softmax(dim)\n",
    "    else:\n",
    "        ret = input.softmax(dim, dtype=dtype)\n",
    "    return ret\n",
    "\n",
    "def linear(input, weight, bias=None):\n",
    "    # type: (Tensor, Tensor, Optional[Tensor]) -> Tensor\n",
    "    r\"\"\"\n",
    "    Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.\n",
    "    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *, in\\_features)` N is the batch size, `*` means any number of\n",
    "          additional dimensions\n",
    "        - Weight: :math:`(out\\_features, in\\_features)`\n",
    "        - Bias: :math:`(out\\_features)`\n",
    "        - Output: :math:`(N, *, out\\_features)`\n",
    "    \"\"\"\n",
    "    tens_ops = (input, weight)\n",
    "    if input.dim() == 2 and bias is not None:\n",
    "        # fused op is marginally faster\n",
    "        ret = torch.addmm(bias, input, weight.t())\n",
    "    else:\n",
    "        output = input.matmul(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias\n",
    "        ret = output\n",
    "    return ret\n",
    "\n",
    "test = 0\n",
    "def multi_head_attention_forward2(query: Tensor,\n",
    "                                 key: Tensor,\n",
    "                                 value: Tensor,\n",
    "                                 embed_dim_to_check: int,\n",
    "                                 num_heads: int,\n",
    "                                 in_proj_weight: Tensor,\n",
    "                                 in_proj_bias: Tensor,\n",
    "                                 bias_k: Optional[Tensor],\n",
    "                                 bias_v: Optional[Tensor],\n",
    "                                 add_zero_attn: bool,\n",
    "                                 dropout_p: float,\n",
    "                                 out_proj_weight: Tensor,\n",
    "                                 out_proj_bias: Tensor,\n",
    "                                 training: bool = True,\n",
    "                                 key_padding_mask: Optional[Tensor] = None,\n",
    "                                 need_weights: bool = True,\n",
    "                                 attn_mask: Optional[Tensor] = None,\n",
    "                                 use_separate_proj_weight: bool = False,\n",
    "                                 q_proj_weight: Optional[Tensor] = None,\n",
    "                                 k_proj_weight: Optional[Tensor] = None,\n",
    "                                 v_proj_weight: Optional[Tensor] = None,\n",
    "                                 static_k: Optional[Tensor] = None,\n",
    "                                 static_v: Optional[Tensor] = None\n",
    "                                 ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        embed_dim_to_check: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        in_proj_weight, in_proj_bias: input projection weight and bias.\n",
    "        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        dropout_p: probability of an element to be zeroed.\n",
    "        out_proj_weight, out_proj_bias: the output projection weight and bias.\n",
    "        training: apply dropout if is ``True``.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. This is an binary mask. When the value is True,\n",
    "            the corresponding value on the attention layer will be filled with -inf.\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
    "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
    "        use_separate_proj_weight: the function accept the proj. weights for query, key,\n",
    "            and value in different forms. If false, in_proj_weight will be used, which is\n",
    "            a combination of q_proj_weight, k_proj_weight, v_proj_weight.\n",
    "        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.\n",
    "        static_k, static_v: static key and value used for attention operators.\n",
    "    Shape:\n",
    "        Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
    "          If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions\n",
    "          will be unchanged. If a BoolTensor is provided, the positions with the\n",
    "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked\n",
    "          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
    "          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
    "          are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "          is provided, it will be added to the attention weight.\n",
    "        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
    "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
    "        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
    "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
    "        Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension.\n",
    "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
    "          L is the target sequence length, S is the source sequence length.\n",
    "    \"\"\"\n",
    "    global test\n",
    "    tgt_len, bsz, embed_dim = query.size()\n",
    "    assert embed_dim == embed_dim_to_check\n",
    "    # allow MHA to have different sizes for the feature dimension\n",
    "    assert key.size(0) == value.size(0) and key.size(1) == value.size(1)\n",
    "\n",
    "    head_dim = embed_dim // num_heads\n",
    "    assert head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "    scaling = float(head_dim) ** -0.5\n",
    "\n",
    "    if not use_separate_proj_weight:\n",
    "        if (query is key or torch.equal(query, key)) and (key is value or torch.equal(key, value)):\n",
    "            # self-attention\n",
    "            print('in_proj_weight', in_proj_weight)\n",
    "            print('query', query)\n",
    "            print(in_proj_bias)\n",
    "            q, k, v = linear(query, in_proj_weight, in_proj_bias).chunk(3, dim=-1)\n",
    "            print('q', q)\n",
    "            print('here2')\n",
    "            test = (in_proj_weight, in_proj_bias)\n",
    "            print('here!')\n",
    "\n",
    "        elif (key is value or torch.equal(key, value)):\n",
    "            # encoder-decoder attention\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = 0\n",
    "            _end = embed_dim\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            q = linear(query, _w, _b)\n",
    "\n",
    "            if key is None:\n",
    "                assert value is None\n",
    "                k = None\n",
    "                v = None\n",
    "            else:\n",
    "\n",
    "                # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "                _b = in_proj_bias\n",
    "                _start = embed_dim\n",
    "                _end = None\n",
    "                _w = in_proj_weight[_start:, :]\n",
    "                if _b is not None:\n",
    "                    _b = _b[_start:]\n",
    "                k, v = linear(key, _w, _b).chunk(2, dim=-1)\n",
    "\n",
    "        else:\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = 0\n",
    "            _end = embed_dim\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            q = linear(query, _w, _b)\n",
    "\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = embed_dim\n",
    "            _end = embed_dim * 2\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            k = linear(key, _w, _b)\n",
    "\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = embed_dim * 2\n",
    "            _end = None\n",
    "            _w = in_proj_weight[_start:, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:]\n",
    "            v = linear(value, _w, _b)\n",
    "    else:\n",
    "        q_proj_weight_non_opt = torch.jit._unwrap_optional(q_proj_weight)\n",
    "        len1, len2 = q_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == query.size(-1)\n",
    "\n",
    "        k_proj_weight_non_opt = torch.jit._unwrap_optional(k_proj_weight)\n",
    "        len1, len2 = k_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == key.size(-1)\n",
    "\n",
    "        v_proj_weight_non_opt = torch.jit._unwrap_optional(v_proj_weight)\n",
    "        len1, len2 = v_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == value.size(-1)\n",
    "\n",
    "        if in_proj_bias is not None:\n",
    "            q = linear(query, q_proj_weight_non_opt, in_proj_bias[0:embed_dim])\n",
    "            k = linear(key, k_proj_weight_non_opt, in_proj_bias[embed_dim:(embed_dim * 2)])\n",
    "            v = linear(value, v_proj_weight_non_opt, in_proj_bias[(embed_dim * 2):])\n",
    "        else:\n",
    "            q = linear(query, q_proj_weight_non_opt, in_proj_bias)\n",
    "            k = linear(key, k_proj_weight_non_opt, in_proj_bias)\n",
    "            v = linear(value, v_proj_weight_non_opt, in_proj_bias)\n",
    "    q = q * scaling\n",
    "    \n",
    "    print('q', q)\n",
    "    print('k', k)\n",
    "    print('v', v)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        assert attn_mask.dtype == torch.float32 or attn_mask.dtype == torch.float64 or \\\n",
    "            attn_mask.dtype == torch.float16 or attn_mask.dtype == torch.uint8 or attn_mask.dtype == torch.bool, \\\n",
    "            'Only float, byte, and bool types are supported for attn_mask, not {}'.format(attn_mask.dtype)\n",
    "        if attn_mask.dtype == torch.uint8:\n",
    "            warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "            attn_mask = attn_mask.to(torch.bool)\n",
    "\n",
    "        if attn_mask.dim() == 2:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "            if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:\n",
    "                raise RuntimeError('The size of the 2D attn_mask is not correct.')\n",
    "        elif attn_mask.dim() == 3:\n",
    "            if list(attn_mask.size()) != [bsz * num_heads, query.size(0), key.size(0)]:\n",
    "                raise RuntimeError('The size of the 3D attn_mask is not correct.')\n",
    "        else:\n",
    "            raise RuntimeError(\"attn_mask's dimension {} is not supported\".format(attn_mask.dim()))\n",
    "        # attn_mask's dim is 3 now.\n",
    "\n",
    "    # convert ByteTensor key_padding_mask to bool\n",
    "    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n",
    "        warnings.warn(\"Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "        key_padding_mask = key_padding_mask.to(torch.bool)\n",
    "\n",
    "    if bias_k is not None and bias_v is not None:\n",
    "        if static_k is None and static_v is None:\n",
    "            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
    "            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = pad(attn_mask, (0, 1))\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "        else:\n",
    "            assert static_k is None, \"bias cannot be added to static key.\"\n",
    "            assert static_v is None, \"bias cannot be added to static value.\"\n",
    "    else:\n",
    "        assert bias_k is None\n",
    "        assert bias_v is None\n",
    "\n",
    "    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if k is not None:\n",
    "        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if v is not None:\n",
    "        v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    if static_k is not None:\n",
    "        assert static_k.size(0) == bsz * num_heads\n",
    "        assert static_k.size(2) == head_dim\n",
    "        k = static_k\n",
    "\n",
    "    if static_v is not None:\n",
    "        assert static_v.size(0) == bsz * num_heads\n",
    "        assert static_v.size(2) == head_dim\n",
    "        v = static_v\n",
    "\n",
    "    src_len = k.size(1)\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        assert key_padding_mask.size(0) == bsz\n",
    "        assert key_padding_mask.size(1) == src_len\n",
    "\n",
    "    if add_zero_attn:\n",
    "        src_len += 1\n",
    "        k = torch.cat([k, torch.zeros((k.size(0), 1) + k.size()[2:], dtype=k.dtype, device=k.device)], dim=1)\n",
    "        v = torch.cat([v, torch.zeros((v.size(0), 1) + v.size()[2:], dtype=v.dtype, device=v.device)], dim=1)\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = pad(attn_mask, (0, 1))\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "\n",
    "    attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "    print('presoftmax (scores)', attn_output_weights)\n",
    "    assert list(attn_output_weights.size()) == [bsz * num_heads, tgt_len, src_len]\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_output_weights.masked_fill_(attn_mask, float('-inf'))\n",
    "        else:\n",
    "            attn_output_weights += attn_mask\n",
    "\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        attn_output_weights = attn_output_weights.masked_fill(\n",
    "            key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "            float('-inf'),\n",
    "        )\n",
    "        attn_output_weights = attn_output_weights.view(bsz * num_heads, tgt_len, src_len)\n",
    "    print('key padding mask', key_padding_mask)\n",
    "    print('post mask', attn_output_weights)\n",
    "    attn_output_weights = softmax(\n",
    "        attn_output_weights, dim=-1)\n",
    "    print('post softmax', attn_output_weights)\n",
    "    attn_output_weights = dropout(attn_output_weights, p=dropout_p, training=training)\n",
    "\n",
    "    attn_output = torch.bmm(attn_output_weights, v)\n",
    "    print(attn_output)\n",
    "    assert list(attn_output.size()) == [bsz * num_heads, tgt_len, head_dim]\n",
    "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "    attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
    "    print('postlinear', attn_output)\n",
    "\n",
    "    if need_weights:\n",
    "        # average attention weights over heads\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        return attn_output, attn_output_weights.sum(dim=1) / num_heads\n",
    "    else:\n",
    "        return attn_output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "embedding_size = 8\n",
    "src_len = 5\n",
    "\n",
    "key_padding_mask = torch.ones(batch_size, src_len)\n",
    "key_padding_mask[0, 0:1] = 0\n",
    "print(key_padding_mask)\n",
    "model = MultiheadAttention2(embedding_size, 1, 0.0)\n",
    "#saveStateDict(model, filepath)\n",
    "src = torch.rand(src_len, batch_size, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_proj_weight Parameter containing:\n",
      "tensor([[ 0.3382, -0.4189, -0.2080,  0.1070,  0.0978,  0.0952,  0.0713, -0.0189],\n",
      "        [-0.4318, -0.4071, -0.0021, -0.0950, -0.1944,  0.3466,  0.3030, -0.1512],\n",
      "        [ 0.0771, -0.1150, -0.0088,  0.3251,  0.1080, -0.2373,  0.4255, -0.0057],\n",
      "        [-0.0861,  0.1821,  0.2320, -0.1782,  0.2418,  0.1226,  0.4281, -0.1536],\n",
      "        [ 0.1410, -0.3341, -0.1530,  0.2945,  0.4011,  0.2456,  0.3258, -0.2479],\n",
      "        [ 0.3435, -0.1643,  0.0962,  0.0242, -0.1449, -0.1071, -0.0611,  0.1019],\n",
      "        [ 0.0223, -0.0345, -0.3621,  0.4296, -0.2906, -0.3505,  0.3298,  0.1880],\n",
      "        [ 0.1074, -0.1688,  0.2655,  0.3842,  0.4082, -0.0840, -0.1044,  0.3072],\n",
      "        [ 0.2716, -0.2783,  0.2305, -0.0053,  0.2381,  0.1279,  0.1497,  0.2401],\n",
      "        [ 0.1131, -0.3005,  0.0181, -0.2212,  0.3108,  0.3672,  0.0872,  0.1239],\n",
      "        [-0.1832, -0.3999,  0.4046, -0.0282, -0.2043,  0.0103, -0.1978,  0.3897],\n",
      "        [ 0.0038,  0.2960, -0.3261,  0.3195,  0.3704,  0.2262,  0.3636,  0.2585],\n",
      "        [ 0.0522, -0.1331, -0.0889,  0.0658, -0.3625,  0.0347,  0.1408, -0.3904],\n",
      "        [-0.1176,  0.1930, -0.0081, -0.1859, -0.4123,  0.1118,  0.3012,  0.1508],\n",
      "        [-0.3254, -0.3059, -0.1184,  0.1765,  0.2208, -0.2230, -0.0968, -0.1084],\n",
      "        [ 0.1283, -0.0389, -0.0369,  0.1474,  0.0554,  0.3583,  0.1764, -0.0954],\n",
      "        [-0.3985,  0.0610, -0.0988,  0.0469, -0.2157,  0.2241,  0.2447, -0.2788],\n",
      "        [ 0.1123, -0.1180, -0.4033,  0.3464, -0.3360, -0.0836, -0.2518,  0.4068],\n",
      "        [-0.2888, -0.2153,  0.1307, -0.3904,  0.2609,  0.0200,  0.2910,  0.3366],\n",
      "        [ 0.1290,  0.2933,  0.4160,  0.1506, -0.1268,  0.1870,  0.0634,  0.0949],\n",
      "        [ 0.3404, -0.3579,  0.1080,  0.1174, -0.4067, -0.2221, -0.1225,  0.3323],\n",
      "        [-0.4124, -0.1158, -0.2777, -0.0410,  0.1434, -0.0225,  0.4175, -0.0517],\n",
      "        [ 0.0893, -0.4069, -0.0895, -0.2973,  0.3974,  0.1345,  0.2042,  0.0085],\n",
      "        [-0.3534,  0.3435, -0.2665, -0.1188, -0.1441,  0.4100, -0.3505, -0.3701]],\n",
      "       requires_grad=True)\n",
      "query tensor([[[0.9550, 0.9846, 0.7809, 0.0545, 0.2014, 0.8243, 0.7096, 0.7640]],\n",
      "\n",
      "        [[0.6869, 0.4428, 0.5213, 0.3924, 0.9823, 0.1426, 0.7374, 0.1637]],\n",
      "\n",
      "        [[0.4804, 0.0461, 0.4428, 0.8804, 0.1794, 0.7991, 0.3487, 0.2765]],\n",
      "\n",
      "        [[0.5027, 0.1438, 0.9391, 0.7353, 0.7937, 0.6363, 0.5238, 0.5000]],\n",
      "\n",
      "        [[0.1486, 0.4543, 0.6091, 0.9469, 0.9309, 0.1170, 0.4105, 0.5949]]])\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "q tensor([[[-0.1118, -0.4740,  0.0950,  0.6048,  0.0273,  0.1598, -0.2418,\n",
      "           0.3382]],\n",
      "\n",
      "        [[ 0.1394, -0.4581,  0.5101,  0.6181,  0.6134,  0.0368, -0.0816,\n",
      "           0.6504]],\n",
      "\n",
      "        [[ 0.2585, -0.0048,  0.2906,  0.1611,  0.5572,  0.1167,  0.0618,\n",
      "           0.5542]],\n",
      "\n",
      "        [[ 0.1591, -0.1980,  0.4078,  0.4872,  0.6170,  0.0930, -0.2048,\n",
      "           0.9310]],\n",
      "\n",
      "        [[-0.0453, -0.4463,  0.5058,  0.3664,  0.4432, -0.0540,  0.1096,\n",
      "           0.9748]]], grad_fn=<SplitBackward>)\n",
      "here2\n",
      "here!\n",
      "q tensor([[[-0.0395, -0.1676,  0.0336,  0.2138,  0.0096,  0.0565, -0.0855,\n",
      "           0.1196]],\n",
      "\n",
      "        [[ 0.0493, -0.1620,  0.1803,  0.2185,  0.2169,  0.0130, -0.0289,\n",
      "           0.2300]],\n",
      "\n",
      "        [[ 0.0914, -0.0017,  0.1027,  0.0570,  0.1970,  0.0413,  0.0218,\n",
      "           0.1960]],\n",
      "\n",
      "        [[ 0.0563, -0.0700,  0.1442,  0.1723,  0.2182,  0.0329, -0.0724,\n",
      "           0.3291]],\n",
      "\n",
      "        [[-0.0160, -0.1578,  0.1788,  0.1295,  0.1567, -0.0191,  0.0388,\n",
      "           0.3447]]], grad_fn=<MulBackward0>)\n",
      "k tensor([[[ 0.6081,  0.3360, -0.1295,  0.7745, -0.3899,  0.3993, -0.9857,\n",
      "           0.4223]],\n",
      "\n",
      "        [[ 0.5832,  0.3095, -0.3843,  0.7956, -0.3549, -0.2148, -0.2555,\n",
      "           0.3295]],\n",
      "\n",
      "        [[ 0.4785,  0.2675,  0.0583,  0.5978, -0.0587, -0.0528, -0.2697,\n",
      "           0.5047]],\n",
      "\n",
      "        [[ 0.7779,  0.4559,  0.1453,  0.7308, -0.4152, -0.1986, -0.2605,\n",
      "           0.4493]],\n",
      "\n",
      "        [[ 0.4902,  0.1236, -0.0274,  0.9132, -0.5525, -0.2682, -0.0170,\n",
      "           0.2276]]], grad_fn=<SplitBackward>)\n",
      "v tensor([[[-0.2932, -0.3094,  0.1257,  0.9912, -0.0347, -0.4598, -0.0591,\n",
      "          -0.4364]],\n",
      "\n",
      "        [[-0.3249, -0.5105,  0.1500,  0.4589, -0.2894, -0.0583,  0.2794,\n",
      "          -0.6784]],\n",
      "\n",
      "        [[-0.0425,  0.0725, -0.1772,  0.5674,  0.0969, -0.2235, -0.0249,\n",
      "          -0.2993]],\n",
      "\n",
      "        [[-0.2897, -0.3330,  0.2001,  0.7075, -0.0548, -0.2225,  0.1960,\n",
      "          -0.6880]],\n",
      "\n",
      "        [[-0.2872, -0.1385,  0.1341,  0.5348, -0.1923, -0.0504, -0.0330,\n",
      "          -0.6216]]], grad_fn=<SplitBackward>)\n",
      "presoftmax (scores) tensor([[[0.2345, 0.1280, 0.1459, 0.1148, 0.1625],\n",
      "         [0.1677, 0.0866, 0.2318, 0.1686, 0.1283],\n",
      "         [0.0867, 0.0388, 0.1626, 0.1192, 0.0181],\n",
      "         [0.2639, 0.1353, 0.2907, 0.2283, 0.1191],\n",
      "         [0.0530, 0.0283, 0.1933, 0.1197, 0.0824]]], grad_fn=<BmmBackward0>)\n",
      "key padding mask tensor([[False,  True,  True,  True,  True]])\n",
      "post mask tensor([[[0.2345,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.1677,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.0867,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.2639,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.0530,   -inf,   -inf,   -inf,   -inf]]], grad_fn=<ViewBackward>)\n",
      "post softmax tensor([[[1., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[-0.2932, -0.3094,  0.1257,  0.9912, -0.0347, -0.4598, -0.0591,\n",
      "          -0.4364],\n",
      "         [-0.2932, -0.3094,  0.1257,  0.9912, -0.0347, -0.4598, -0.0591,\n",
      "          -0.4364],\n",
      "         [-0.2932, -0.3094,  0.1257,  0.9912, -0.0347, -0.4598, -0.0591,\n",
      "          -0.4364],\n",
      "         [-0.2932, -0.3094,  0.1257,  0.9912, -0.0347, -0.4598, -0.0591,\n",
      "          -0.4364],\n",
      "         [-0.2932, -0.3094,  0.1257,  0.9912, -0.0347, -0.4598, -0.0591,\n",
      "          -0.4364]]], grad_fn=<BmmBackward0>)\n",
      "postlinear tensor([[[ 0.2660,  0.1230, -0.3428,  0.2869,  0.1439, -0.3100, -0.3208,\n",
      "           0.0415]],\n",
      "\n",
      "        [[ 0.2660,  0.1230, -0.3428,  0.2869,  0.1439, -0.3100, -0.3208,\n",
      "           0.0415]],\n",
      "\n",
      "        [[ 0.2660,  0.1230, -0.3428,  0.2869,  0.1439, -0.3100, -0.3208,\n",
      "           0.0415]],\n",
      "\n",
      "        [[ 0.2660,  0.1230, -0.3428,  0.2869,  0.1439, -0.3100, -0.3208,\n",
      "           0.0415]],\n",
      "\n",
      "        [[ 0.2660,  0.1230, -0.3428,  0.2869,  0.1439, -0.3100, -0.3208,\n",
      "           0.0415]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = model.forward(src, src, src, key_padding_mask=key_padding_mask.to(torch.bool))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving 0q_0in_proj_weight shape torch.Size([8, 8]) 2\n",
      "saving 0q_1in_proj_bias shape torch.Size([8]) 3\n",
      "saving 1k_0in_proj_weight shape torch.Size([8, 8]) 4\n",
      "saving 1k_1in_proj_bias shape torch.Size([8]) 5\n",
      "saving 2v_0in_proj_weight shape torch.Size([8, 8]) 6\n",
      "saving 2v_1in_proj_bias shape torch.Size([8]) 7\n",
      "out_proj.weight 7\n",
      "saving out_proj.weight 8\n",
      "out_proj.bias 8\n",
      "saving out_proj.bias 9\n",
      "saving output 10\n"
     ]
    }
   ],
   "source": [
    "filepath = '/private/home/padentomasello/scratch/pytorch_testing/multi_headed_attention.array'\n",
    "af.array.save_array('input', toArrayFire(src), filepath, False)\n",
    "af.array.save_array('mask', toArrayFire(key_padding_mask), filepath, True)\n",
    "i = 1\n",
    "params = {}\n",
    "for (name, param) in model.named_parameters():\n",
    "        if 'in_proj' in name:\n",
    "            q, k, v = param.chunk(3, dim=0)\n",
    "            hack = '0'\n",
    "            if 'in_proj_bias' in name: hack = '1'\n",
    "            params['0q_' + hack + name] = q\n",
    "            params['1k_' + hack + name] = k\n",
    "            params['2v_' + hack + name] = v\n",
    "            if 'in_proj_bias' in name:\n",
    "                for key in sorted(params.keys()):\n",
    "                    af_array = toArrayFire(params[key])\n",
    "                    if 'weight' in key:\n",
    "                        af_array = af.array.transpose(af_array)\n",
    "                    print('saving', key, 'shape', params[key].shape, af.array.save_array(key, af_array, filepath, True))\n",
    "                    i = i + 1\n",
    "            continue\n",
    "        elif len(param.size()) > 0:\n",
    "            af_array = toArrayFire(param)\n",
    "            if 'fc' in name and 'weight' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            if 'weight' in name and 'proj' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            print(name, i)\n",
    "            print('saving', name, af.array.save_array(name, af_array, filepath, True))\n",
    "            i = i + 1\n",
    "print('saving', 'output', af.array.save_array('output', toArrayFire(output), filepath, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2660,  0.1230, -0.3428,  0.2869,  0.1439, -0.3100, -0.3208,\n",
       "           0.0415]],\n",
       "\n",
       "        [[ 0.2660,  0.1230, -0.3428,  0.2869,  0.1439, -0.3100, -0.3208,\n",
       "           0.0415]],\n",
       "\n",
       "        [[ 0.2660,  0.1230, -0.3428,  0.2869,  0.1439, -0.3100, -0.3208,\n",
       "           0.0415]],\n",
       "\n",
       "        [[ 0.2660,  0.1230, -0.3428,  0.2869,  0.1439, -0.3100, -0.3208,\n",
       "           0.0415]],\n",
       "\n",
       "        [[ 0.2660,  0.1230, -0.3428,  0.2869,  0.1439, -0.3100, -0.3208,\n",
       "           0.0415]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
