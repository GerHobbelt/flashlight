{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arrayfire as af\n",
    "import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toArrayFire(x):\n",
    "    x_np = x.detach().contiguous().numpy()\n",
    "    shape = 1\n",
    "    if len(x_np.shape) == 0:\n",
    "        shape = (1,)\n",
    "    else:\n",
    "        shape = x_np.shape[::-1]\n",
    "    afArray = af.Array(x_np.ctypes.data, shape, x_np.dtype.char)\n",
    "    return afArray\n",
    "\n",
    "def saveStateDict(model, filepath):\n",
    "    params = {}\n",
    "    i = 0\n",
    "    for (name, param) in model.state_dict().items():\n",
    "        if 'running' in name:\n",
    "            continue\n",
    "        if 'in_proj' in name:\n",
    "            q, k, v = param.chunk(3, dim=0)\n",
    "            hack = '0'\n",
    "            if 'in_proj_bias' in name: hack = '1'\n",
    "            params['0q_' + hack + name] = q\n",
    "            params['1k_' + hack + name] = k\n",
    "            params['2v_' + hack + name] = v\n",
    "            if 'in_proj_bias' in name:\n",
    "                for key in sorted(params.keys()):\n",
    "                    af_array = toArrayFire(params[key])\n",
    "                    if 'weight' in key:\n",
    "                        af_array = af.array.transpose(af_array)\n",
    "                    print(key, i, params[key].shape)\n",
    "                    print(af.array.save_array(key, af_array, filepath, True))\n",
    "                    i = i + 1\n",
    "                params = {}\n",
    "            continue\n",
    "        elif len(param.size()) > 0:\n",
    "            if 'input_proj.bias' in name:\n",
    "                param = param.reshape((1, 1, 256))\n",
    "            af_array = toArrayFire(param)\n",
    "            if 'fc' in name and 'weight' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            elif 'weight' in name and 'proj' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            elif 'weight' in name and 'linear' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            elif 'query_embed' in name:\n",
    "                af_array = af_array\n",
    "            elif 'weight' in name and 'embed' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "\n",
    "            print(name, i, param.shape)\n",
    "            print(af.array.save_array(name, af_array, filepath, True))\n",
    "            i = i + 1\n",
    "    for name in model.state_dict():\n",
    "        if 'running' in name:\n",
    "            print(name)\n",
    "            af_array = toArrayFire(model.state_dict()[name])\n",
    "            print(name, model.state_dict()[name].shape, af.array.save_array(name, af_array, filepath + 'running', True))\n",
    "    \n",
    "def create_parser():\n",
    "    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n",
    "    parser.add_argument('--lr', default=1e-4, type=float)\n",
    "    parser.add_argument('--lr_backbone', default=1e-5, type=float)\n",
    "    parser.add_argument('--batch_size', default=2, type=int)\n",
    "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
    "    parser.add_argument('--epochs', default=300, type=int)\n",
    "    parser.add_argument('--lr_drop', default=200, type=int)\n",
    "    parser.add_argument('--optimizer', default=\"adam\", type=str)\n",
    "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
    "                        help='gradient clipping max norm')\n",
    "    parser.add_argument('--eval_skip', default=1, type=int,\n",
    "                        help='do evaluation every \"eval_skip\" frames')\n",
    "    parser.add_argument('--schedule', default='step', type=str,\n",
    "                        choices=('step', 'multistep'))\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--frozen_weights', type=str, default=None,\n",
    "                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n",
    "    # * Backbone\n",
    "    parser.add_argument('--backbone', default='resnet50', type=str,\n",
    "                        help=\"Name of the convolutional backbone to use\")\n",
    "    parser.add_argument('--dilation', action='store_true',\n",
    "                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
    "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n",
    "                        help=\"Type of positional embedding to use on top of the image features\")\n",
    "\n",
    "    # * Transformer\n",
    "    parser.add_argument('--enc_layers', default=6, type=int,\n",
    "                        help=\"Number of encoding layers in the transformer\")\n",
    "    parser.add_argument('--dec_layers', default=6, type=int,\n",
    "                        help=\"Number of decoding layers in the transformer\")\n",
    "    parser.add_argument('--dim_feedforward', default=2048, type=int,\n",
    "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
    "    parser.add_argument('--hidden_dim', default=256, type=int,\n",
    "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
    "    parser.add_argument('--dropout', default=0.1, type=float,\n",
    "                        help=\"Dropout applied in the transformer\")\n",
    "    parser.add_argument('--nheads', default=8, type=int,\n",
    "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
    "    parser.add_argument('--num_queries', default=100, type=int,\n",
    "                        help=\"Number of query slots\")\n",
    "    parser.add_argument('--pre_norm', action='store_true')\n",
    "    parser.add_argument('--no_pass_pos_and_query', dest='pass_pos_and_query', action='store_false',\n",
    "                        help=\"Disables passing the positional encodings to each attention layers\")\n",
    "\n",
    "    # * Segmentation\n",
    "    parser.add_argument('--mask_model', default='none', type=str, choices=(\"none\", \"smallconv\", \"v2\"),\n",
    "                        help=\"Segmentation head to be used (if None, segmentation will not be trained)\")\n",
    "\n",
    "    # Loss\n",
    "    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',\n",
    "                        help=\"Disables auxiliary decoding losses (loss at each layer)\")\n",
    "    parser.add_argument('--set_loss', default='hungarian', type=str,\n",
    "                        choices=('sequential', 'hungarian', 'lexicographical'),\n",
    "                        help=\"Type of matching to perform in the loss\")\n",
    "    parser.add_argument('--bcl', dest='use_bcl', action='store_true',\n",
    "                        help=\"Use balanced classification loss\")\n",
    "    # * Matcher\n",
    "    parser.add_argument('--set_cost_class', default=1, type=float,\n",
    "                        help=\"Class coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_bbox', default=5, type=float,\n",
    "                        help=\"L1 box coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_giou', default=2, type=float,\n",
    "                        help=\"giou box coefficient in the matching cost\")\n",
    "    # * Loss coefficients\n",
    "    parser.add_argument('--mask_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--dice_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--bbox_loss_coef', default=5, type=float)\n",
    "    parser.add_argument('--giou_loss_coef', default=2, type=float)\n",
    "    parser.add_argument('--eos_coef', default=0.1, type=float,\n",
    "                        help=\"Relative classification weight of the no-object class\")\n",
    "\n",
    "    # dataset parameters\n",
    "    parser.add_argument('--dataset_file', default='coco')\n",
    "    parser.add_argument('--coco_path', type=str, default='/datasets01/COCO/022719')\n",
    "    parser.add_argument('--coco_panoptic_path', type=str, default='/datasets01/COCO/060419')\n",
    "    parser.add_argument('--remove_difficult', action='store_true')\n",
    "    parser.add_argument('--masks', action='store_true')\n",
    "\n",
    "    parser.add_argument('--output-dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=42, type=int)\n",
    "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "    parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--num_workers', default=2, type=int)\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world-size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--dist-url', default='env://', help='url used to set up distributed training')\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.detr import *\n",
    "parser = create_parser()\n",
    "pretrained_path = '/private/home/padentomasello/scratch/pytorch_testing/detr-r50-e632da11.pth'\n",
    "args = parser.parse_args([\"--resume=/private/home/padentomasello/scratch/pytorch_testing/detr-r50-e632da11.pth\"])\n",
    "model, _, _ = build(args)   \n",
    "if args.resume:\n",
    "    if args.resume.startswith('https'):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            args.resume, map_location='cpu', check_hash=True)\n",
    "    else:\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7, 7])\n",
      "torch.Size([2, 256, 7, 7])\n",
      "0.body.conv1.weight 0 torch.Size([64, 3, 7, 7])\n",
      "2\n",
      "0.body.bn1.weight 1 torch.Size([64])\n",
      "3\n",
      "0.body.bn1.bias 2 torch.Size([64])\n",
      "4\n",
      "0.body.layer1.0.conv1.weight 3 torch.Size([64, 64, 1, 1])\n",
      "5\n",
      "0.body.layer1.0.bn1.weight 4 torch.Size([64])\n",
      "6\n",
      "0.body.layer1.0.bn1.bias 5 torch.Size([64])\n",
      "7\n",
      "0.body.layer1.0.conv2.weight 6 torch.Size([64, 64, 3, 3])\n",
      "8\n",
      "0.body.layer1.0.bn2.weight 7 torch.Size([64])\n",
      "9\n",
      "0.body.layer1.0.bn2.bias 8 torch.Size([64])\n",
      "10\n",
      "0.body.layer1.0.conv3.weight 9 torch.Size([256, 64, 1, 1])\n",
      "11\n",
      "0.body.layer1.0.bn3.weight 10 torch.Size([256])\n",
      "12\n",
      "0.body.layer1.0.bn3.bias 11 torch.Size([256])\n",
      "13\n",
      "0.body.layer1.0.downsample.0.weight 12 torch.Size([256, 64, 1, 1])\n",
      "14\n",
      "0.body.layer1.0.downsample.1.weight 13 torch.Size([256])\n",
      "15\n",
      "0.body.layer1.0.downsample.1.bias 14 torch.Size([256])\n",
      "16\n",
      "0.body.layer1.1.conv1.weight 15 torch.Size([64, 256, 1, 1])\n",
      "17\n",
      "0.body.layer1.1.bn1.weight 16 torch.Size([64])\n",
      "18\n",
      "0.body.layer1.1.bn1.bias 17 torch.Size([64])\n",
      "19\n",
      "0.body.layer1.1.conv2.weight 18 torch.Size([64, 64, 3, 3])\n",
      "20\n",
      "0.body.layer1.1.bn2.weight 19 torch.Size([64])\n",
      "21\n",
      "0.body.layer1.1.bn2.bias 20 torch.Size([64])\n",
      "22\n",
      "0.body.layer1.1.conv3.weight 21 torch.Size([256, 64, 1, 1])\n",
      "23\n",
      "0.body.layer1.1.bn3.weight 22 torch.Size([256])\n",
      "24\n",
      "0.body.layer1.1.bn3.bias 23 torch.Size([256])\n",
      "25\n",
      "0.body.layer1.2.conv1.weight 24 torch.Size([64, 256, 1, 1])\n",
      "26\n",
      "0.body.layer1.2.bn1.weight 25 torch.Size([64])\n",
      "27\n",
      "0.body.layer1.2.bn1.bias 26 torch.Size([64])\n",
      "28\n",
      "0.body.layer1.2.conv2.weight 27 torch.Size([64, 64, 3, 3])\n",
      "29\n",
      "0.body.layer1.2.bn2.weight 28 torch.Size([64])\n",
      "30\n",
      "0.body.layer1.2.bn2.bias 29 torch.Size([64])\n",
      "31\n",
      "0.body.layer1.2.conv3.weight 30 torch.Size([256, 64, 1, 1])\n",
      "32\n",
      "0.body.layer1.2.bn3.weight 31 torch.Size([256])\n",
      "33\n",
      "0.body.layer1.2.bn3.bias 32 torch.Size([256])\n",
      "34\n",
      "0.body.layer2.0.conv1.weight 33 torch.Size([128, 256, 1, 1])\n",
      "35\n",
      "0.body.layer2.0.bn1.weight 34 torch.Size([128])\n",
      "36\n",
      "0.body.layer2.0.bn1.bias 35 torch.Size([128])\n",
      "37\n",
      "0.body.layer2.0.conv2.weight 36 torch.Size([128, 128, 3, 3])\n",
      "38\n",
      "0.body.layer2.0.bn2.weight 37 torch.Size([128])\n",
      "39\n",
      "0.body.layer2.0.bn2.bias 38 torch.Size([128])\n",
      "40\n",
      "0.body.layer2.0.conv3.weight 39 torch.Size([512, 128, 1, 1])\n",
      "41\n",
      "0.body.layer2.0.bn3.weight 40 torch.Size([512])\n",
      "42\n",
      "0.body.layer2.0.bn3.bias 41 torch.Size([512])\n",
      "43\n",
      "0.body.layer2.0.downsample.0.weight 42 torch.Size([512, 256, 1, 1])\n",
      "44\n",
      "0.body.layer2.0.downsample.1.weight 43 torch.Size([512])\n",
      "45\n",
      "0.body.layer2.0.downsample.1.bias 44 torch.Size([512])\n",
      "46\n",
      "0.body.layer2.1.conv1.weight 45 torch.Size([128, 512, 1, 1])\n",
      "47\n",
      "0.body.layer2.1.bn1.weight 46 torch.Size([128])\n",
      "48\n",
      "0.body.layer2.1.bn1.bias 47 torch.Size([128])\n",
      "49\n",
      "0.body.layer2.1.conv2.weight 48 torch.Size([128, 128, 3, 3])\n",
      "50\n",
      "0.body.layer2.1.bn2.weight 49 torch.Size([128])\n",
      "51\n",
      "0.body.layer2.1.bn2.bias 50 torch.Size([128])\n",
      "52\n",
      "0.body.layer2.1.conv3.weight 51 torch.Size([512, 128, 1, 1])\n",
      "53\n",
      "0.body.layer2.1.bn3.weight 52 torch.Size([512])\n",
      "54\n",
      "0.body.layer2.1.bn3.bias 53 torch.Size([512])\n",
      "55\n",
      "0.body.layer2.2.conv1.weight 54 torch.Size([128, 512, 1, 1])\n",
      "56\n",
      "0.body.layer2.2.bn1.weight 55 torch.Size([128])\n",
      "57\n",
      "0.body.layer2.2.bn1.bias 56 torch.Size([128])\n",
      "58\n",
      "0.body.layer2.2.conv2.weight 57 torch.Size([128, 128, 3, 3])\n",
      "59\n",
      "0.body.layer2.2.bn2.weight 58 torch.Size([128])\n",
      "60\n",
      "0.body.layer2.2.bn2.bias 59 torch.Size([128])\n",
      "61\n",
      "0.body.layer2.2.conv3.weight 60 torch.Size([512, 128, 1, 1])\n",
      "62\n",
      "0.body.layer2.2.bn3.weight 61 torch.Size([512])\n",
      "63\n",
      "0.body.layer2.2.bn3.bias 62 torch.Size([512])\n",
      "64\n",
      "0.body.layer2.3.conv1.weight 63 torch.Size([128, 512, 1, 1])\n",
      "65\n",
      "0.body.layer2.3.bn1.weight 64 torch.Size([128])\n",
      "66\n",
      "0.body.layer2.3.bn1.bias 65 torch.Size([128])\n",
      "67\n",
      "0.body.layer2.3.conv2.weight 66 torch.Size([128, 128, 3, 3])\n",
      "68\n",
      "0.body.layer2.3.bn2.weight 67 torch.Size([128])\n",
      "69\n",
      "0.body.layer2.3.bn2.bias 68 torch.Size([128])\n",
      "70\n",
      "0.body.layer2.3.conv3.weight 69 torch.Size([512, 128, 1, 1])\n",
      "71\n",
      "0.body.layer2.3.bn3.weight 70 torch.Size([512])\n",
      "72\n",
      "0.body.layer2.3.bn3.bias 71 torch.Size([512])\n",
      "73\n",
      "0.body.layer3.0.conv1.weight 72 torch.Size([256, 512, 1, 1])\n",
      "74\n",
      "0.body.layer3.0.bn1.weight 73 torch.Size([256])\n",
      "75\n",
      "0.body.layer3.0.bn1.bias 74 torch.Size([256])\n",
      "76\n",
      "0.body.layer3.0.conv2.weight 75 torch.Size([256, 256, 3, 3])\n",
      "77\n",
      "0.body.layer3.0.bn2.weight 76 torch.Size([256])\n",
      "78\n",
      "0.body.layer3.0.bn2.bias 77 torch.Size([256])\n",
      "79\n",
      "0.body.layer3.0.conv3.weight 78 torch.Size([1024, 256, 1, 1])\n",
      "80\n",
      "0.body.layer3.0.bn3.weight 79 torch.Size([1024])\n",
      "81\n",
      "0.body.layer3.0.bn3.bias 80 torch.Size([1024])\n",
      "82\n",
      "0.body.layer3.0.downsample.0.weight 81 torch.Size([1024, 512, 1, 1])\n",
      "83\n",
      "0.body.layer3.0.downsample.1.weight 82 torch.Size([1024])\n",
      "84\n",
      "0.body.layer3.0.downsample.1.bias 83 torch.Size([1024])\n",
      "85\n",
      "0.body.layer3.1.conv1.weight 84 torch.Size([256, 1024, 1, 1])\n",
      "86\n",
      "0.body.layer3.1.bn1.weight 85 torch.Size([256])\n",
      "87\n",
      "0.body.layer3.1.bn1.bias 86 torch.Size([256])\n",
      "88\n",
      "0.body.layer3.1.conv2.weight 87 torch.Size([256, 256, 3, 3])\n",
      "89\n",
      "0.body.layer3.1.bn2.weight 88 torch.Size([256])\n",
      "90\n",
      "0.body.layer3.1.bn2.bias 89 torch.Size([256])\n",
      "91\n",
      "0.body.layer3.1.conv3.weight 90 torch.Size([1024, 256, 1, 1])\n",
      "92\n",
      "0.body.layer3.1.bn3.weight 91 torch.Size([1024])\n",
      "93\n",
      "0.body.layer3.1.bn3.bias 92 torch.Size([1024])\n",
      "94\n",
      "0.body.layer3.2.conv1.weight 93 torch.Size([256, 1024, 1, 1])\n",
      "95\n",
      "0.body.layer3.2.bn1.weight 94 torch.Size([256])\n",
      "96\n",
      "0.body.layer3.2.bn1.bias 95 torch.Size([256])\n",
      "97\n",
      "0.body.layer3.2.conv2.weight 96 torch.Size([256, 256, 3, 3])\n",
      "98\n",
      "0.body.layer3.2.bn2.weight 97 torch.Size([256])\n",
      "99\n",
      "0.body.layer3.2.bn2.bias 98 torch.Size([256])\n",
      "100\n",
      "0.body.layer3.2.conv3.weight 99 torch.Size([1024, 256, 1, 1])\n",
      "101\n",
      "0.body.layer3.2.bn3.weight 100 torch.Size([1024])\n",
      "102\n",
      "0.body.layer3.2.bn3.bias 101 torch.Size([1024])\n",
      "103\n",
      "0.body.layer3.3.conv1.weight 102 torch.Size([256, 1024, 1, 1])\n",
      "104\n",
      "0.body.layer3.3.bn1.weight 103 torch.Size([256])\n",
      "105\n",
      "0.body.layer3.3.bn1.bias 104 torch.Size([256])\n",
      "106\n",
      "0.body.layer3.3.conv2.weight 105 torch.Size([256, 256, 3, 3])\n",
      "107\n",
      "0.body.layer3.3.bn2.weight 106 torch.Size([256])\n",
      "108\n",
      "0.body.layer3.3.bn2.bias 107 torch.Size([256])\n",
      "109\n",
      "0.body.layer3.3.conv3.weight 108 torch.Size([1024, 256, 1, 1])\n",
      "110\n",
      "0.body.layer3.3.bn3.weight 109 torch.Size([1024])\n",
      "111\n",
      "0.body.layer3.3.bn3.bias 110 torch.Size([1024])\n",
      "112\n",
      "0.body.layer3.4.conv1.weight 111 torch.Size([256, 1024, 1, 1])\n",
      "113\n",
      "0.body.layer3.4.bn1.weight 112 torch.Size([256])\n",
      "114\n",
      "0.body.layer3.4.bn1.bias 113 torch.Size([256])\n",
      "115\n",
      "0.body.layer3.4.conv2.weight 114 torch.Size([256, 256, 3, 3])\n",
      "116\n",
      "0.body.layer3.4.bn2.weight 115 torch.Size([256])\n",
      "117\n",
      "0.body.layer3.4.bn2.bias 116 torch.Size([256])\n",
      "118\n",
      "0.body.layer3.4.conv3.weight 117 torch.Size([1024, 256, 1, 1])\n",
      "119\n",
      "0.body.layer3.4.bn3.weight 118 torch.Size([1024])\n",
      "120\n",
      "0.body.layer3.4.bn3.bias 119 torch.Size([1024])\n",
      "121\n",
      "0.body.layer3.5.conv1.weight 120 torch.Size([256, 1024, 1, 1])\n",
      "122\n",
      "0.body.layer3.5.bn1.weight 121 torch.Size([256])\n",
      "123\n",
      "0.body.layer3.5.bn1.bias 122 torch.Size([256])\n",
      "124\n",
      "0.body.layer3.5.conv2.weight 123 torch.Size([256, 256, 3, 3])\n",
      "125\n",
      "0.body.layer3.5.bn2.weight 124 torch.Size([256])\n",
      "126\n",
      "0.body.layer3.5.bn2.bias 125 torch.Size([256])\n",
      "127\n",
      "0.body.layer3.5.conv3.weight 126 torch.Size([1024, 256, 1, 1])\n",
      "128\n",
      "0.body.layer3.5.bn3.weight 127 torch.Size([1024])\n",
      "129\n",
      "0.body.layer3.5.bn3.bias 128 torch.Size([1024])\n",
      "130\n",
      "0.body.layer4.0.conv1.weight 129 torch.Size([512, 1024, 1, 1])\n",
      "131\n",
      "0.body.layer4.0.bn1.weight 130 torch.Size([512])\n",
      "132\n",
      "0.body.layer4.0.bn1.bias 131 torch.Size([512])\n",
      "133\n",
      "0.body.layer4.0.conv2.weight 132 torch.Size([512, 512, 3, 3])\n",
      "134\n",
      "0.body.layer4.0.bn2.weight 133 torch.Size([512])\n",
      "135\n",
      "0.body.layer4.0.bn2.bias 134 torch.Size([512])\n",
      "136\n",
      "0.body.layer4.0.conv3.weight 135 torch.Size([2048, 512, 1, 1])\n",
      "137\n",
      "0.body.layer4.0.bn3.weight 136 torch.Size([2048])\n",
      "138\n",
      "0.body.layer4.0.bn3.bias 137 torch.Size([2048])\n",
      "139\n",
      "0.body.layer4.0.downsample.0.weight 138 torch.Size([2048, 1024, 1, 1])\n",
      "140\n",
      "0.body.layer4.0.downsample.1.weight 139 torch.Size([2048])\n",
      "141\n",
      "0.body.layer4.0.downsample.1.bias 140 torch.Size([2048])\n",
      "142\n",
      "0.body.layer4.1.conv1.weight 141 torch.Size([512, 2048, 1, 1])\n",
      "143\n",
      "0.body.layer4.1.bn1.weight 142 torch.Size([512])\n",
      "144\n",
      "0.body.layer4.1.bn1.bias 143 torch.Size([512])\n",
      "145\n",
      "0.body.layer4.1.conv2.weight 144 torch.Size([512, 512, 3, 3])\n",
      "146\n",
      "0.body.layer4.1.bn2.weight 145 torch.Size([512])\n",
      "147\n",
      "0.body.layer4.1.bn2.bias 146 torch.Size([512])\n",
      "148\n",
      "0.body.layer4.1.conv3.weight 147 torch.Size([2048, 512, 1, 1])\n",
      "149\n",
      "0.body.layer4.1.bn3.weight 148 torch.Size([2048])\n",
      "150\n",
      "0.body.layer4.1.bn3.bias 149 torch.Size([2048])\n",
      "151\n",
      "0.body.layer4.2.conv1.weight 150 torch.Size([512, 2048, 1, 1])\n",
      "152\n",
      "0.body.layer4.2.bn1.weight 151 torch.Size([512])\n",
      "153\n",
      "0.body.layer4.2.bn1.bias 152 torch.Size([512])\n",
      "154\n",
      "0.body.layer4.2.conv2.weight 153 torch.Size([512, 512, 3, 3])\n",
      "155\n",
      "0.body.layer4.2.bn2.weight 154 torch.Size([512])\n",
      "156\n",
      "0.body.layer4.2.bn2.bias 155 torch.Size([512])\n",
      "157\n",
      "0.body.layer4.2.conv3.weight 156 torch.Size([2048, 512, 1, 1])\n",
      "158\n",
      "0.body.layer4.2.bn3.weight 157 torch.Size([2048])\n",
      "159\n",
      "0.body.layer4.2.bn3.bias 158 torch.Size([2048])\n",
      "160\n",
      "0.body.bn1.running_mean\n",
      "0.body.bn1.running_mean torch.Size([64]) 636\n",
      "0.body.bn1.running_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.body.bn1.running_var torch.Size([64]) 637\n",
      "0.body.layer1.0.bn1.running_mean\n",
      "0.body.layer1.0.bn1.running_mean torch.Size([64]) 638\n",
      "0.body.layer1.0.bn1.running_var\n",
      "0.body.layer1.0.bn1.running_var torch.Size([64]) 639\n",
      "0.body.layer1.0.bn2.running_mean\n",
      "0.body.layer1.0.bn2.running_mean torch.Size([64]) 640\n",
      "0.body.layer1.0.bn2.running_var\n",
      "0.body.layer1.0.bn2.running_var torch.Size([64]) 641\n",
      "0.body.layer1.0.bn3.running_mean\n",
      "0.body.layer1.0.bn3.running_mean torch.Size([256]) 642\n",
      "0.body.layer1.0.bn3.running_var\n",
      "0.body.layer1.0.bn3.running_var torch.Size([256]) 643\n",
      "0.body.layer1.0.downsample.1.running_mean\n",
      "0.body.layer1.0.downsample.1.running_mean torch.Size([256]) 644\n",
      "0.body.layer1.0.downsample.1.running_var\n",
      "0.body.layer1.0.downsample.1.running_var torch.Size([256]) 645\n",
      "0.body.layer1.1.bn1.running_mean\n",
      "0.body.layer1.1.bn1.running_mean torch.Size([64]) 646\n",
      "0.body.layer1.1.bn1.running_var\n",
      "0.body.layer1.1.bn1.running_var torch.Size([64]) 647\n",
      "0.body.layer1.1.bn2.running_mean\n",
      "0.body.layer1.1.bn2.running_mean torch.Size([64]) 648\n",
      "0.body.layer1.1.bn2.running_var\n",
      "0.body.layer1.1.bn2.running_var torch.Size([64]) 649\n",
      "0.body.layer1.1.bn3.running_mean\n",
      "0.body.layer1.1.bn3.running_mean torch.Size([256]) 650\n",
      "0.body.layer1.1.bn3.running_var\n",
      "0.body.layer1.1.bn3.running_var torch.Size([256]) 651\n",
      "0.body.layer1.2.bn1.running_mean\n",
      "0.body.layer1.2.bn1.running_mean torch.Size([64]) 652\n",
      "0.body.layer1.2.bn1.running_var\n",
      "0.body.layer1.2.bn1.running_var torch.Size([64]) 653\n",
      "0.body.layer1.2.bn2.running_mean\n",
      "0.body.layer1.2.bn2.running_mean torch.Size([64]) 654\n",
      "0.body.layer1.2.bn2.running_var\n",
      "0.body.layer1.2.bn2.running_var torch.Size([64]) 655\n",
      "0.body.layer1.2.bn3.running_mean\n",
      "0.body.layer1.2.bn3.running_mean torch.Size([256]) 656\n",
      "0.body.layer1.2.bn3.running_var\n",
      "0.body.layer1.2.bn3.running_var torch.Size([256]) 657\n",
      "0.body.layer2.0.bn1.running_mean\n",
      "0.body.layer2.0.bn1.running_mean torch.Size([128]) 658\n",
      "0.body.layer2.0.bn1.running_var\n",
      "0.body.layer2.0.bn1.running_var torch.Size([128]) 659\n",
      "0.body.layer2.0.bn2.running_mean\n",
      "0.body.layer2.0.bn2.running_mean torch.Size([128]) 660\n",
      "0.body.layer2.0.bn2.running_var\n",
      "0.body.layer2.0.bn2.running_var torch.Size([128]) 661\n",
      "0.body.layer2.0.bn3.running_mean\n",
      "0.body.layer2.0.bn3.running_mean torch.Size([512]) 662\n",
      "0.body.layer2.0.bn3.running_var\n",
      "0.body.layer2.0.bn3.running_var torch.Size([512]) 663\n",
      "0.body.layer2.0.downsample.1.running_mean\n",
      "0.body.layer2.0.downsample.1.running_mean torch.Size([512]) 664\n",
      "0.body.layer2.0.downsample.1.running_var\n",
      "0.body.layer2.0.downsample.1.running_var torch.Size([512]) 665\n",
      "0.body.layer2.1.bn1.running_mean\n",
      "0.body.layer2.1.bn1.running_mean torch.Size([128]) 666\n",
      "0.body.layer2.1.bn1.running_var\n",
      "0.body.layer2.1.bn1.running_var torch.Size([128]) 667\n",
      "0.body.layer2.1.bn2.running_mean\n",
      "0.body.layer2.1.bn2.running_mean torch.Size([128]) 668\n",
      "0.body.layer2.1.bn2.running_var\n",
      "0.body.layer2.1.bn2.running_var torch.Size([128]) 669\n",
      "0.body.layer2.1.bn3.running_mean\n",
      "0.body.layer2.1.bn3.running_mean torch.Size([512]) 670\n",
      "0.body.layer2.1.bn3.running_var\n",
      "0.body.layer2.1.bn3.running_var torch.Size([512]) 671\n",
      "0.body.layer2.2.bn1.running_mean\n",
      "0.body.layer2.2.bn1.running_mean torch.Size([128]) 672\n",
      "0.body.layer2.2.bn1.running_var\n",
      "0.body.layer2.2.bn1.running_var torch.Size([128]) 673\n",
      "0.body.layer2.2.bn2.running_mean\n",
      "0.body.layer2.2.bn2.running_mean torch.Size([128]) 674\n",
      "0.body.layer2.2.bn2.running_var\n",
      "0.body.layer2.2.bn2.running_var torch.Size([128]) 675\n",
      "0.body.layer2.2.bn3.running_mean\n",
      "0.body.layer2.2.bn3.running_mean torch.Size([512]) 676\n",
      "0.body.layer2.2.bn3.running_var\n",
      "0.body.layer2.2.bn3.running_var torch.Size([512]) 677\n",
      "0.body.layer2.3.bn1.running_mean\n",
      "0.body.layer2.3.bn1.running_mean torch.Size([128]) 678\n",
      "0.body.layer2.3.bn1.running_var\n",
      "0.body.layer2.3.bn1.running_var torch.Size([128]) 679\n",
      "0.body.layer2.3.bn2.running_mean\n",
      "0.body.layer2.3.bn2.running_mean torch.Size([128]) 680\n",
      "0.body.layer2.3.bn2.running_var\n",
      "0.body.layer2.3.bn2.running_var torch.Size([128]) 681\n",
      "0.body.layer2.3.bn3.running_mean\n",
      "0.body.layer2.3.bn3.running_mean torch.Size([512]) 682\n",
      "0.body.layer2.3.bn3.running_var\n",
      "0.body.layer2.3.bn3.running_var torch.Size([512]) 683\n",
      "0.body.layer3.0.bn1.running_mean\n",
      "0.body.layer3.0.bn1.running_mean torch.Size([256]) 684\n",
      "0.body.layer3.0.bn1.running_var\n",
      "0.body.layer3.0.bn1.running_var torch.Size([256]) 685\n",
      "0.body.layer3.0.bn2.running_mean\n",
      "0.body.layer3.0.bn2.running_mean torch.Size([256]) 686\n",
      "0.body.layer3.0.bn2.running_var\n",
      "0.body.layer3.0.bn2.running_var torch.Size([256]) 687\n",
      "0.body.layer3.0.bn3.running_mean\n",
      "0.body.layer3.0.bn3.running_mean torch.Size([1024]) 688\n",
      "0.body.layer3.0.bn3.running_var\n",
      "0.body.layer3.0.bn3.running_var torch.Size([1024]) 689\n",
      "0.body.layer3.0.downsample.1.running_mean\n",
      "0.body.layer3.0.downsample.1.running_mean torch.Size([1024]) 690\n",
      "0.body.layer3.0.downsample.1.running_var\n",
      "0.body.layer3.0.downsample.1.running_var torch.Size([1024]) 691\n",
      "0.body.layer3.1.bn1.running_mean\n",
      "0.body.layer3.1.bn1.running_mean torch.Size([256]) 692\n",
      "0.body.layer3.1.bn1.running_var\n",
      "0.body.layer3.1.bn1.running_var torch.Size([256]) 693\n",
      "0.body.layer3.1.bn2.running_mean\n",
      "0.body.layer3.1.bn2.running_mean torch.Size([256]) 694\n",
      "0.body.layer3.1.bn2.running_var\n",
      "0.body.layer3.1.bn2.running_var torch.Size([256]) 695\n",
      "0.body.layer3.1.bn3.running_mean\n",
      "0.body.layer3.1.bn3.running_mean torch.Size([1024]) 696\n",
      "0.body.layer3.1.bn3.running_var\n",
      "0.body.layer3.1.bn3.running_var torch.Size([1024]) 697\n",
      "0.body.layer3.2.bn1.running_mean\n",
      "0.body.layer3.2.bn1.running_mean torch.Size([256]) 698\n",
      "0.body.layer3.2.bn1.running_var\n",
      "0.body.layer3.2.bn1.running_var torch.Size([256]) 699\n",
      "0.body.layer3.2.bn2.running_mean\n",
      "0.body.layer3.2.bn2.running_mean torch.Size([256]) 700\n",
      "0.body.layer3.2.bn2.running_var\n",
      "0.body.layer3.2.bn2.running_var torch.Size([256]) 701\n",
      "0.body.layer3.2.bn3.running_mean\n",
      "0.body.layer3.2.bn3.running_mean torch.Size([1024]) 702\n",
      "0.body.layer3.2.bn3.running_var\n",
      "0.body.layer3.2.bn3.running_var torch.Size([1024]) 703\n",
      "0.body.layer3.3.bn1.running_mean\n",
      "0.body.layer3.3.bn1.running_mean torch.Size([256]) 704\n",
      "0.body.layer3.3.bn1.running_var\n",
      "0.body.layer3.3.bn1.running_var torch.Size([256]) 705\n",
      "0.body.layer3.3.bn2.running_mean\n",
      "0.body.layer3.3.bn2.running_mean torch.Size([256]) 706\n",
      "0.body.layer3.3.bn2.running_var\n",
      "0.body.layer3.3.bn2.running_var torch.Size([256]) 707\n",
      "0.body.layer3.3.bn3.running_mean\n",
      "0.body.layer3.3.bn3.running_mean torch.Size([1024]) 708\n",
      "0.body.layer3.3.bn3.running_var\n",
      "0.body.layer3.3.bn3.running_var torch.Size([1024]) 709\n",
      "0.body.layer3.4.bn1.running_mean\n",
      "0.body.layer3.4.bn1.running_mean torch.Size([256]) 710\n",
      "0.body.layer3.4.bn1.running_var\n",
      "0.body.layer3.4.bn1.running_var torch.Size([256]) 711\n",
      "0.body.layer3.4.bn2.running_mean\n",
      "0.body.layer3.4.bn2.running_mean torch.Size([256]) 712\n",
      "0.body.layer3.4.bn2.running_var\n",
      "0.body.layer3.4.bn2.running_var torch.Size([256]) 713\n",
      "0.body.layer3.4.bn3.running_mean\n",
      "0.body.layer3.4.bn3.running_mean torch.Size([1024]) 714\n",
      "0.body.layer3.4.bn3.running_var\n",
      "0.body.layer3.4.bn3.running_var torch.Size([1024]) 715\n",
      "0.body.layer3.5.bn1.running_mean\n",
      "0.body.layer3.5.bn1.running_mean torch.Size([256]) 716\n",
      "0.body.layer3.5.bn1.running_var\n",
      "0.body.layer3.5.bn1.running_var torch.Size([256]) 717\n",
      "0.body.layer3.5.bn2.running_mean\n",
      "0.body.layer3.5.bn2.running_mean torch.Size([256]) 718\n",
      "0.body.layer3.5.bn2.running_var\n",
      "0.body.layer3.5.bn2.running_var torch.Size([256]) 719\n",
      "0.body.layer3.5.bn3.running_mean\n",
      "0.body.layer3.5.bn3.running_mean torch.Size([1024]) 720\n",
      "0.body.layer3.5.bn3.running_var\n",
      "0.body.layer3.5.bn3.running_var torch.Size([1024]) 721\n",
      "0.body.layer4.0.bn1.running_mean\n",
      "0.body.layer4.0.bn1.running_mean torch.Size([512]) 722\n",
      "0.body.layer4.0.bn1.running_var\n",
      "0.body.layer4.0.bn1.running_var torch.Size([512]) 723\n",
      "0.body.layer4.0.bn2.running_mean\n",
      "0.body.layer4.0.bn2.running_mean torch.Size([512]) 724\n",
      "0.body.layer4.0.bn2.running_var\n",
      "0.body.layer4.0.bn2.running_var torch.Size([512]) 725\n",
      "0.body.layer4.0.bn3.running_mean\n",
      "0.body.layer4.0.bn3.running_mean torch.Size([2048]) 726\n",
      "0.body.layer4.0.bn3.running_var\n",
      "0.body.layer4.0.bn3.running_var torch.Size([2048]) 727\n",
      "0.body.layer4.0.downsample.1.running_mean\n",
      "0.body.layer4.0.downsample.1.running_mean torch.Size([2048]) 728\n",
      "0.body.layer4.0.downsample.1.running_var\n",
      "0.body.layer4.0.downsample.1.running_var torch.Size([2048]) 729\n",
      "0.body.layer4.1.bn1.running_mean\n",
      "0.body.layer4.1.bn1.running_mean torch.Size([512]) 730\n",
      "0.body.layer4.1.bn1.running_var\n",
      "0.body.layer4.1.bn1.running_var torch.Size([512]) 731\n",
      "0.body.layer4.1.bn2.running_mean\n",
      "0.body.layer4.1.bn2.running_mean torch.Size([512]) 732\n",
      "0.body.layer4.1.bn2.running_var\n",
      "0.body.layer4.1.bn2.running_var torch.Size([512]) 733\n",
      "0.body.layer4.1.bn3.running_mean\n",
      "0.body.layer4.1.bn3.running_mean torch.Size([2048]) 734\n",
      "0.body.layer4.1.bn3.running_var\n",
      "0.body.layer4.1.bn3.running_var torch.Size([2048]) 735\n",
      "0.body.layer4.2.bn1.running_mean\n",
      "0.body.layer4.2.bn1.running_mean torch.Size([512]) 736\n",
      "0.body.layer4.2.bn1.running_var\n",
      "0.body.layer4.2.bn1.running_var torch.Size([512]) 737\n",
      "0.body.layer4.2.bn2.running_mean\n",
      "0.body.layer4.2.bn2.running_mean torch.Size([512]) 738\n",
      "0.body.layer4.2.bn2.running_var\n",
      "0.body.layer4.2.bn2.running_var torch.Size([512]) 739\n",
      "0.body.layer4.2.bn3.running_mean\n",
      "0.body.layer4.2.bn3.running_mean torch.Size([2048]) 740\n",
      "0.body.layer4.2.bn3.running_var\n",
      "0.body.layer4.2.bn3.running_var torch.Size([2048]) 741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.transformer import *\n",
    "class NestedTensor(object):\n",
    "\n",
    "    def __init__(self, tensor, mask):\n",
    "        self.mask = mask\n",
    "        self.tensors = tensor\n",
    "\n",
    "from models.backbone import *\n",
    "filepath = '/private/home/padentomasello/scratch/pytorch_testing/detr_backbone.array'\n",
    "\n",
    "N = 2\n",
    "C = 3\n",
    "H = 224\n",
    "W = 224\n",
    "\n",
    "embedding_size = 8\n",
    "tgt_len = 10\n",
    "\n",
    "queries = torch.rand(tgt_len, embedding_size)\n",
    "image = torch.rand(N, C, H, W)\n",
    "mask = torch.zeros(N, H, W)\n",
    "#mask[0, :20, :20] = 1\n",
    "##mask[1, :4, :10] = 1\n",
    "\n",
    "\n",
    "\n",
    "af.array.save_array('image', toArrayFire(image), filepath, False)\n",
    "#af.array.save_array('queries', toArrayFire(queries), filepath, True)\n",
    "af.array.save_array('mask', toArrayFire(mask), filepath, True)\n",
    "#af.array.save_array('pos', toArrayFire(pos), filepath, True)\n",
    "       \n",
    "\n",
    "\n",
    "model.eval()\n",
    "output = model(NestedTensor(image, mask.to(bool)))[0][0].tensors\n",
    "saveStateDict(model, filepath)\n",
    "af.array.save_array('output', toArrayFire(output), filepath, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0918, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0577, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0475, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]],\n",
       "       grad_fn=<ReluBackward1>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0138, 0.7155, 0.0281, 0.3282],\n",
       "         [0.4167, 0.0306, 0.1975, 0.0623],\n",
       "         [0.0614, 0.9355, 0.1117, 0.0924],\n",
       "         [0.4960, 0.0225, 0.2026, 0.0464],\n",
       "         [0.2465, 0.0329, 0.4336, 0.0678],\n",
       "         [0.2381, 0.4508, 0.4817, 0.9087],\n",
       "         [0.3605, 0.0357, 0.1016, 0.0707],\n",
       "         [0.0819, 0.9070, 0.0848, 0.0790],\n",
       "         [0.0912, 0.0395, 0.1197, 0.0707],\n",
       "         [0.0461, 0.0519, 0.0915, 0.0933],\n",
       "         [0.0868, 0.9752, 0.1390, 0.0505],\n",
       "         [0.0203, 0.7859, 0.0411, 0.2515],\n",
       "         [0.9033, 0.4890, 0.1877, 0.9462],\n",
       "         [0.0494, 0.9197, 0.0891, 0.0795],\n",
       "         [0.2723, 0.9859, 0.1981, 0.0285],\n",
       "         [0.0423, 0.2093, 0.0849, 0.3743],\n",
       "         [0.0300, 0.0474, 0.0603, 0.0820],\n",
       "         [0.8596, 0.0318, 0.2200, 0.0649],\n",
       "         [0.8910, 0.0531, 0.2012, 0.1041],\n",
       "         [0.0634, 0.0670, 0.1242, 0.1220],\n",
       "         [0.1077, 0.8496, 0.1872, 0.2288],\n",
       "         [0.1546, 0.5108, 0.2737, 0.7131],\n",
       "         [0.9884, 0.1421, 0.0233, 0.2512],\n",
       "         [0.9882, 0.8576, 0.0239, 0.2003],\n",
       "         [0.5397, 0.0337, 0.3589, 0.0688],\n",
       "         [0.5169, 0.0529, 0.3883, 0.1059],\n",
       "         [0.9481, 0.2513, 0.1035, 0.4757],\n",
       "         [0.1759, 0.4589, 0.3564, 0.9034],\n",
       "         [0.3670, 0.0277, 0.1116, 0.0561],\n",
       "         [0.0097, 0.1415, 0.0198, 0.2425],\n",
       "         [0.0120, 0.4747, 0.0246, 0.8344],\n",
       "         [0.2135, 0.9754, 0.3109, 0.0495],\n",
       "         [0.0347, 0.9424, 0.0690, 0.0791],\n",
       "         [0.9313, 0.0883, 0.1385, 0.1678],\n",
       "         [0.9733, 0.4653, 0.0535, 0.9118],\n",
       "         [0.3451, 0.0352, 0.1039, 0.0701],\n",
       "         [0.9040, 0.4471, 0.1887, 0.8856],\n",
       "         [0.1305, 0.0948, 0.2580, 0.1722],\n",
       "         [0.1727, 0.9590, 0.1811, 0.0819],\n",
       "         [0.6768, 0.0260, 0.3245, 0.0534],\n",
       "         [0.8839, 0.0535, 0.2028, 0.1054],\n",
       "         [0.9070, 0.1373, 0.1866, 0.2694],\n",
       "         [0.3917, 0.0302, 0.6690, 0.0625],\n",
       "         [0.1641, 0.9068, 0.2972, 0.2017],\n",
       "         [0.0324, 0.1483, 0.0656, 0.2558],\n",
       "         [0.2008, 0.1881, 0.3990, 0.4319],\n",
       "         [0.7771, 0.0382, 0.3957, 0.0784],\n",
       "         [0.4271, 0.9893, 0.1788, 0.0218],\n",
       "         [0.0280, 0.4427, 0.0571, 0.8746],\n",
       "         [0.1479, 0.0462, 0.2723, 0.0920],\n",
       "         [0.1956, 0.0313, 0.3807, 0.0647],\n",
       "         [0.6558, 0.0434, 0.6466, 0.0903],\n",
       "         [0.1509, 0.9776, 0.1808, 0.0452],\n",
       "         [0.2038, 0.0425, 0.3966, 0.0868],\n",
       "         [0.0496, 0.8947, 0.0831, 0.0853],\n",
       "         [0.1198, 0.4615, 0.2430, 0.8853],\n",
       "         [0.0271, 0.2624, 0.0539, 0.0802],\n",
       "         [0.4122, 0.0316, 0.2688, 0.0643],\n",
       "         [0.2995, 0.0331, 0.4962, 0.0683],\n",
       "         [0.3447, 0.4615, 0.6690, 0.9475],\n",
       "         [0.1574, 0.0535, 0.3125, 0.1068],\n",
       "         [0.1204, 0.4791, 0.2453, 0.9341],\n",
       "         [0.9684, 0.0505, 0.0633, 0.1003],\n",
       "         [0.0611, 0.0595, 0.1129, 0.0998],\n",
       "         [0.8774, 0.0330, 0.1961, 0.0665],\n",
       "         [0.1085, 0.9696, 0.1186, 0.0609],\n",
       "         [0.0146, 0.0634, 0.0298, 0.1082],\n",
       "         [0.0604, 0.4587, 0.1213, 0.8875],\n",
       "         [0.1363, 0.0417, 0.2241, 0.0830],\n",
       "         [0.9737, 0.2175, 0.0529, 0.3715],\n",
       "         [0.9452, 0.0446, 0.1125, 0.0888],\n",
       "         [0.3000, 0.4705, 0.6120, 0.9511],\n",
       "         [0.1328, 0.4171, 0.2721, 0.8349],\n",
       "         [0.6289, 0.0294, 0.2924, 0.0601],\n",
       "         [0.1165, 0.9809, 0.1997, 0.0385],\n",
       "         [0.6638, 0.4547, 0.6552, 0.9159],\n",
       "         [0.0587, 0.3397, 0.1176, 0.6735],\n",
       "         [0.0119, 0.8619, 0.0241, 0.1548],\n",
       "         [0.2854, 0.0291, 0.1762, 0.0592],\n",
       "         [0.0207, 0.0660, 0.0419, 0.1058],\n",
       "         [0.9671, 0.5433, 0.0648, 0.7633],\n",
       "         [0.2396, 0.0405, 0.3661, 0.0826],\n",
       "         [0.9903, 0.4399, 0.0197, 0.8211],\n",
       "         [0.3399, 0.0311, 0.6657, 0.0647],\n",
       "         [0.0649, 0.0992, 0.1266, 0.1611],\n",
       "         [0.9579, 0.9688, 0.0864, 0.0648],\n",
       "         [0.9817, 0.7589, 0.0366, 0.3342],\n",
       "         [0.4995, 0.0923, 1.0000, 0.1905],\n",
       "         [0.0587, 0.8707, 0.1091, 0.1805],\n",
       "         [0.3572, 0.0374, 0.3357, 0.0758],\n",
       "         [0.3743, 0.0393, 0.7284, 0.0803],\n",
       "         [0.3349, 0.3178, 0.5898, 0.6407],\n",
       "         [0.3950, 0.4684, 0.7960, 0.9419],\n",
       "         [0.4416, 0.0295, 0.1735, 0.0602],\n",
       "         [0.5080, 0.0334, 0.2313, 0.0680],\n",
       "         [0.2072, 0.9705, 0.3746, 0.0609],\n",
       "         [0.0484, 0.0583, 0.0915, 0.0907],\n",
       "         [0.4993, 0.0297, 0.9558, 0.0633],\n",
       "         [0.4948, 0.4929, 0.9950, 0.9722],\n",
       "         [0.5634, 0.0270, 0.2403, 0.0554]],\n",
       "\n",
       "        [[0.0142, 0.6589, 0.0288, 0.2517],\n",
       "         [0.4249, 0.0350, 0.1809, 0.0698],\n",
       "         [0.0622, 0.9082, 0.1178, 0.1245],\n",
       "         [0.5348, 0.0226, 0.1653, 0.0459],\n",
       "         [0.2719, 0.0325, 0.4807, 0.0671],\n",
       "         [0.2689, 0.4156, 0.5410, 0.8569],\n",
       "         [0.3958, 0.0307, 0.1315, 0.0609],\n",
       "         [0.0935, 0.9229, 0.0949, 0.0731],\n",
       "         [0.0929, 0.0413, 0.1215, 0.0698],\n",
       "         [0.0454, 0.0542, 0.0902, 0.0900],\n",
       "         [0.7817, 0.9824, 0.2508, 0.0354],\n",
       "         [0.0188, 0.7830, 0.0381, 0.2419],\n",
       "         [0.9284, 0.4850, 0.1407, 0.9176],\n",
       "         [0.9421, 0.9240, 0.1098, 0.1038],\n",
       "         [0.2778, 0.9881, 0.1748, 0.0241],\n",
       "         [0.0413, 0.1554, 0.0829, 0.2729],\n",
       "         [0.0302, 0.0504, 0.0607, 0.0806],\n",
       "         [0.8631, 0.0308, 0.2120, 0.0628],\n",
       "         [0.9131, 0.1706, 0.1436, 0.1677],\n",
       "         [0.0598, 0.0786, 0.1178, 0.1350],\n",
       "         [0.8983, 0.9224, 0.1597, 0.1216],\n",
       "         [0.8814, 0.7042, 0.1836, 0.4485],\n",
       "         [0.9909, 0.1434, 0.0183, 0.2499],\n",
       "         [0.9882, 0.8620, 0.0238, 0.1990],\n",
       "         [0.5672, 0.0333, 0.3273, 0.0681],\n",
       "         [0.5573, 0.0496, 0.3719, 0.0983],\n",
       "         [0.9677, 0.2327, 0.0652, 0.4243],\n",
       "         [0.1806, 0.4726, 0.3663, 0.9202],\n",
       "         [0.4198, 0.0249, 0.1622, 0.0502],\n",
       "         [0.0086, 0.1321, 0.0175, 0.2189],\n",
       "         [0.0108, 0.4760, 0.0222, 0.8181],\n",
       "         [0.2623, 0.9785, 0.2372, 0.0423],\n",
       "         [0.0350, 0.9539, 0.0699, 0.0692],\n",
       "         [0.9504, 0.1066, 0.0997, 0.1950],\n",
       "         [0.9778, 0.4628, 0.0447, 0.8965],\n",
       "         [0.3637, 0.0343, 0.0999, 0.0671],\n",
       "         [0.9233, 0.4389, 0.1517, 0.8568],\n",
       "         [0.1284, 0.0869, 0.2519, 0.1522],\n",
       "         [0.2268, 0.9302, 0.2456, 0.1433],\n",
       "         [0.6978, 0.0241, 0.2075, 0.0495],\n",
       "         [0.8995, 0.0662, 0.1887, 0.1274],\n",
       "         [0.9281, 0.1158, 0.1442, 0.2225],\n",
       "         [0.4329, 0.0331, 0.4865, 0.0682],\n",
       "         [0.1805, 0.9401, 0.3228, 0.1299],\n",
       "         [0.0306, 0.1424, 0.0618, 0.2378],\n",
       "         [0.2528, 0.1496, 0.5112, 0.3304],\n",
       "         [0.7990, 0.0477, 0.3801, 0.0977],\n",
       "         [0.4448, 0.9897, 0.1803, 0.0209],\n",
       "         [0.0262, 0.4376, 0.0536, 0.8542],\n",
       "         [0.1711, 0.0456, 0.2984, 0.0910],\n",
       "         [0.1971, 0.0323, 0.3752, 0.0665],\n",
       "         [0.6887, 0.0445, 0.5555, 0.0921],\n",
       "         [0.1522, 0.9807, 0.2038, 0.0391],\n",
       "         [0.2301, 0.0424, 0.3347, 0.0859],\n",
       "         [0.0524, 0.8998, 0.0864, 0.0779],\n",
       "         [0.1117, 0.4824, 0.2260, 0.8791],\n",
       "         [0.0329, 0.0925, 0.0651, 0.0921],\n",
       "         [0.4306, 0.0383, 0.2034, 0.0763],\n",
       "         [0.3718, 0.0426, 0.3704, 0.0863],\n",
       "         [0.3196, 0.4652, 0.6362, 0.9482],\n",
       "         [0.1784, 0.0544, 0.3471, 0.1082],\n",
       "         [0.1296, 0.4866, 0.2659, 0.9359],\n",
       "         [0.9815, 0.0547, 0.0373, 0.1071],\n",
       "         [0.0612, 0.0633, 0.1112, 0.0960],\n",
       "         [0.8813, 0.0331, 0.1839, 0.0665],\n",
       "         [0.1459, 0.9758, 0.1630, 0.0492],\n",
       "         [0.0135, 0.0650, 0.0276, 0.1017],\n",
       "         [0.0593, 0.4611, 0.1192, 0.8773],\n",
       "         [0.1380, 0.0435, 0.2094, 0.0841],\n",
       "         [0.9835, 0.2274, 0.0334, 0.3755],\n",
       "         [0.9587, 0.0525, 0.0832, 0.1018],\n",
       "         [0.3353, 0.4717, 0.6764, 0.9533],\n",
       "         [0.1350, 0.4278, 0.2782, 0.8455],\n",
       "         [0.6766, 0.0323, 0.2829, 0.0657],\n",
       "         [0.1837, 0.9825, 0.3388, 0.0356],\n",
       "         [0.6864, 0.4279, 0.6207, 0.8633],\n",
       "         [0.0586, 0.3532, 0.1171, 0.6829],\n",
       "         [0.0108, 0.8739, 0.0218, 0.1368],\n",
       "         [0.3215, 0.0302, 0.1179, 0.0602],\n",
       "         [0.0201, 0.0695, 0.0407, 0.0995],\n",
       "         [0.9790, 0.7827, 0.0425, 0.3622],\n",
       "         [0.2941, 0.0433, 0.2522, 0.0866],\n",
       "         [0.9913, 0.4445, 0.0178, 0.8020],\n",
       "         [0.3494, 0.0348, 0.6893, 0.0724],\n",
       "         [0.0668, 0.0818, 0.1174, 0.1155],\n",
       "         [0.9577, 0.9629, 0.0845, 0.0754],\n",
       "         [0.9856, 0.7712, 0.0291, 0.3342],\n",
       "         [0.4994, 0.0450, 1.0000, 0.0931],\n",
       "         [0.0602, 0.8847, 0.1101, 0.1497],\n",
       "         [0.4051, 0.0422, 0.2817, 0.0846],\n",
       "         [0.4053, 0.0401, 0.6054, 0.0824],\n",
       "         [0.8812, 0.2456, 0.2444, 0.4855],\n",
       "         [0.4995, 0.4641, 0.9993, 0.9345],\n",
       "         [0.4926, 0.0341, 0.1333, 0.0676],\n",
       "         [0.5498, 0.0380, 0.1919, 0.0756],\n",
       "         [0.2587, 0.9728, 0.4833, 0.0563],\n",
       "         [0.0490, 0.0630, 0.0909, 0.0854],\n",
       "         [0.4984, 0.0286, 0.9345, 0.0607],\n",
       "         [0.4996, 0.4886, 0.9965, 0.9673],\n",
       "         [0.5652, 0.0288, 0.2406, 0.0590]]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"pred_boxes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['transformer.encoder.layers.0.self_attn.in_proj_weight', 'transformer.encoder.layers.0.self_attn.in_proj_bias', 'transformer.encoder.layers.0.self_attn.out_proj.weight', 'transformer.encoder.layers.0.self_attn.out_proj.bias', 'transformer.encoder.layers.0.linear1.weight', 'transformer.encoder.layers.0.linear1.bias', 'transformer.encoder.layers.0.linear2.weight', 'transformer.encoder.layers.0.linear2.bias', 'transformer.encoder.layers.0.norm1.weight', 'transformer.encoder.layers.0.norm1.bias', 'transformer.encoder.layers.0.norm2.weight', 'transformer.encoder.layers.0.norm2.bias', 'transformer.encoder.layers.1.self_attn.in_proj_weight', 'transformer.encoder.layers.1.self_attn.in_proj_bias', 'transformer.encoder.layers.1.self_attn.out_proj.weight', 'transformer.encoder.layers.1.self_attn.out_proj.bias', 'transformer.encoder.layers.1.linear1.weight', 'transformer.encoder.layers.1.linear1.bias', 'transformer.encoder.layers.1.linear2.weight', 'transformer.encoder.layers.1.linear2.bias', 'transformer.encoder.layers.1.norm1.weight', 'transformer.encoder.layers.1.norm1.bias', 'transformer.encoder.layers.1.norm2.weight', 'transformer.encoder.layers.1.norm2.bias', 'transformer.encoder.layers.2.self_attn.in_proj_weight', 'transformer.encoder.layers.2.self_attn.in_proj_bias', 'transformer.encoder.layers.2.self_attn.out_proj.weight', 'transformer.encoder.layers.2.self_attn.out_proj.bias', 'transformer.encoder.layers.2.linear1.weight', 'transformer.encoder.layers.2.linear1.bias', 'transformer.encoder.layers.2.linear2.weight', 'transformer.encoder.layers.2.linear2.bias', 'transformer.encoder.layers.2.norm1.weight', 'transformer.encoder.layers.2.norm1.bias', 'transformer.encoder.layers.2.norm2.weight', 'transformer.encoder.layers.2.norm2.bias', 'transformer.encoder.layers.3.self_attn.in_proj_weight', 'transformer.encoder.layers.3.self_attn.in_proj_bias', 'transformer.encoder.layers.3.self_attn.out_proj.weight', 'transformer.encoder.layers.3.self_attn.out_proj.bias', 'transformer.encoder.layers.3.linear1.weight', 'transformer.encoder.layers.3.linear1.bias', 'transformer.encoder.layers.3.linear2.weight', 'transformer.encoder.layers.3.linear2.bias', 'transformer.encoder.layers.3.norm1.weight', 'transformer.encoder.layers.3.norm1.bias', 'transformer.encoder.layers.3.norm2.weight', 'transformer.encoder.layers.3.norm2.bias', 'transformer.encoder.layers.4.self_attn.in_proj_weight', 'transformer.encoder.layers.4.self_attn.in_proj_bias', 'transformer.encoder.layers.4.self_attn.out_proj.weight', 'transformer.encoder.layers.4.self_attn.out_proj.bias', 'transformer.encoder.layers.4.linear1.weight', 'transformer.encoder.layers.4.linear1.bias', 'transformer.encoder.layers.4.linear2.weight', 'transformer.encoder.layers.4.linear2.bias', 'transformer.encoder.layers.4.norm1.weight', 'transformer.encoder.layers.4.norm1.bias', 'transformer.encoder.layers.4.norm2.weight', 'transformer.encoder.layers.4.norm2.bias', 'transformer.encoder.layers.5.self_attn.in_proj_weight', 'transformer.encoder.layers.5.self_attn.in_proj_bias', 'transformer.encoder.layers.5.self_attn.out_proj.weight', 'transformer.encoder.layers.5.self_attn.out_proj.bias', 'transformer.encoder.layers.5.linear1.weight', 'transformer.encoder.layers.5.linear1.bias', 'transformer.encoder.layers.5.linear2.weight', 'transformer.encoder.layers.5.linear2.bias', 'transformer.encoder.layers.5.norm1.weight', 'transformer.encoder.layers.5.norm1.bias', 'transformer.encoder.layers.5.norm2.weight', 'transformer.encoder.layers.5.norm2.bias', 'transformer.decoder.layers.0.self_attn.in_proj_weight', 'transformer.decoder.layers.0.self_attn.in_proj_bias', 'transformer.decoder.layers.0.self_attn.out_proj.weight', 'transformer.decoder.layers.0.self_attn.out_proj.bias', 'transformer.decoder.layers.0.multihead_attn.in_proj_weight', 'transformer.decoder.layers.0.multihead_attn.in_proj_bias', 'transformer.decoder.layers.0.multihead_attn.out_proj.weight', 'transformer.decoder.layers.0.multihead_attn.out_proj.bias', 'transformer.decoder.layers.0.linear1.weight', 'transformer.decoder.layers.0.linear1.bias', 'transformer.decoder.layers.0.linear2.weight', 'transformer.decoder.layers.0.linear2.bias', 'transformer.decoder.layers.0.norm1.weight', 'transformer.decoder.layers.0.norm1.bias', 'transformer.decoder.layers.0.norm2.weight', 'transformer.decoder.layers.0.norm2.bias', 'transformer.decoder.layers.0.norm3.weight', 'transformer.decoder.layers.0.norm3.bias', 'transformer.decoder.layers.1.self_attn.in_proj_weight', 'transformer.decoder.layers.1.self_attn.in_proj_bias', 'transformer.decoder.layers.1.self_attn.out_proj.weight', 'transformer.decoder.layers.1.self_attn.out_proj.bias', 'transformer.decoder.layers.1.multihead_attn.in_proj_weight', 'transformer.decoder.layers.1.multihead_attn.in_proj_bias', 'transformer.decoder.layers.1.multihead_attn.out_proj.weight', 'transformer.decoder.layers.1.multihead_attn.out_proj.bias', 'transformer.decoder.layers.1.linear1.weight', 'transformer.decoder.layers.1.linear1.bias', 'transformer.decoder.layers.1.linear2.weight', 'transformer.decoder.layers.1.linear2.bias', 'transformer.decoder.layers.1.norm1.weight', 'transformer.decoder.layers.1.norm1.bias', 'transformer.decoder.layers.1.norm2.weight', 'transformer.decoder.layers.1.norm2.bias', 'transformer.decoder.layers.1.norm3.weight', 'transformer.decoder.layers.1.norm3.bias', 'transformer.decoder.layers.2.self_attn.in_proj_weight', 'transformer.decoder.layers.2.self_attn.in_proj_bias', 'transformer.decoder.layers.2.self_attn.out_proj.weight', 'transformer.decoder.layers.2.self_attn.out_proj.bias', 'transformer.decoder.layers.2.multihead_attn.in_proj_weight', 'transformer.decoder.layers.2.multihead_attn.in_proj_bias', 'transformer.decoder.layers.2.multihead_attn.out_proj.weight', 'transformer.decoder.layers.2.multihead_attn.out_proj.bias', 'transformer.decoder.layers.2.linear1.weight', 'transformer.decoder.layers.2.linear1.bias', 'transformer.decoder.layers.2.linear2.weight', 'transformer.decoder.layers.2.linear2.bias', 'transformer.decoder.layers.2.norm1.weight', 'transformer.decoder.layers.2.norm1.bias', 'transformer.decoder.layers.2.norm2.weight', 'transformer.decoder.layers.2.norm2.bias', 'transformer.decoder.layers.2.norm3.weight', 'transformer.decoder.layers.2.norm3.bias', 'transformer.decoder.layers.3.self_attn.in_proj_weight', 'transformer.decoder.layers.3.self_attn.in_proj_bias', 'transformer.decoder.layers.3.self_attn.out_proj.weight', 'transformer.decoder.layers.3.self_attn.out_proj.bias', 'transformer.decoder.layers.3.multihead_attn.in_proj_weight', 'transformer.decoder.layers.3.multihead_attn.in_proj_bias', 'transformer.decoder.layers.3.multihead_attn.out_proj.weight', 'transformer.decoder.layers.3.multihead_attn.out_proj.bias', 'transformer.decoder.layers.3.linear1.weight', 'transformer.decoder.layers.3.linear1.bias', 'transformer.decoder.layers.3.linear2.weight', 'transformer.decoder.layers.3.linear2.bias', 'transformer.decoder.layers.3.norm1.weight', 'transformer.decoder.layers.3.norm1.bias', 'transformer.decoder.layers.3.norm2.weight', 'transformer.decoder.layers.3.norm2.bias', 'transformer.decoder.layers.3.norm3.weight', 'transformer.decoder.layers.3.norm3.bias', 'transformer.decoder.layers.4.self_attn.in_proj_weight', 'transformer.decoder.layers.4.self_attn.in_proj_bias', 'transformer.decoder.layers.4.self_attn.out_proj.weight', 'transformer.decoder.layers.4.self_attn.out_proj.bias', 'transformer.decoder.layers.4.multihead_attn.in_proj_weight', 'transformer.decoder.layers.4.multihead_attn.in_proj_bias', 'transformer.decoder.layers.4.multihead_attn.out_proj.weight', 'transformer.decoder.layers.4.multihead_attn.out_proj.bias', 'transformer.decoder.layers.4.linear1.weight', 'transformer.decoder.layers.4.linear1.bias', 'transformer.decoder.layers.4.linear2.weight', 'transformer.decoder.layers.4.linear2.bias', 'transformer.decoder.layers.4.norm1.weight', 'transformer.decoder.layers.4.norm1.bias', 'transformer.decoder.layers.4.norm2.weight', 'transformer.decoder.layers.4.norm2.bias', 'transformer.decoder.layers.4.norm3.weight', 'transformer.decoder.layers.4.norm3.bias', 'transformer.decoder.layers.5.self_attn.in_proj_weight', 'transformer.decoder.layers.5.self_attn.in_proj_bias', 'transformer.decoder.layers.5.self_attn.out_proj.weight', 'transformer.decoder.layers.5.self_attn.out_proj.bias', 'transformer.decoder.layers.5.multihead_attn.in_proj_weight', 'transformer.decoder.layers.5.multihead_attn.in_proj_bias', 'transformer.decoder.layers.5.multihead_attn.out_proj.weight', 'transformer.decoder.layers.5.multihead_attn.out_proj.bias', 'transformer.decoder.layers.5.linear1.weight', 'transformer.decoder.layers.5.linear1.bias', 'transformer.decoder.layers.5.linear2.weight', 'transformer.decoder.layers.5.linear2.bias', 'transformer.decoder.layers.5.norm1.weight', 'transformer.decoder.layers.5.norm1.bias', 'transformer.decoder.layers.5.norm2.weight', 'transformer.decoder.layers.5.norm2.bias', 'transformer.decoder.layers.5.norm3.weight', 'transformer.decoder.layers.5.norm3.bias', 'transformer.decoder.norm.weight', 'transformer.decoder.norm.bias', 'class_embed.weight', 'class_embed.bias', 'bbox_embed.layers.0.weight', 'bbox_embed.layers.0.bias', 'bbox_embed.layers.1.weight', 'bbox_embed.layers.1.bias', 'bbox_embed.layers.2.weight', 'bbox_embed.layers.2.bias', 'query_embed.weight', 'input_proj.weight', 'input_proj.bias', 'backbone.0.body.conv1.weight', 'backbone.0.body.bn1.weight', 'backbone.0.body.bn1.bias', 'backbone.0.body.bn1.running_mean', 'backbone.0.body.bn1.running_var', 'backbone.0.body.layer1.0.conv1.weight', 'backbone.0.body.layer1.0.bn1.weight', 'backbone.0.body.layer1.0.bn1.bias', 'backbone.0.body.layer1.0.bn1.running_mean', 'backbone.0.body.layer1.0.bn1.running_var', 'backbone.0.body.layer1.0.conv2.weight', 'backbone.0.body.layer1.0.bn2.weight', 'backbone.0.body.layer1.0.bn2.bias', 'backbone.0.body.layer1.0.bn2.running_mean', 'backbone.0.body.layer1.0.bn2.running_var', 'backbone.0.body.layer1.0.conv3.weight', 'backbone.0.body.layer1.0.bn3.weight', 'backbone.0.body.layer1.0.bn3.bias', 'backbone.0.body.layer1.0.bn3.running_mean', 'backbone.0.body.layer1.0.bn3.running_var', 'backbone.0.body.layer1.0.downsample.0.weight', 'backbone.0.body.layer1.0.downsample.1.weight', 'backbone.0.body.layer1.0.downsample.1.bias', 'backbone.0.body.layer1.0.downsample.1.running_mean', 'backbone.0.body.layer1.0.downsample.1.running_var', 'backbone.0.body.layer1.1.conv1.weight', 'backbone.0.body.layer1.1.bn1.weight', 'backbone.0.body.layer1.1.bn1.bias', 'backbone.0.body.layer1.1.bn1.running_mean', 'backbone.0.body.layer1.1.bn1.running_var', 'backbone.0.body.layer1.1.conv2.weight', 'backbone.0.body.layer1.1.bn2.weight', 'backbone.0.body.layer1.1.bn2.bias', 'backbone.0.body.layer1.1.bn2.running_mean', 'backbone.0.body.layer1.1.bn2.running_var', 'backbone.0.body.layer1.1.conv3.weight', 'backbone.0.body.layer1.1.bn3.weight', 'backbone.0.body.layer1.1.bn3.bias', 'backbone.0.body.layer1.1.bn3.running_mean', 'backbone.0.body.layer1.1.bn3.running_var', 'backbone.0.body.layer1.2.conv1.weight', 'backbone.0.body.layer1.2.bn1.weight', 'backbone.0.body.layer1.2.bn1.bias', 'backbone.0.body.layer1.2.bn1.running_mean', 'backbone.0.body.layer1.2.bn1.running_var', 'backbone.0.body.layer1.2.conv2.weight', 'backbone.0.body.layer1.2.bn2.weight', 'backbone.0.body.layer1.2.bn2.bias', 'backbone.0.body.layer1.2.bn2.running_mean', 'backbone.0.body.layer1.2.bn2.running_var', 'backbone.0.body.layer1.2.conv3.weight', 'backbone.0.body.layer1.2.bn3.weight', 'backbone.0.body.layer1.2.bn3.bias', 'backbone.0.body.layer1.2.bn3.running_mean', 'backbone.0.body.layer1.2.bn3.running_var', 'backbone.0.body.layer2.0.conv1.weight', 'backbone.0.body.layer2.0.bn1.weight', 'backbone.0.body.layer2.0.bn1.bias', 'backbone.0.body.layer2.0.bn1.running_mean', 'backbone.0.body.layer2.0.bn1.running_var', 'backbone.0.body.layer2.0.conv2.weight', 'backbone.0.body.layer2.0.bn2.weight', 'backbone.0.body.layer2.0.bn2.bias', 'backbone.0.body.layer2.0.bn2.running_mean', 'backbone.0.body.layer2.0.bn2.running_var', 'backbone.0.body.layer2.0.conv3.weight', 'backbone.0.body.layer2.0.bn3.weight', 'backbone.0.body.layer2.0.bn3.bias', 'backbone.0.body.layer2.0.bn3.running_mean', 'backbone.0.body.layer2.0.bn3.running_var', 'backbone.0.body.layer2.0.downsample.0.weight', 'backbone.0.body.layer2.0.downsample.1.weight', 'backbone.0.body.layer2.0.downsample.1.bias', 'backbone.0.body.layer2.0.downsample.1.running_mean', 'backbone.0.body.layer2.0.downsample.1.running_var', 'backbone.0.body.layer2.1.conv1.weight', 'backbone.0.body.layer2.1.bn1.weight', 'backbone.0.body.layer2.1.bn1.bias', 'backbone.0.body.layer2.1.bn1.running_mean', 'backbone.0.body.layer2.1.bn1.running_var', 'backbone.0.body.layer2.1.conv2.weight', 'backbone.0.body.layer2.1.bn2.weight', 'backbone.0.body.layer2.1.bn2.bias', 'backbone.0.body.layer2.1.bn2.running_mean', 'backbone.0.body.layer2.1.bn2.running_var', 'backbone.0.body.layer2.1.conv3.weight', 'backbone.0.body.layer2.1.bn3.weight', 'backbone.0.body.layer2.1.bn3.bias', 'backbone.0.body.layer2.1.bn3.running_mean', 'backbone.0.body.layer2.1.bn3.running_var', 'backbone.0.body.layer2.2.conv1.weight', 'backbone.0.body.layer2.2.bn1.weight', 'backbone.0.body.layer2.2.bn1.bias', 'backbone.0.body.layer2.2.bn1.running_mean', 'backbone.0.body.layer2.2.bn1.running_var', 'backbone.0.body.layer2.2.conv2.weight', 'backbone.0.body.layer2.2.bn2.weight', 'backbone.0.body.layer2.2.bn2.bias', 'backbone.0.body.layer2.2.bn2.running_mean', 'backbone.0.body.layer2.2.bn2.running_var', 'backbone.0.body.layer2.2.conv3.weight', 'backbone.0.body.layer2.2.bn3.weight', 'backbone.0.body.layer2.2.bn3.bias', 'backbone.0.body.layer2.2.bn3.running_mean', 'backbone.0.body.layer2.2.bn3.running_var', 'backbone.0.body.layer2.3.conv1.weight', 'backbone.0.body.layer2.3.bn1.weight', 'backbone.0.body.layer2.3.bn1.bias', 'backbone.0.body.layer2.3.bn1.running_mean', 'backbone.0.body.layer2.3.bn1.running_var', 'backbone.0.body.layer2.3.conv2.weight', 'backbone.0.body.layer2.3.bn2.weight', 'backbone.0.body.layer2.3.bn2.bias', 'backbone.0.body.layer2.3.bn2.running_mean', 'backbone.0.body.layer2.3.bn2.running_var', 'backbone.0.body.layer2.3.conv3.weight', 'backbone.0.body.layer2.3.bn3.weight', 'backbone.0.body.layer2.3.bn3.bias', 'backbone.0.body.layer2.3.bn3.running_mean', 'backbone.0.body.layer2.3.bn3.running_var', 'backbone.0.body.layer3.0.conv1.weight', 'backbone.0.body.layer3.0.bn1.weight', 'backbone.0.body.layer3.0.bn1.bias', 'backbone.0.body.layer3.0.bn1.running_mean', 'backbone.0.body.layer3.0.bn1.running_var', 'backbone.0.body.layer3.0.conv2.weight', 'backbone.0.body.layer3.0.bn2.weight', 'backbone.0.body.layer3.0.bn2.bias', 'backbone.0.body.layer3.0.bn2.running_mean', 'backbone.0.body.layer3.0.bn2.running_var', 'backbone.0.body.layer3.0.conv3.weight', 'backbone.0.body.layer3.0.bn3.weight', 'backbone.0.body.layer3.0.bn3.bias', 'backbone.0.body.layer3.0.bn3.running_mean', 'backbone.0.body.layer3.0.bn3.running_var', 'backbone.0.body.layer3.0.downsample.0.weight', 'backbone.0.body.layer3.0.downsample.1.weight', 'backbone.0.body.layer3.0.downsample.1.bias', 'backbone.0.body.layer3.0.downsample.1.running_mean', 'backbone.0.body.layer3.0.downsample.1.running_var', 'backbone.0.body.layer3.1.conv1.weight', 'backbone.0.body.layer3.1.bn1.weight', 'backbone.0.body.layer3.1.bn1.bias', 'backbone.0.body.layer3.1.bn1.running_mean', 'backbone.0.body.layer3.1.bn1.running_var', 'backbone.0.body.layer3.1.conv2.weight', 'backbone.0.body.layer3.1.bn2.weight', 'backbone.0.body.layer3.1.bn2.bias', 'backbone.0.body.layer3.1.bn2.running_mean', 'backbone.0.body.layer3.1.bn2.running_var', 'backbone.0.body.layer3.1.conv3.weight', 'backbone.0.body.layer3.1.bn3.weight', 'backbone.0.body.layer3.1.bn3.bias', 'backbone.0.body.layer3.1.bn3.running_mean', 'backbone.0.body.layer3.1.bn3.running_var', 'backbone.0.body.layer3.2.conv1.weight', 'backbone.0.body.layer3.2.bn1.weight', 'backbone.0.body.layer3.2.bn1.bias', 'backbone.0.body.layer3.2.bn1.running_mean', 'backbone.0.body.layer3.2.bn1.running_var', 'backbone.0.body.layer3.2.conv2.weight', 'backbone.0.body.layer3.2.bn2.weight', 'backbone.0.body.layer3.2.bn2.bias', 'backbone.0.body.layer3.2.bn2.running_mean', 'backbone.0.body.layer3.2.bn2.running_var', 'backbone.0.body.layer3.2.conv3.weight', 'backbone.0.body.layer3.2.bn3.weight', 'backbone.0.body.layer3.2.bn3.bias', 'backbone.0.body.layer3.2.bn3.running_mean', 'backbone.0.body.layer3.2.bn3.running_var', 'backbone.0.body.layer3.3.conv1.weight', 'backbone.0.body.layer3.3.bn1.weight', 'backbone.0.body.layer3.3.bn1.bias', 'backbone.0.body.layer3.3.bn1.running_mean', 'backbone.0.body.layer3.3.bn1.running_var', 'backbone.0.body.layer3.3.conv2.weight', 'backbone.0.body.layer3.3.bn2.weight', 'backbone.0.body.layer3.3.bn2.bias', 'backbone.0.body.layer3.3.bn2.running_mean', 'backbone.0.body.layer3.3.bn2.running_var', 'backbone.0.body.layer3.3.conv3.weight', 'backbone.0.body.layer3.3.bn3.weight', 'backbone.0.body.layer3.3.bn3.bias', 'backbone.0.body.layer3.3.bn3.running_mean', 'backbone.0.body.layer3.3.bn3.running_var', 'backbone.0.body.layer3.4.conv1.weight', 'backbone.0.body.layer3.4.bn1.weight', 'backbone.0.body.layer3.4.bn1.bias', 'backbone.0.body.layer3.4.bn1.running_mean', 'backbone.0.body.layer3.4.bn1.running_var', 'backbone.0.body.layer3.4.conv2.weight', 'backbone.0.body.layer3.4.bn2.weight', 'backbone.0.body.layer3.4.bn2.bias', 'backbone.0.body.layer3.4.bn2.running_mean', 'backbone.0.body.layer3.4.bn2.running_var', 'backbone.0.body.layer3.4.conv3.weight', 'backbone.0.body.layer3.4.bn3.weight', 'backbone.0.body.layer3.4.bn3.bias', 'backbone.0.body.layer3.4.bn3.running_mean', 'backbone.0.body.layer3.4.bn3.running_var', 'backbone.0.body.layer3.5.conv1.weight', 'backbone.0.body.layer3.5.bn1.weight', 'backbone.0.body.layer3.5.bn1.bias', 'backbone.0.body.layer3.5.bn1.running_mean', 'backbone.0.body.layer3.5.bn1.running_var', 'backbone.0.body.layer3.5.conv2.weight', 'backbone.0.body.layer3.5.bn2.weight', 'backbone.0.body.layer3.5.bn2.bias', 'backbone.0.body.layer3.5.bn2.running_mean', 'backbone.0.body.layer3.5.bn2.running_var', 'backbone.0.body.layer3.5.conv3.weight', 'backbone.0.body.layer3.5.bn3.weight', 'backbone.0.body.layer3.5.bn3.bias', 'backbone.0.body.layer3.5.bn3.running_mean', 'backbone.0.body.layer3.5.bn3.running_var', 'backbone.0.body.layer4.0.conv1.weight', 'backbone.0.body.layer4.0.bn1.weight', 'backbone.0.body.layer4.0.bn1.bias', 'backbone.0.body.layer4.0.bn1.running_mean', 'backbone.0.body.layer4.0.bn1.running_var', 'backbone.0.body.layer4.0.conv2.weight', 'backbone.0.body.layer4.0.bn2.weight', 'backbone.0.body.layer4.0.bn2.bias', 'backbone.0.body.layer4.0.bn2.running_mean', 'backbone.0.body.layer4.0.bn2.running_var', 'backbone.0.body.layer4.0.conv3.weight', 'backbone.0.body.layer4.0.bn3.weight', 'backbone.0.body.layer4.0.bn3.bias', 'backbone.0.body.layer4.0.bn3.running_mean', 'backbone.0.body.layer4.0.bn3.running_var', 'backbone.0.body.layer4.0.downsample.0.weight', 'backbone.0.body.layer4.0.downsample.1.weight', 'backbone.0.body.layer4.0.downsample.1.bias', 'backbone.0.body.layer4.0.downsample.1.running_mean', 'backbone.0.body.layer4.0.downsample.1.running_var', 'backbone.0.body.layer4.1.conv1.weight', 'backbone.0.body.layer4.1.bn1.weight', 'backbone.0.body.layer4.1.bn1.bias', 'backbone.0.body.layer4.1.bn1.running_mean', 'backbone.0.body.layer4.1.bn1.running_var', 'backbone.0.body.layer4.1.conv2.weight', 'backbone.0.body.layer4.1.bn2.weight', 'backbone.0.body.layer4.1.bn2.bias', 'backbone.0.body.layer4.1.bn2.running_mean', 'backbone.0.body.layer4.1.bn2.running_var', 'backbone.0.body.layer4.1.conv3.weight', 'backbone.0.body.layer4.1.bn3.weight', 'backbone.0.body.layer4.1.bn3.bias', 'backbone.0.body.layer4.1.bn3.running_mean', 'backbone.0.body.layer4.1.bn3.running_var', 'backbone.0.body.layer4.2.conv1.weight', 'backbone.0.body.layer4.2.bn1.weight', 'backbone.0.body.layer4.2.bn1.bias', 'backbone.0.body.layer4.2.bn1.running_mean', 'backbone.0.body.layer4.2.bn1.running_var', 'backbone.0.body.layer4.2.conv2.weight', 'backbone.0.body.layer4.2.bn2.weight', 'backbone.0.body.layer4.2.bn2.bias', 'backbone.0.body.layer4.2.bn2.running_mean', 'backbone.0.body.layer4.2.bn2.running_var', 'backbone.0.body.layer4.2.conv3.weight', 'backbone.0.body.layer4.2.bn3.weight', 'backbone.0.body.layer4.2.bn3.bias', 'backbone.0.body.layer4.2.bn3.running_mean', 'backbone.0.body.layer4.2.bn3.running_var'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d34bded2808c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNestedTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "output = model(NestedTensor(x, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0][0].tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
