{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arrayfire as af\n",
    "import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toArrayFire(x):\n",
    "    x_np = x.detach().contiguous().numpy()\n",
    "    shape = 1\n",
    "    if len(x_np.shape) == 0:\n",
    "        shape = (1,)\n",
    "    else:\n",
    "        shape = x_np.shape[::-1]\n",
    "    afArray = af.Array(x_np.ctypes.data, shape, x_np.dtype.char)\n",
    "    return afArray\n",
    "\n",
    "def saveStateDict(module, filepath):\n",
    "    i = 0\n",
    "    for (name, param) in module.named_parameters():\n",
    "        #param = module.state_dict()[name]\n",
    "        print(name, \"\\t\", param.size())\n",
    "        if 'in_proj' in name:\n",
    "            print(param.shape)\n",
    "            q, k, v = param.chunk(3, dim=0)\n",
    "            print('in_proj!')\n",
    "            af.array.save_array(name + 'q', toArrayFire(q), filepath, True)\n",
    "            af.array.save_array(name + 'k', toArrayFire(k), filepath, True)\n",
    "            af.array.save_array(name + 'v', toArrayFire(k), filepath, True)\n",
    "            continue\n",
    "        if len(param.size()) > 0:\n",
    "            af_array = toArrayFire(param)\n",
    "            if 'fc' in name and 'weight' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            af.array.save_array(name, af_array, filepath, True)\n",
    "            i = i + 1\n",
    "    print(i)\n",
    "    for name in module.state_dict():\n",
    "        if 'running' in name:\n",
    "            print(name)\n",
    "            af_array = toArrayFire(module.state_dict()[name])\n",
    "            af.array.save_array(name, af_array, filepath + 'running', True)\n",
    "            \n",
    "def create_parser():\n",
    "    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n",
    "    parser.add_argument('--lr', default=1e-4, type=float)\n",
    "    parser.add_argument('--lr_backbone', default=1e-5, type=float)\n",
    "    parser.add_argument('--batch_size', default=2, type=int)\n",
    "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
    "    parser.add_argument('--epochs', default=300, type=int)\n",
    "    parser.add_argument('--lr_drop', default=200, type=int)\n",
    "    parser.add_argument('--optimizer', default=\"adam\", type=str)\n",
    "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
    "                        help='gradient clipping max norm')\n",
    "    parser.add_argument('--eval_skip', default=1, type=int,\n",
    "                        help='do evaluation every \"eval_skip\" frames')\n",
    "    parser.add_argument('--schedule', default='step', type=str,\n",
    "                        choices=('step', 'multistep'))\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--frozen_weights', type=str, default=None,\n",
    "                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n",
    "    # * Backbone\n",
    "    parser.add_argument('--backbone', default='resnet50', type=str,\n",
    "                        help=\"Name of the convolutional backbone to use\")\n",
    "    parser.add_argument('--dilation', action='store_true',\n",
    "                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
    "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n",
    "                        help=\"Type of positional embedding to use on top of the image features\")\n",
    "\n",
    "    # * Transformer\n",
    "    parser.add_argument('--enc_layers', default=6, type=int,\n",
    "                        help=\"Number of encoding layers in the transformer\")\n",
    "    parser.add_argument('--dec_layers', default=6, type=int,\n",
    "                        help=\"Number of decoding layers in the transformer\")\n",
    "    parser.add_argument('--dim_feedforward', default=2048, type=int,\n",
    "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
    "    parser.add_argument('--hidden_dim', default=256, type=int,\n",
    "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
    "    parser.add_argument('--dropout', default=0.1, type=float,\n",
    "                        help=\"Dropout applied in the transformer\")\n",
    "    parser.add_argument('--nheads', default=8, type=int,\n",
    "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
    "    parser.add_argument('--num_queries', default=100, type=int,\n",
    "                        help=\"Number of query slots\")\n",
    "    parser.add_argument('--pre_norm', action='store_true')\n",
    "    parser.add_argument('--no_pass_pos_and_query', dest='pass_pos_and_query', action='store_false',\n",
    "                        help=\"Disables passing the positional encodings to each attention layers\")\n",
    "\n",
    "    # * Segmentation\n",
    "    parser.add_argument('--mask_model', default='none', type=str, choices=(\"none\", \"smallconv\", \"v2\"),\n",
    "                        help=\"Segmentation head to be used (if None, segmentation will not be trained)\")\n",
    "\n",
    "    # Loss\n",
    "    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',\n",
    "                        help=\"Disables auxiliary decoding losses (loss at each layer)\")\n",
    "    parser.add_argument('--set_loss', default='hungarian', type=str,\n",
    "                        choices=('sequential', 'hungarian', 'lexicographical'),\n",
    "                        help=\"Type of matching to perform in the loss\")\n",
    "    parser.add_argument('--bcl', dest='use_bcl', action='store_true',\n",
    "                        help=\"Use balanced classification loss\")\n",
    "    # * Matcher\n",
    "    parser.add_argument('--set_cost_class', default=1, type=float,\n",
    "                        help=\"Class coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_bbox', default=5, type=float,\n",
    "                        help=\"L1 box coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_giou', default=2, type=float,\n",
    "                        help=\"giou box coefficient in the matching cost\")\n",
    "    # * Loss coefficients\n",
    "    parser.add_argument('--mask_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--dice_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--bbox_loss_coef', default=5, type=float)\n",
    "    parser.add_argument('--giou_loss_coef', default=2, type=float)\n",
    "    parser.add_argument('--eos_coef', default=0.1, type=float,\n",
    "                        help=\"Relative classification weight of the no-object class\")\n",
    "\n",
    "    # dataset parameters\n",
    "    parser.add_argument('--dataset_file', default='coco')\n",
    "    parser.add_argument('--coco_path', type=str, default='/datasets01/COCO/022719')\n",
    "    parser.add_argument('--coco_panoptic_path', type=str, default='/datasets01/COCO/060419')\n",
    "    parser.add_argument('--remove_difficult', action='store_true')\n",
    "    parser.add_argument('--masks', action='store_true')\n",
    "\n",
    "    parser.add_argument('--output-dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=42, type=int)\n",
    "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "    parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--num_workers', default=2, type=int)\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world-size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--dist-url', default='env://', help='url used to set up distributed training')\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.backbone import *\n",
    "from models.position_encoding import *\n",
    "from models.matcher import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "embedding_size = 128\n",
    "src_len = 5\n",
    "tgt_len = 10\n",
    "num_layers = 2\n",
    "\n",
    "parser = create_parser()\n",
    "args = parser.parse_args('')\n",
    "\n",
    "\n",
    "queries = torch.rand(tgt_len, batch_size, embedding_size)\n",
    "memory = torch.rand(src_len, batch_size, embedding_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0q_0layers.0.self_attn.in_proj_weight 2 torch.Size([128, 128])\n",
      "2\n",
      "0q_1layers.0.self_attn.in_proj_bias 3 torch.Size([128])\n",
      "3\n",
      "1k_0layers.0.self_attn.in_proj_weight 4 torch.Size([128, 128])\n",
      "4\n",
      "1k_1layers.0.self_attn.in_proj_bias 5 torch.Size([128])\n",
      "5\n",
      "2v_0layers.0.self_attn.in_proj_weight 6 torch.Size([128, 128])\n",
      "6\n",
      "2v_1layers.0.self_attn.in_proj_bias 7 torch.Size([128])\n",
      "7\n",
      "layers.0.self_attn.out_proj.weight 8 torch.Size([128, 128])\n",
      "8\n",
      "layers.0.self_attn.out_proj.bias 9 torch.Size([128])\n",
      "9\n",
      "0q_0layers.0.multihead_attn.in_proj_weight 10 torch.Size([128, 128])\n",
      "10\n",
      "0q_1layers.0.multihead_attn.in_proj_bias 11 torch.Size([128])\n",
      "11\n",
      "1k_0layers.0.multihead_attn.in_proj_weight 12 torch.Size([128, 128])\n",
      "12\n",
      "1k_1layers.0.multihead_attn.in_proj_bias 13 torch.Size([128])\n",
      "13\n",
      "2v_0layers.0.multihead_attn.in_proj_weight 14 torch.Size([128, 128])\n",
      "14\n",
      "2v_1layers.0.multihead_attn.in_proj_bias 15 torch.Size([128])\n",
      "15\n",
      "layers.0.multihead_attn.out_proj.weight 16 torch.Size([128, 128])\n",
      "16\n",
      "layers.0.multihead_attn.out_proj.bias 17 torch.Size([128])\n",
      "17\n",
      "layers.0.linear1.weight 18 torch.Size([128, 128])\n",
      "18\n",
      "layers.0.linear1.bias 19 torch.Size([128])\n",
      "19\n",
      "layers.0.linear2.weight 20 torch.Size([128, 128])\n",
      "20\n",
      "layers.0.linear2.bias 21 torch.Size([128])\n",
      "21\n",
      "layers.0.norm1.weight 22 torch.Size([128])\n",
      "22\n",
      "layers.0.norm1.bias 23 torch.Size([128])\n",
      "23\n",
      "layers.0.norm2.weight 24 torch.Size([128])\n",
      "24\n",
      "layers.0.norm2.bias 25 torch.Size([128])\n",
      "25\n",
      "layers.0.norm3.weight 26 torch.Size([128])\n",
      "26\n",
      "layers.0.norm3.bias 27 torch.Size([128])\n",
      "27\n",
      "0q_0layers.1.self_attn.in_proj_weight 28 torch.Size([128, 128])\n",
      "28\n",
      "0q_1layers.1.self_attn.in_proj_bias 29 torch.Size([128])\n",
      "29\n",
      "1k_0layers.1.self_attn.in_proj_weight 30 torch.Size([128, 128])\n",
      "30\n",
      "1k_1layers.1.self_attn.in_proj_bias 31 torch.Size([128])\n",
      "31\n",
      "2v_0layers.1.self_attn.in_proj_weight 32 torch.Size([128, 128])\n",
      "32\n",
      "2v_1layers.1.self_attn.in_proj_bias 33 torch.Size([128])\n",
      "33\n",
      "layers.1.self_attn.out_proj.weight 34 torch.Size([128, 128])\n",
      "34\n",
      "layers.1.self_attn.out_proj.bias 35 torch.Size([128])\n",
      "35\n",
      "0q_0layers.1.multihead_attn.in_proj_weight 36 torch.Size([128, 128])\n",
      "36\n",
      "0q_1layers.1.multihead_attn.in_proj_bias 37 torch.Size([128])\n",
      "37\n",
      "1k_0layers.1.multihead_attn.in_proj_weight 38 torch.Size([128, 128])\n",
      "38\n",
      "1k_1layers.1.multihead_attn.in_proj_bias 39 torch.Size([128])\n",
      "39\n",
      "2v_0layers.1.multihead_attn.in_proj_weight 40 torch.Size([128, 128])\n",
      "40\n",
      "2v_1layers.1.multihead_attn.in_proj_bias 41 torch.Size([128])\n",
      "41\n",
      "layers.1.multihead_attn.out_proj.weight 42 torch.Size([128, 128])\n",
      "42\n",
      "layers.1.multihead_attn.out_proj.bias 43 torch.Size([128])\n",
      "43\n",
      "layers.1.linear1.weight 44 torch.Size([128, 128])\n",
      "44\n",
      "layers.1.linear1.bias 45 torch.Size([128])\n",
      "45\n",
      "layers.1.linear2.weight 46 torch.Size([128, 128])\n",
      "46\n",
      "layers.1.linear2.bias 47 torch.Size([128])\n",
      "47\n",
      "layers.1.norm1.weight 48 torch.Size([128])\n",
      "48\n",
      "layers.1.norm1.bias 49 torch.Size([128])\n",
      "49\n",
      "layers.1.norm2.weight 50 torch.Size([128])\n",
      "50\n",
      "layers.1.norm2.bias 51 torch.Size([128])\n",
      "51\n",
      "layers.1.norm3.weight 52 torch.Size([128])\n",
      "52\n",
      "layers.1.norm3.bias 53 torch.Size([128])\n",
      "53\n",
      "norm.weight 54 torch.Size([128])\n",
      "54\n",
      "norm.bias 55 torch.Size([128])\n",
      "55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = '/private/home/padentomasello/scratch/pytorch_testing/transformer_decoder.array'\n",
    "af.array.save_array('queries', toArrayFire(queries), filepath, False)\n",
    "af.array.save_array('memory', toArrayFire(memory), filepath, True)\n",
    "i = 2\n",
    "params = {}\n",
    "for (name, param) in model.named_parameters():\n",
    "        if 'in_proj' in name:\n",
    "            q, k, v = param.chunk(3, dim=0)\n",
    "            hack = '0'\n",
    "            if 'in_proj_bias' in name: hack = '1'\n",
    "            params['0q_' + hack + name] = q\n",
    "            params['1k_' + hack + name] = k\n",
    "            params['2v_' + hack + name] = v\n",
    "            if 'in_proj_bias' in name:\n",
    "                for key in sorted(params.keys()):\n",
    "                    af_array = toArrayFire(params[key])\n",
    "                    if 'weight' in key:\n",
    "                        af_array = af.array.transpose(af_array)\n",
    "                    print(key, i, params[key].shape)\n",
    "                    print(af.array.save_array(key, af_array, filepath, True))\n",
    "                    i = i + 1\n",
    "                params = {}\n",
    "            continue\n",
    "        elif len(param.size()) > 0:\n",
    "            af_array = toArrayFire(param)\n",
    "            if 'fc' in name and 'weight' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            if 'weight' in name and 'proj' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            if 'weight' in name and 'linear' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            print(name, i, param.shape)\n",
    "            print(af.array.save_array(name, af_array, filepath, True))\n",
    "            i = i + 1\n",
    "#af.array.save_array('output', toArrayFire(output), filepath, True)\n",
    "af.array.save_array('output', toArrayFire(output), filepath, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 1, 128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
