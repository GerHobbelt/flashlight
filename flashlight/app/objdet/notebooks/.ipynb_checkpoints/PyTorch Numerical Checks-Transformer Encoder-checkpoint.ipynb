{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arrayfire as af\n",
    "import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toArrayFire(x):\n",
    "    x_np = x.detach().contiguous().numpy()\n",
    "    shape = 1\n",
    "    if len(x_np.shape) == 0:\n",
    "        shape = (1,)\n",
    "    else:\n",
    "        shape = x_np.shape[::-1]\n",
    "    print(shape)\n",
    "    afArray = af.Array(x_np.ctypes.data, shape, x_np.dtype.char)\n",
    "    return afArray\n",
    "\n",
    "def saveStateDict(module, filepath):\n",
    "    i = 0\n",
    "    for (name, param) in module.named_parameters():\n",
    "        #param = module.state_dict()[name]\n",
    "        print(name, \"\\t\", param.size())\n",
    "        if 'in_proj' in name:\n",
    "            print(param.shape)\n",
    "            q, k, v = param.chunk(3, dim=0)\n",
    "            print('in_proj!')\n",
    "            af.array.save_array(name + 'q', toArrayFire(q), filepath, True)\n",
    "            af.array.save_array(name + 'k', toArrayFire(k), filepath, True)\n",
    "            af.array.save_array(name + 'v', toArrayFire(k), filepath, True)\n",
    "            continue\n",
    "        if len(param.size()) > 0:\n",
    "            af_array = toArrayFire(param)\n",
    "            if 'fc' in name and 'weight' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            af.array.save_array(name, af_array, filepath, True)\n",
    "            i = i + 1\n",
    "    print(i)\n",
    "    for name in module.state_dict():\n",
    "        if 'running' in name:\n",
    "            print(name)\n",
    "            af_array = toArrayFire(module.state_dict()[name])\n",
    "            af.array.save_array(name, af_array, filepath + 'running', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.backbone import *\n",
    "from models.position_encoding import *\n",
    "from models.matcher import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after self attention tensor([[[ 0.2310, -0.4288, -0.2431,  0.0449, -0.4100,  0.3400,  0.1781,\n",
      "          -0.1560, -0.1527, -0.3016,  0.5055, -0.0278]],\n",
      "\n",
      "        [[ 0.2334, -0.4242, -0.2405,  0.0427, -0.4118,  0.3394,  0.1841,\n",
      "          -0.1544, -0.1586, -0.2983,  0.5045, -0.0276]],\n",
      "\n",
      "        [[ 0.2335, -0.4236, -0.2393,  0.0429, -0.4118,  0.3396,  0.1844,\n",
      "          -0.1557, -0.1579, -0.2985,  0.5043, -0.0287]],\n",
      "\n",
      "        [[ 0.2320, -0.4283, -0.2396,  0.0444, -0.4097,  0.3399,  0.1782,\n",
      "          -0.1590, -0.1501, -0.3019,  0.5041, -0.0304]],\n",
      "\n",
      "        [[ 0.2318, -0.4274, -0.2427,  0.0440, -0.4108,  0.3397,  0.1805,\n",
      "          -0.1547, -0.1557, -0.3002,  0.5055, -0.0271]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "after dropout tensor([[[ 0.9534,  0.1263, -0.1960,  0.8470, -0.3142,  1.2343,  0.2509,\n",
      "           0.1605,  0.7483,  0.0972,  0.7158,  0.4215]],\n",
      "\n",
      "        [[ 1.1922,  0.3208, -0.1261,  0.3631,  0.2563,  0.5813,  1.1257,\n",
      "           0.2529,  0.2898, -0.1682,  1.1010, -0.0240]],\n",
      "\n",
      "        [[ 0.9180,  0.4651, -0.0045,  0.0935,  0.2328,  0.7436,  1.0150,\n",
      "           0.2493, -0.0836,  0.3926,  0.8953,  0.3848]],\n",
      "\n",
      "        [[ 0.7368,  0.3167,  0.6290,  0.3862, -0.3320,  0.7419,  1.1416,\n",
      "           0.2099,  0.3106,  0.3703,  0.6148,  0.2852]],\n",
      "\n",
      "        [[ 0.7657,  0.2843,  0.6433,  0.3415, -0.3179,  1.0657,  0.9010,\n",
      "           0.7518,  0.5333,  0.3822,  0.9902,  0.8974]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "after norm tensor([[[ 1.1607e+00, -6.4058e-01, -1.3425e+00,  9.2895e-01, -1.5998e+00,\n",
      "           1.7724e+00, -3.6908e-01, -5.6614e-01,  7.1413e-01, -7.0384e-01,\n",
      "           6.4330e-01,  2.4420e-03]],\n",
      "\n",
      "        [[ 1.6661e+00, -2.3970e-01, -1.2171e+00, -1.4720e-01, -3.8080e-01,\n",
      "           3.2993e-01,  1.5207e+00, -3.8810e-01, -3.0754e-01, -1.3091e+00,\n",
      "           1.4665e+00, -9.9370e-01]],\n",
      "\n",
      "        [[ 1.3312e+00,  6.5078e-02, -1.2476e+00, -9.7364e-01, -5.8439e-01,\n",
      "           8.4346e-01,  1.6023e+00, -5.3820e-01, -1.4687e+00, -1.3764e-01,\n",
      "           1.2676e+00, -1.5950e-01]],\n",
      "\n",
      "        [[ 8.2274e-01, -3.8620e-01,  5.1251e-01, -1.8618e-01, -2.2531e+00,\n",
      "           8.3738e-01,  1.9875e+00, -6.9367e-01, -4.0385e-01, -2.3202e-01,\n",
      "           4.7170e-01, -4.7679e-01]],\n",
      "\n",
      "        [[ 4.3676e-01, -8.5706e-01,  1.0761e-01, -7.0320e-01, -2.4755e+00,\n",
      "           1.2429e+00,  8.0036e-01,  3.9920e-01, -1.8780e-01, -5.9408e-01,\n",
      "           1.0401e+00,  7.9068e-01]]], grad_fn=<NativeLayerNormBackward>)\n",
      "after mlp tensor([[[-0.0731,  0.0790, -0.2862, -0.0538,  0.1293, -0.0382, -0.2416,\n",
      "           0.6488, -0.1611,  0.2736, -0.2914, -0.1779]],\n",
      "\n",
      "        [[ 0.0400, -0.1358, -0.2766, -0.0699,  0.4981, -0.2105, -0.1920,\n",
      "           0.4454, -0.2567,  0.3999, -0.1968, -0.0652]],\n",
      "\n",
      "        [[ 0.2184,  0.0161, -0.1310, -0.0735,  0.4972, -0.0447, -0.2053,\n",
      "           0.3695, -0.0687,  0.3956, -0.2882, -0.3784]],\n",
      "\n",
      "        [[-0.0856,  0.1433, -0.1672, -0.1709,  0.3388, -0.1103,  0.0531,\n",
      "           0.3546,  0.1326,  0.4370, -0.4943, -0.0286]],\n",
      "\n",
      "        [[-0.1137,  0.0370, -0.0390, -0.3677,  0.2446, -0.1056,  0.0492,\n",
      "           0.4450,  0.3062,  0.1986, -0.3227, -0.1120]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "after second norm tensor([[[ 1.1470, -0.5670, -1.6761,  0.9262, -1.5116,  1.8191, -0.6180,\n",
      "           0.1026,  0.5914, -0.4305,  0.3824, -0.1656]],\n",
      "\n",
      "        [[ 1.7833, -0.3904, -1.5579, -0.2250,  0.1242,  0.1265,  1.3892,\n",
      "           0.0616, -0.5874, -0.9477,  1.3276, -1.1040]],\n",
      "\n",
      "        [[ 1.5433,  0.0563, -1.4220, -1.0863, -0.1142,  0.7830,  1.3888,\n",
      "          -0.1968, -1.5828,  0.2354,  0.9660, -0.5706]],\n",
      "\n",
      "        [[ 0.7835, -0.3078,  0.3471, -0.4350, -2.1688,  0.7722,  2.2347,\n",
      "          -0.4148, -0.3394,  0.1909, -0.0625, -0.6001]],\n",
      "\n",
      "        [[ 0.3216, -0.8848,  0.0531, -1.1495, -2.3736,  1.1808,  0.8772,\n",
      "           0.8716,  0.1056, -0.4367,  0.7377,  0.6969]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "embedding_size = 12\n",
    "src_len = 5\n",
    "src = torch.rand(src_len, batch_size, embedding_size)\n",
    "model = TransformerEncoderLayer(embedding_size, 1, dropout=0.0, dim_feedforward=128)\n",
    "output = model.forward(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 1, 5)\n",
      "(12, 12)\n",
      "self_attn.in_proj_bias\n",
      "here!\n",
      "0q_0self_attn.in_proj_weight 1\n",
      "(12,)\n",
      "self_attn.in_proj_bias\n",
      "0q_1self_attn.in_proj_bias 2\n",
      "(12, 12)\n",
      "self_attn.in_proj_bias\n",
      "here!\n",
      "1k_0self_attn.in_proj_weight 3\n",
      "(12,)\n",
      "self_attn.in_proj_bias\n",
      "1k_1self_attn.in_proj_bias 4\n",
      "(12, 12)\n",
      "self_attn.in_proj_bias\n",
      "here!\n",
      "2v_0self_attn.in_proj_weight 5\n",
      "(12,)\n",
      "self_attn.in_proj_bias\n",
      "2v_1self_attn.in_proj_bias 6\n",
      "(12, 12)\n",
      "self_attn.out_proj.weight 7\n",
      "(12,)\n",
      "self_attn.out_proj.bias 8\n",
      "(12, 128)\n",
      "linear1.weight 9\n",
      "(128,)\n",
      "linear1.bias 10\n",
      "(128, 12)\n",
      "linear2.weight 11\n",
      "(12,)\n",
      "linear2.bias 12\n",
      "(12,)\n",
      "norm1.weight 13\n",
      "(12,)\n",
      "norm1.bias 14\n",
      "(12,)\n",
      "norm2.weight 15\n",
      "(12,)\n",
      "norm2.bias 16\n",
      "(12, 1, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = '/private/home/padentomasello/scratch/pytorch_testing/transformer_encoder_layer.array'\n",
    "af.array.save_array('input', toArrayFire(src), filepath, False)\n",
    "i = 1\n",
    "params = {}\n",
    "for (name, param) in model.named_parameters():\n",
    "        if 'in_proj' in name:\n",
    "            q, k, v = param.chunk(3, dim=0)\n",
    "            hack = '0'\n",
    "            if 'in_proj_bias' in name: hack = '1'\n",
    "            params['0q_' + hack + name] = q\n",
    "            params['1k_' + hack + name] = k\n",
    "            params['2v_' + hack + name] = v\n",
    "            if 'in_proj_bias' in name:\n",
    "                for key in sorted(params.keys()):\n",
    "                    af_array = toArrayFire(params[key])\n",
    "                    print(name)\n",
    "                    if 'weight' in key:\n",
    "                        print('here!')\n",
    "                        af_array = af.array.transpose(af_array)\n",
    "                    print(key, i)\n",
    "                    af.array.save_array(key, af_array, filepath, True)  \n",
    "                    i = i + 1\n",
    "            continue\n",
    "        elif len(param.size()) > 0:\n",
    "            af_array = toArrayFire(param)\n",
    "            if 'fc' in name and 'weight' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            if 'weight' in name and 'proj' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            if 'weight' in name and 'linear' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            print(name, i)\n",
    "            af.array.save_array(name, af_array, filepath, True)\n",
    "            i = i + 1\n",
    "#af.array.save_array('output', toArrayFire(output), filepath, True)\n",
    "af.array.save_array('output', toArrayFire(output), filepath, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8188, -0.8131, -1.1027, -1.2139,  1.3754,  0.2462, -1.2540,\n",
       "           1.6625, -0.7953, -0.3104,  0.7027,  0.6837]],\n",
       "\n",
       "        [[ 0.5006, -0.3333,  0.3904, -0.2153, -1.5290,  1.3152, -0.4167,\n",
       "           1.3011, -1.0204, -1.7167,  0.7378,  0.9862]],\n",
       "\n",
       "        [[ 0.6038, -0.6803, -0.2174, -0.1719,  1.1383, -0.7165, -1.2133,\n",
       "           2.1317,  0.8927, -0.0180, -0.1876, -1.5614]],\n",
       "\n",
       "        [[ 0.7928, -0.7490,  1.2031, -0.8406, -1.0975,  0.6829,  0.4576,\n",
       "          -0.2225, -0.0312, -1.4745,  2.0058, -0.7268]],\n",
       "\n",
       "        [[ 0.3869, -0.2117, -1.1351, -1.3118,  0.9864,  1.0178, -1.0721,\n",
       "           1.3694, -1.2195,  1.2316, -0.6364,  0.5946]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.modules.linear import _LinearWithBias\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.init import constant_\n",
    "from torch.nn.init import xavier_normal_\n",
    "from torch.nn.parameter import Parameter\n",
    "#from .module import Module\n",
    "#from .. import functional as F\n",
    "\n",
    "class MultiheadAttention2(nn.Module):\n",
    "    r\"\"\"Allows the model to jointly attend to information\n",
    "    from different representation subspaces.\n",
    "    See reference: Attention Is All You Need\n",
    "\n",
    "    .. math::\n",
    "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
    "        \\text{where} head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "\n",
    "    Args:\n",
    "        embed_dim: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n",
    "        bias: add bias as module parameter. Default: True.\n",
    "        add_bias_kv: add bias to the key and value sequences at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        kdim: total number of features in key. Default: None.\n",
    "        vdim: total number of features in value. Default: None.\n",
    "\n",
    "        Note: if kdim and vdim are None, they will be set to embed_dim such that\n",
    "        query, key, and value have the same number of features.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "    \"\"\"\n",
    "    bias_k: Optional[torch.Tensor]\n",
    "    bias_v: Optional[torch.Tensor]\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None):\n",
    "        super(MultiheadAttention2, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        if self._qkv_same_embed_dim is False:\n",
    "            self.q_proj_weight = Parameter(torch.Tensor(embed_dim, embed_dim))\n",
    "            self.k_proj_weight = Parameter(torch.Tensor(embed_dim, self.kdim))\n",
    "            self.v_proj_weight = Parameter(torch.Tensor(embed_dim, self.vdim))\n",
    "            self.register_parameter('in_proj_weight', None)\n",
    "        else:\n",
    "            self.in_proj_weight = Parameter(torch.empty(3 * embed_dim, embed_dim))\n",
    "            self.register_parameter('q_proj_weight', None)\n",
    "            self.register_parameter('k_proj_weight', None)\n",
    "            self.register_parameter('v_proj_weight', None)\n",
    "\n",
    "        if bias:\n",
    "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim))\n",
    "        else:\n",
    "            self.register_parameter('in_proj_bias', None)\n",
    "        self.out_proj = _LinearWithBias(embed_dim, embed_dim)\n",
    "\n",
    "        if add_bias_kv:\n",
    "            self.bias_k = Parameter(torch.empty(1, 1, embed_dim))\n",
    "            self.bias_v = Parameter(torch.empty(1, 1, embed_dim))\n",
    "        else:\n",
    "            self.bias_k = self.bias_v = None\n",
    "\n",
    "        self.add_zero_attn = add_zero_attn\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        if self._qkv_same_embed_dim:\n",
    "            xavier_uniform_(self.in_proj_weight)\n",
    "        else:\n",
    "            xavier_uniform_(self.q_proj_weight)\n",
    "            xavier_uniform_(self.k_proj_weight)\n",
    "            xavier_uniform_(self.v_proj_weight)\n",
    "\n",
    "        if self.in_proj_bias is not None:\n",
    "            constant_(self.in_proj_bias, 0.)\n",
    "            constant_(self.out_proj.bias, 0.)\n",
    "        if self.bias_k is not None:\n",
    "            xavier_normal_(self.bias_k)\n",
    "        if self.bias_v is not None:\n",
    "            xavier_normal_(self.bias_v)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
    "        if '_qkv_same_embed_dim' not in state:\n",
    "            state['_qkv_same_embed_dim'] = True\n",
    "\n",
    "        super(MultiheadAttention, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None,\n",
    "                need_weights=True, attn_mask=None):\n",
    "        # type: (Tensor, Tensor, Tensor, Optional[Tensor], bool, Optional[Tensor]) -> Tuple[Tensor, Optional[Tensor]]\n",
    "        r\"\"\"\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. When given a binary mask and a value is True,\n",
    "            the corresponding value on the attention layer will be ignored. When given\n",
    "            a byte mask and a value is non-zero, the corresponding value on the attention\n",
    "            layer will be ignored\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
    "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
    "\n",
    "    Shape:\n",
    "        - Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
    "          If a ByteTensor is provided, the non-zero positions will be ignored while the position\n",
    "          with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n",
    "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked\n",
    "          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
    "          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
    "          is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "          is provided, it will be added to the attention weight.\n",
    "\n",
    "        - Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension.\n",
    "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
    "          L is the target sequence length, S is the source sequence length.\n",
    "        \"\"\"\n",
    "        if not self._qkv_same_embed_dim:\n",
    "            return multi_head_attention_forward2(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask, use_separate_proj_weight=True,\n",
    "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
    "                v_proj_weight=self.v_proj_weight)\n",
    "        else:\n",
    "            return multi_head_attention_forward2(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(input, p=0.5, training=True, inplace=False):\n",
    "    # type: (Tensor, float, bool, bool) -> Tensor\n",
    "    r\"\"\"\n",
    "    During training, randomly zeroes some of the elements of the input\n",
    "    tensor with probability :attr:`p` using samples from a Bernoulli\n",
    "    distribution.\n",
    "    See :class:`~torch.nn.Dropout` for details.\n",
    "    Args:\n",
    "        p: probability of an element to be zeroed. Default: 0.5\n",
    "        training: apply dropout if is ``True``. Default: ``True``\n",
    "        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n",
    "    \"\"\"\n",
    "    assert(p == 0.0)\n",
    "    return input\n",
    "    if p < 0. or p > 1.:\n",
    "        raise ValueError(\"dropout probability has to be between 0 and 1, \"\n",
    "                         \"but got {}\".format(p))\n",
    "    return (_VF.dropout_(input, p, training)\n",
    "            if inplace\n",
    "            else _VF.dropout(input, p, training))\n",
    "\n",
    "\n",
    "def softmax(input, dim=None, _stacklevel=3, dtype=None):\n",
    "    # type: (Tensor, Optional[int], int, Optional[int]) -> Tensor\n",
    "    r\"\"\"Applies a softmax function.\n",
    "    Softmax is defined as:\n",
    "    :math:`\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}`\n",
    "    It is applied to all slices along dim, and will re-scale them so that the elements\n",
    "    lie in the range `[0, 1]` and sum to 1.\n",
    "    See :class:`~torch.nn.Softmax` for more details.\n",
    "    Arguments:\n",
    "        input (Tensor): input\n",
    "        dim (int): A dimension along which softmax will be computed.\n",
    "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
    "          If specified, the input tensor is casted to :attr:`dtype` before the operation\n",
    "          is performed. This is useful for preventing data type overflows. Default: None.\n",
    "    .. note::\n",
    "        This function doesn't work directly with NLLLoss,\n",
    "        which expects the Log to be computed between the Softmax and itself.\n",
    "        Use log_softmax instead (it's faster and has better numerical properties).\n",
    "    \"\"\"\n",
    "    if dim is None:\n",
    "        dim = _get_softmax_dim('softmax', input.dim(), _stacklevel)\n",
    "    if dtype is None:\n",
    "        ret = input.softmax(dim)\n",
    "    else:\n",
    "        ret = input.softmax(dim, dtype=dtype)\n",
    "    return ret\n",
    "\n",
    "def linear(input, weight, bias=None):\n",
    "    # type: (Tensor, Tensor, Optional[Tensor]) -> Tensor\n",
    "    r\"\"\"\n",
    "    Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.\n",
    "    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *, in\\_features)` N is the batch size, `*` means any number of\n",
    "          additional dimensions\n",
    "        - Weight: :math:`(out\\_features, in\\_features)`\n",
    "        - Bias: :math:`(out\\_features)`\n",
    "        - Output: :math:`(N, *, out\\_features)`\n",
    "    \"\"\"\n",
    "    tens_ops = (input, weight)\n",
    "    if input.dim() == 2 and bias is not None:\n",
    "        # fused op is marginally faster\n",
    "        ret = torch.addmm(bias, input, weight.t())\n",
    "    else:\n",
    "        output = input.matmul(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias\n",
    "        ret = output\n",
    "    return ret\n",
    "\n",
    "test = 0\n",
    "def multi_head_attention_forward2(query: Tensor,\n",
    "                                 key: Tensor,\n",
    "                                 value: Tensor,\n",
    "                                 embed_dim_to_check: int,\n",
    "                                 num_heads: int,\n",
    "                                 in_proj_weight: Tensor,\n",
    "                                 in_proj_bias: Tensor,\n",
    "                                 bias_k: Optional[Tensor],\n",
    "                                 bias_v: Optional[Tensor],\n",
    "                                 add_zero_attn: bool,\n",
    "                                 dropout_p: float,\n",
    "                                 out_proj_weight: Tensor,\n",
    "                                 out_proj_bias: Tensor,\n",
    "                                 training: bool = True,\n",
    "                                 key_padding_mask: Optional[Tensor] = None,\n",
    "                                 need_weights: bool = True,\n",
    "                                 attn_mask: Optional[Tensor] = None,\n",
    "                                 use_separate_proj_weight: bool = False,\n",
    "                                 q_proj_weight: Optional[Tensor] = None,\n",
    "                                 k_proj_weight: Optional[Tensor] = None,\n",
    "                                 v_proj_weight: Optional[Tensor] = None,\n",
    "                                 static_k: Optional[Tensor] = None,\n",
    "                                 static_v: Optional[Tensor] = None\n",
    "                                 ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        embed_dim_to_check: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        in_proj_weight, in_proj_bias: input projection weight and bias.\n",
    "        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        dropout_p: probability of an element to be zeroed.\n",
    "        out_proj_weight, out_proj_bias: the output projection weight and bias.\n",
    "        training: apply dropout if is ``True``.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. This is an binary mask. When the value is True,\n",
    "            the corresponding value on the attention layer will be filled with -inf.\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
    "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
    "        use_separate_proj_weight: the function accept the proj. weights for query, key,\n",
    "            and value in different forms. If false, in_proj_weight will be used, which is\n",
    "            a combination of q_proj_weight, k_proj_weight, v_proj_weight.\n",
    "        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.\n",
    "        static_k, static_v: static key and value used for attention operators.\n",
    "    Shape:\n",
    "        Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
    "          If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions\n",
    "          will be unchanged. If a BoolTensor is provided, the positions with the\n",
    "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked\n",
    "          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
    "          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
    "          are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "          is provided, it will be added to the attention weight.\n",
    "        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
    "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
    "        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
    "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
    "        Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension.\n",
    "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
    "          L is the target sequence length, S is the source sequence length.\n",
    "    \"\"\"\n",
    "    global test\n",
    "    tgt_len, bsz, embed_dim = query.size()\n",
    "    assert embed_dim == embed_dim_to_check\n",
    "    # allow MHA to have different sizes for the feature dimension\n",
    "    assert key.size(0) == value.size(0) and key.size(1) == value.size(1)\n",
    "\n",
    "    head_dim = embed_dim // num_heads\n",
    "    assert head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "    scaling = float(head_dim) ** -0.5\n",
    "\n",
    "    if not use_separate_proj_weight:\n",
    "        if (query is key or torch.equal(query, key)) and (key is value or torch.equal(key, value)):\n",
    "            # self-attention\n",
    "            print('in_proj_weight', in_proj_weight)\n",
    "            print('query', query)\n",
    "            print(in_proj_bias)\n",
    "            q, k, v = linear(query, in_proj_weight, in_proj_bias).chunk(3, dim=-1)\n",
    "            print('q', q)\n",
    "            print('here2')\n",
    "            test = (in_proj_weight, in_proj_bias)\n",
    "            print('here!')\n",
    "\n",
    "        elif (key is value or torch.equal(key, value)):\n",
    "            # encoder-decoder attention\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = 0\n",
    "            _end = embed_dim\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            q = linear(query, _w, _b)\n",
    "\n",
    "            if key is None:\n",
    "                assert value is None\n",
    "                k = None\n",
    "                v = None\n",
    "            else:\n",
    "\n",
    "                # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "                _b = in_proj_bias\n",
    "                _start = embed_dim\n",
    "                _end = None\n",
    "                _w = in_proj_weight[_start:, :]\n",
    "                if _b is not None:\n",
    "                    _b = _b[_start:]\n",
    "                k, v = linear(key, _w, _b).chunk(2, dim=-1)\n",
    "\n",
    "        else:\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = 0\n",
    "            _end = embed_dim\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            q = linear(query, _w, _b)\n",
    "\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = embed_dim\n",
    "            _end = embed_dim * 2\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            k = linear(key, _w, _b)\n",
    "\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = embed_dim * 2\n",
    "            _end = None\n",
    "            _w = in_proj_weight[_start:, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:]\n",
    "            v = linear(value, _w, _b)\n",
    "    else:\n",
    "        q_proj_weight_non_opt = torch.jit._unwrap_optional(q_proj_weight)\n",
    "        len1, len2 = q_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == query.size(-1)\n",
    "\n",
    "        k_proj_weight_non_opt = torch.jit._unwrap_optional(k_proj_weight)\n",
    "        len1, len2 = k_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == key.size(-1)\n",
    "\n",
    "        v_proj_weight_non_opt = torch.jit._unwrap_optional(v_proj_weight)\n",
    "        len1, len2 = v_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == value.size(-1)\n",
    "\n",
    "        if in_proj_bias is not None:\n",
    "            q = linear(query, q_proj_weight_non_opt, in_proj_bias[0:embed_dim])\n",
    "            k = linear(key, k_proj_weight_non_opt, in_proj_bias[embed_dim:(embed_dim * 2)])\n",
    "            v = linear(value, v_proj_weight_non_opt, in_proj_bias[(embed_dim * 2):])\n",
    "        else:\n",
    "            q = linear(query, q_proj_weight_non_opt, in_proj_bias)\n",
    "            k = linear(key, k_proj_weight_non_opt, in_proj_bias)\n",
    "            v = linear(value, v_proj_weight_non_opt, in_proj_bias)\n",
    "    q = q * scaling\n",
    "    \n",
    "    print('q', q)\n",
    "    print('k', k)\n",
    "    print('v', v)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        assert attn_mask.dtype == torch.float32 or attn_mask.dtype == torch.float64 or \\\n",
    "            attn_mask.dtype == torch.float16 or attn_mask.dtype == torch.uint8 or attn_mask.dtype == torch.bool, \\\n",
    "            'Only float, byte, and bool types are supported for attn_mask, not {}'.format(attn_mask.dtype)\n",
    "        if attn_mask.dtype == torch.uint8:\n",
    "            warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "            attn_mask = attn_mask.to(torch.bool)\n",
    "\n",
    "        if attn_mask.dim() == 2:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "            if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:\n",
    "                raise RuntimeError('The size of the 2D attn_mask is not correct.')\n",
    "        elif attn_mask.dim() == 3:\n",
    "            if list(attn_mask.size()) != [bsz * num_heads, query.size(0), key.size(0)]:\n",
    "                raise RuntimeError('The size of the 3D attn_mask is not correct.')\n",
    "        else:\n",
    "            raise RuntimeError(\"attn_mask's dimension {} is not supported\".format(attn_mask.dim()))\n",
    "        # attn_mask's dim is 3 now.\n",
    "\n",
    "    # convert ByteTensor key_padding_mask to bool\n",
    "    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n",
    "        warnings.warn(\"Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "        key_padding_mask = key_padding_mask.to(torch.bool)\n",
    "\n",
    "    if bias_k is not None and bias_v is not None:\n",
    "        if static_k is None and static_v is None:\n",
    "            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
    "            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = pad(attn_mask, (0, 1))\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "        else:\n",
    "            assert static_k is None, \"bias cannot be added to static key.\"\n",
    "            assert static_v is None, \"bias cannot be added to static value.\"\n",
    "    else:\n",
    "        assert bias_k is None\n",
    "        assert bias_v is None\n",
    "\n",
    "    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if k is not None:\n",
    "        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if v is not None:\n",
    "        v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    if static_k is not None:\n",
    "        assert static_k.size(0) == bsz * num_heads\n",
    "        assert static_k.size(2) == head_dim\n",
    "        k = static_k\n",
    "\n",
    "    if static_v is not None:\n",
    "        assert static_v.size(0) == bsz * num_heads\n",
    "        assert static_v.size(2) == head_dim\n",
    "        v = static_v\n",
    "\n",
    "    src_len = k.size(1)\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        assert key_padding_mask.size(0) == bsz\n",
    "        assert key_padding_mask.size(1) == src_len\n",
    "\n",
    "    if add_zero_attn:\n",
    "        src_len += 1\n",
    "        k = torch.cat([k, torch.zeros((k.size(0), 1) + k.size()[2:], dtype=k.dtype, device=k.device)], dim=1)\n",
    "        v = torch.cat([v, torch.zeros((v.size(0), 1) + v.size()[2:], dtype=v.dtype, device=v.device)], dim=1)\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = pad(attn_mask, (0, 1))\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "\n",
    "    attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "    print('presoftmax', attn_output_weights)\n",
    "    assert list(attn_output_weights.size()) == [bsz * num_heads, tgt_len, src_len]\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_output_weights.masked_fill_(attn_mask, float('-inf'))\n",
    "        else:\n",
    "            attn_output_weights += attn_mask\n",
    "\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        attn_output_weights = attn_output_weights.masked_fill(\n",
    "            key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "            float('-inf'),\n",
    "        )\n",
    "        attn_output_weights = attn_output_weights.view(bsz * num_heads, tgt_len, src_len)\n",
    "\n",
    "    attn_output_weights = softmax(\n",
    "        attn_output_weights, dim=-1)\n",
    "    print('post softmax', attn_output_weights)\n",
    "    attn_output_weights = dropout(attn_output_weights, p=dropout_p, training=training)\n",
    "\n",
    "    attn_output = torch.bmm(attn_output_weights, v)\n",
    "    print(attn_output)\n",
    "    assert list(attn_output.size()) == [bsz * num_heads, tgt_len, head_dim]\n",
    "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "    attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
    "    print('postlinear', attn_output)\n",
    "\n",
    "    if need_weights:\n",
    "        # average attention weights over heads\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        return attn_output, attn_output_weights.sum(dim=1) / num_heads\n",
    "    else:\n",
    "        return attn_output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "embedding_size = 128\n",
    "src_len = 5\n",
    "model = MultiheadAttention2(embedding_size, 1, 0.0)\n",
    "#saveStateDict(model, filepath)\n",
    "src = torch.rand(src_len, batch_size, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_proj_weight Parameter containing:\n",
      "tensor([[-0.0304,  0.0714, -0.0197,  ..., -0.0713,  0.0892, -0.0649],\n",
      "        [ 0.0807, -0.0504, -0.0537,  ..., -0.0990,  0.0702, -0.1041],\n",
      "        [-0.0684,  0.0077, -0.0212,  ..., -0.0112,  0.0275, -0.0939],\n",
      "        ...,\n",
      "        [-0.0832, -0.0930,  0.0146,  ..., -0.0329,  0.0885,  0.0669],\n",
      "        [ 0.0751, -0.0531, -0.0746,  ..., -0.0936,  0.0032,  0.0827],\n",
      "        [ 0.0973,  0.0723, -0.0582,  ..., -0.0217, -0.1075, -0.0729]],\n",
      "       requires_grad=True)\n",
      "query tensor([[[6.5580e-01, 6.7645e-01, 4.2053e-01, 8.9145e-01, 2.4312e-01,\n",
      "          1.0258e-01, 7.4126e-01, 5.9265e-01, 7.3318e-01, 9.4323e-01,\n",
      "          6.2272e-01, 9.7688e-01, 2.7367e-01, 6.0029e-01, 4.0472e-01,\n",
      "          5.0296e-01, 2.3671e-01, 9.1568e-01, 3.0918e-01, 2.8747e-01,\n",
      "          2.6097e-01, 1.7049e-01, 6.1015e-03, 1.9075e-02, 6.7696e-02,\n",
      "          7.0685e-01, 5.5409e-01, 9.0990e-01, 6.4519e-01, 6.6409e-01,\n",
      "          9.5939e-01, 6.8551e-01, 9.7827e-01, 6.7857e-01, 6.5455e-01,\n",
      "          1.7457e-01, 1.0372e-02, 9.5209e-01, 1.5827e-01, 1.4970e-01,\n",
      "          5.9903e-01, 4.6848e-01, 9.0825e-01, 7.8385e-01, 9.6623e-03,\n",
      "          4.0837e-01, 7.9070e-01, 1.0696e-01, 1.7586e-02, 8.2827e-01,\n",
      "          6.6048e-01, 7.8029e-01, 1.4286e-01, 4.4982e-01, 6.6455e-01,\n",
      "          3.7002e-02, 9.7021e-01, 7.6847e-01, 6.9995e-01, 6.6920e-01,\n",
      "          3.7863e-01, 5.5731e-01, 6.4042e-01, 1.3745e-01, 1.5277e-01,\n",
      "          3.1518e-02, 6.5846e-01, 2.5212e-01, 3.3640e-01, 2.1761e-01,\n",
      "          8.9124e-01, 2.6901e-01, 6.5644e-01, 7.5302e-01, 9.1805e-01,\n",
      "          3.7296e-01, 5.4244e-01, 9.7379e-01, 4.1188e-01, 1.0914e-01,\n",
      "          6.6407e-01, 4.7522e-01, 3.1778e-01, 3.2570e-01, 7.2275e-01,\n",
      "          7.4188e-01, 9.2217e-01, 6.8999e-01, 1.6567e-01, 2.0934e-01,\n",
      "          6.4876e-02, 6.1828e-02, 3.6849e-01, 8.8512e-01, 4.4879e-01,\n",
      "          6.3929e-01, 3.6447e-01, 6.2365e-01, 5.0518e-01, 3.2001e-02,\n",
      "          3.1936e-01, 3.3463e-01, 3.4284e-01, 6.4370e-01, 4.4390e-01,\n",
      "          1.2881e-01, 1.2497e-01, 4.5206e-01, 5.5550e-01, 2.2620e-01,\n",
      "          7.6123e-01, 3.9014e-01, 5.5509e-01, 1.1947e-01, 8.5631e-01,\n",
      "          3.2544e-01, 3.4619e-01, 8.0862e-01, 2.2777e-01, 8.3246e-01,\n",
      "          1.1074e-02, 2.3820e-01, 7.2051e-01, 7.1378e-01, 7.7986e-01,\n",
      "          5.3057e-01, 3.5421e-03, 2.9454e-02]],\n",
      "\n",
      "        [[3.7257e-01, 5.6698e-01, 6.0922e-01, 6.6192e-01, 4.9270e-01,\n",
      "          4.7223e-01, 5.8499e-02, 2.5636e-02, 8.0044e-01, 2.4048e-01,\n",
      "          1.9643e-02, 6.9699e-02, 2.9063e-02, 7.5520e-01, 8.2282e-01,\n",
      "          4.3135e-01, 1.5530e-01, 7.4719e-01, 8.9097e-01, 7.6514e-02,\n",
      "          9.1556e-01, 9.5038e-01, 2.0269e-01, 4.7266e-01, 7.3286e-01,\n",
      "          9.4731e-01, 2.0657e-01, 6.2011e-01, 5.8739e-01, 4.4846e-01,\n",
      "          9.3421e-01, 2.2485e-01, 1.8622e-01, 3.4508e-01, 7.6874e-01,\n",
      "          1.2905e-01, 4.4538e-01, 1.1056e-01, 3.1181e-01, 9.8310e-01,\n",
      "          4.0618e-01, 2.1167e-01, 1.9190e-01, 6.6641e-01, 4.9938e-01,\n",
      "          9.6768e-01, 8.4245e-01, 1.3569e-01, 3.4068e-01, 9.4598e-01,\n",
      "          4.4048e-01, 8.5261e-01, 8.0656e-01, 4.5253e-01, 4.6942e-01,\n",
      "          4.9643e-01, 8.3533e-01, 8.0147e-01, 8.3974e-01, 1.3135e-01,\n",
      "          4.6348e-01, 9.5949e-01, 5.1298e-01, 7.7412e-01, 5.2995e-01,\n",
      "          6.0421e-01, 3.3765e-01, 4.1796e-02, 7.5361e-01, 6.4207e-01,\n",
      "          5.0829e-01, 5.3415e-01, 6.5902e-01, 9.7403e-01, 3.7970e-01,\n",
      "          1.8955e-01, 6.3659e-01, 7.0385e-01, 7.7545e-01, 3.9284e-01,\n",
      "          9.9863e-01, 1.8955e-01, 8.0684e-01, 2.7356e-01, 3.3371e-01,\n",
      "          2.2162e-01, 8.0021e-01, 9.9672e-01, 3.4500e-01, 8.7197e-01,\n",
      "          5.9063e-01, 8.7038e-01, 8.4877e-01, 9.5753e-01, 8.4486e-01,\n",
      "          7.6693e-02, 7.4956e-01, 3.0927e-01, 6.7752e-01, 1.8608e-01,\n",
      "          5.3547e-01, 8.0255e-01, 8.2587e-01, 5.1136e-01, 7.5429e-01,\n",
      "          5.3730e-01, 9.4850e-01, 4.4215e-01, 6.6371e-01, 5.5015e-01,\n",
      "          9.5432e-01, 1.9289e-01, 9.3300e-01, 8.6315e-01, 9.8364e-02,\n",
      "          4.8386e-02, 9.1955e-01, 7.3599e-01, 6.9828e-01, 4.0990e-01,\n",
      "          7.9659e-01, 2.7203e-01, 2.3233e-01, 8.1561e-01, 1.4064e-02,\n",
      "          1.8216e-01, 8.9093e-01, 6.9053e-01]],\n",
      "\n",
      "        [[9.1377e-01, 4.1186e-01, 4.2986e-02, 4.4297e-01, 3.2308e-01,\n",
      "          3.1344e-02, 3.6590e-01, 9.1540e-01, 8.1729e-01, 9.6482e-01,\n",
      "          9.5583e-01, 8.1968e-01, 1.6885e-01, 7.4867e-01, 6.8106e-01,\n",
      "          9.4675e-02, 9.1585e-01, 7.0569e-01, 9.0552e-01, 4.0162e-01,\n",
      "          9.2811e-01, 4.9980e-02, 9.3229e-01, 4.8083e-01, 3.5846e-01,\n",
      "          1.4374e-01, 7.8429e-01, 3.2056e-01, 3.8576e-01, 2.9020e-01,\n",
      "          3.6078e-01, 4.9286e-01, 2.1778e-01, 2.0268e-01, 2.6842e-01,\n",
      "          7.5051e-01, 9.4996e-01, 2.2243e-01, 9.7490e-01, 7.6795e-01,\n",
      "          7.3581e-01, 4.2725e-02, 2.8493e-01, 4.1853e-01, 8.9131e-01,\n",
      "          2.6687e-01, 8.1805e-02, 7.1270e-01, 9.4648e-01, 9.1563e-01,\n",
      "          5.3444e-01, 4.6804e-01, 1.8201e-01, 6.4827e-01, 5.8809e-02,\n",
      "          1.6950e-01, 8.3650e-01, 8.6448e-02, 5.1679e-01, 8.5385e-01,\n",
      "          8.6950e-01, 3.0357e-01, 9.8047e-01, 1.9221e-01, 5.4088e-01,\n",
      "          5.4517e-01, 7.1184e-02, 5.9182e-01, 8.0726e-01, 2.1361e-01,\n",
      "          6.7998e-01, 6.0570e-02, 4.7104e-01, 8.2564e-01, 5.7426e-01,\n",
      "          8.9278e-01, 7.1220e-01, 9.5516e-01, 5.3084e-01, 1.3370e-01,\n",
      "          3.4609e-01, 4.1210e-01, 6.0133e-01, 1.6059e-01, 1.9645e-02,\n",
      "          9.5618e-01, 3.2578e-01, 3.3572e-01, 8.7334e-01, 1.0279e-01,\n",
      "          1.2145e-02, 2.3362e-01, 4.3675e-01, 8.0543e-02, 4.2591e-02,\n",
      "          5.5768e-02, 4.4277e-01, 1.0482e-01, 2.2827e-01, 3.6359e-01,\n",
      "          9.5268e-01, 6.5595e-01, 3.7973e-01, 3.4412e-01, 9.3768e-01,\n",
      "          3.2778e-01, 4.4228e-01, 4.1587e-01, 8.4074e-01, 4.2151e-01,\n",
      "          9.7961e-01, 5.3679e-01, 1.3997e-01, 5.5225e-01, 2.2370e-01,\n",
      "          6.2136e-01, 9.2087e-01, 6.4984e-02, 9.3486e-01, 3.8153e-01,\n",
      "          1.0889e-01, 2.6302e-01, 5.2463e-01, 2.5475e-01, 3.5915e-02,\n",
      "          5.9464e-01, 4.0828e-01, 7.3668e-02]],\n",
      "\n",
      "        [[7.8280e-01, 7.1566e-01, 1.9485e-02, 9.0422e-02, 5.3998e-01,\n",
      "          9.9924e-01, 3.6375e-02, 9.2509e-01, 9.2285e-01, 3.6992e-01,\n",
      "          9.5228e-01, 2.3691e-01, 5.7603e-01, 8.6377e-01, 1.1236e-01,\n",
      "          2.4957e-01, 1.4085e-01, 3.9393e-01, 8.4100e-01, 9.4430e-01,\n",
      "          1.5585e-01, 7.7855e-01, 3.1945e-01, 9.9208e-01, 6.8528e-01,\n",
      "          7.4704e-01, 7.0122e-01, 6.9088e-01, 2.5458e-01, 7.5753e-01,\n",
      "          2.3121e-01, 5.7048e-01, 6.3498e-01, 2.1546e-01, 5.2564e-01,\n",
      "          3.1723e-01, 9.5351e-01, 5.9192e-01, 8.6313e-01, 1.8367e-01,\n",
      "          7.1326e-01, 8.4985e-01, 7.5719e-01, 3.1286e-01, 5.4956e-01,\n",
      "          9.1667e-01, 3.4912e-01, 7.3106e-02, 3.1927e-01, 2.9687e-01,\n",
      "          9.5485e-02, 1.8518e-01, 2.2517e-02, 7.6437e-01, 4.1538e-04,\n",
      "          3.7300e-01, 9.2093e-01, 8.8957e-01, 7.0503e-01, 6.8730e-01,\n",
      "          7.2899e-01, 5.5208e-01, 7.4623e-01, 1.2920e-01, 2.7890e-01,\n",
      "          7.0155e-03, 4.4005e-01, 1.6208e-01, 7.3899e-01, 7.2312e-01,\n",
      "          9.5918e-01, 1.6876e-02, 5.7925e-01, 5.5559e-01, 1.2040e-01,\n",
      "          2.0126e-01, 9.3884e-01, 4.1890e-01, 9.9552e-01, 1.1436e-01,\n",
      "          6.2161e-02, 7.5037e-01, 7.3544e-01, 1.1227e-01, 7.7874e-01,\n",
      "          7.9448e-01, 7.0850e-01, 8.8414e-01, 9.2191e-01, 9.7886e-01,\n",
      "          4.3121e-01, 2.6773e-01, 2.0797e-01, 3.8149e-01, 5.7929e-01,\n",
      "          7.1507e-01, 8.9573e-01, 6.0915e-01, 7.5186e-01, 2.8582e-01,\n",
      "          2.3811e-01, 1.8185e-01, 9.8868e-01, 6.2340e-01, 3.4815e-01,\n",
      "          1.7225e-01, 9.7273e-01, 5.1388e-01, 4.6722e-01, 4.2225e-01,\n",
      "          2.1371e-02, 2.8295e-01, 3.4868e-01, 6.3438e-01, 8.0123e-01,\n",
      "          2.7685e-01, 9.4525e-01, 2.9192e-01, 1.8508e-01, 9.0168e-01,\n",
      "          2.6599e-01, 5.9033e-01, 2.7896e-01, 1.7870e-01, 5.4567e-02,\n",
      "          5.8396e-01, 7.2334e-01, 6.6689e-01]],\n",
      "\n",
      "        [[9.5625e-01, 5.4499e-01, 9.3498e-01, 6.2025e-01, 9.0517e-01,\n",
      "          3.8056e-01, 6.8529e-01, 3.3335e-01, 1.5760e-01, 5.5074e-01,\n",
      "          1.2406e-01, 6.0272e-01, 2.2311e-01, 2.8865e-01, 2.1917e-01,\n",
      "          1.2790e-01, 7.5185e-01, 9.3329e-01, 8.4287e-01, 3.2344e-01,\n",
      "          2.2199e-01, 5.4766e-01, 9.2170e-01, 1.3807e-01, 3.7202e-01,\n",
      "          9.0933e-01, 1.8522e-01, 8.3267e-01, 9.1770e-01, 4.7661e-02,\n",
      "          9.0664e-01, 7.2227e-01, 1.8492e-01, 6.6099e-01, 1.9615e-01,\n",
      "          4.7589e-01, 4.8396e-01, 9.7498e-01, 4.9666e-01, 8.2332e-01,\n",
      "          4.5319e-01, 3.2323e-01, 8.6116e-01, 4.2962e-01, 1.9683e-01,\n",
      "          3.2497e-01, 9.9222e-01, 3.3350e-01, 9.3670e-01, 8.7883e-01,\n",
      "          1.7115e-01, 4.8274e-01, 1.9959e-01, 3.7541e-02, 6.6398e-01,\n",
      "          1.3992e-01, 7.4790e-01, 7.0917e-01, 8.8795e-01, 9.0874e-01,\n",
      "          5.1914e-01, 7.3268e-01, 2.7665e-01, 2.5665e-01, 6.3299e-01,\n",
      "          1.1252e-01, 2.3971e-01, 1.7623e-01, 8.3157e-02, 2.5098e-01,\n",
      "          8.2158e-01, 1.7658e-01, 1.2335e-01, 1.4760e-01, 4.3737e-02,\n",
      "          9.5190e-01, 7.9620e-01, 5.3373e-02, 2.6320e-01, 9.6924e-01,\n",
      "          8.7919e-02, 8.5657e-01, 5.7132e-01, 3.8353e-01, 5.4739e-01,\n",
      "          2.4022e-01, 6.8669e-01, 2.2372e-01, 4.1718e-01, 9.5567e-01,\n",
      "          5.8976e-01, 5.2977e-01, 4.4872e-01, 7.0778e-01, 4.3346e-01,\n",
      "          6.4365e-01, 2.1335e-01, 4.8383e-01, 9.3358e-01, 1.2106e-01,\n",
      "          3.5059e-01, 1.0974e-01, 6.9466e-01, 7.4887e-01, 9.7784e-01,\n",
      "          2.7188e-01, 1.8061e-01, 5.7441e-02, 3.0277e-01, 1.1468e-02,\n",
      "          1.9746e-01, 6.8698e-01, 2.4243e-01, 2.2927e-01, 6.9653e-01,\n",
      "          3.6152e-01, 9.9792e-01, 1.4134e-01, 9.3974e-01, 9.7508e-01,\n",
      "          8.7015e-01, 6.4451e-01, 1.5368e-02, 1.7938e-01, 3.2657e-01,\n",
      "          7.8052e-02, 6.4799e-01, 9.1086e-01]]])\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "q tensor([[[-9.3230e-02, -3.1664e-01,  2.2766e-01,  6.2602e-02, -1.6939e-01,\n",
      "           9.7068e-02,  2.6378e-01,  1.7867e-01, -6.3587e-01, -1.4939e-01,\n",
      "          -4.5908e-01,  5.9245e-01,  1.2420e-01, -4.7608e-01, -3.4031e-01,\n",
      "           1.0017e-01,  1.6993e-01, -6.8586e-02,  5.6567e-01, -4.4146e-01,\n",
      "           3.2865e-01, -2.9011e-02, -2.4721e-01, -5.5670e-01,  2.7859e-02,\n",
      "          -2.5268e-01,  1.8812e-02, -6.5971e-02, -4.7545e-01, -1.2233e-01,\n",
      "          -2.1484e-01, -3.9300e-02, -7.3102e-02,  2.3093e-01,  3.4013e-01,\n",
      "           9.7597e-02,  8.7228e-01,  3.8766e-01, -7.1912e-01, -3.0206e-01,\n",
      "          -1.3512e-01, -1.1493e-01, -2.3988e-01, -7.9064e-01,  6.8818e-01,\n",
      "           1.9137e-02, -2.7939e-01,  7.4670e-01,  4.1821e-01,  1.7998e-01,\n",
      "          -2.1501e-01,  1.9410e-02,  6.3482e-01,  3.5314e-02,  2.7004e-01,\n",
      "          -1.1900e-01,  1.0728e-01, -6.3466e-02,  2.5929e-01, -1.8106e-01,\n",
      "           5.9437e-01,  2.7136e-02, -8.7423e-02,  4.2124e-01, -7.4927e-02,\n",
      "           4.6394e-01,  8.0619e-01,  2.8879e-01, -3.6917e-01,  9.7486e-01,\n",
      "          -2.3317e-01, -5.0904e-01, -4.9242e-01,  3.4261e-01, -2.9481e-01,\n",
      "          -7.4662e-02,  1.2645e-01, -6.9842e-01, -4.3974e-01,  4.9088e-01,\n",
      "          -4.9133e-01,  3.0801e-01, -3.7810e-01,  2.8268e-01, -2.0779e-02,\n",
      "          -5.0257e-01, -2.5016e-01, -2.7010e-01,  2.5143e-01,  3.3027e-01,\n",
      "          -6.0965e-01,  4.3048e-01,  2.9429e-01, -6.1927e-02, -6.0787e-01,\n",
      "          -6.7685e-02, -5.7005e-01,  4.2693e-01,  2.5645e-02,  4.0810e-01,\n",
      "          -2.4836e-01, -1.2185e-01,  2.9910e-01,  2.0631e-01, -6.7250e-02,\n",
      "           9.3299e-01, -3.4880e-01,  7.4059e-01, -2.6716e-01, -4.4411e-01,\n",
      "           5.1696e-01,  2.3636e-01,  5.4637e-01, -3.4470e-01,  3.3478e-01,\n",
      "          -7.8337e-01,  2.4250e-02, -1.3250e-01,  5.1844e-01, -1.1314e-01,\n",
      "          -2.5518e-01, -8.8353e-01,  5.6114e-01, -3.4701e-01,  1.2222e-01,\n",
      "           1.3163e-01,  6.9692e-02, -2.4454e-01]],\n",
      "\n",
      "        [[ 2.5447e-01, -1.6305e-01,  2.2445e-01,  1.2910e-01,  1.6298e-01,\n",
      "          -9.7340e-02,  4.3547e-01,  1.5430e-01, -4.1063e-01,  6.0968e-01,\n",
      "          -3.5619e-01,  5.1315e-01, -4.9732e-02,  4.6956e-01, -2.2573e-01,\n",
      "          -1.3231e-01,  2.2965e-01, -2.1338e-01,  2.1442e-01, -3.7089e-01,\n",
      "          -1.3013e-01,  1.5569e-01,  3.8347e-04, -5.1650e-01,  6.3140e-01,\n",
      "          -1.9814e-01, -3.4936e-01, -7.0221e-03, -3.8385e-01, -3.1135e-01,\n",
      "           3.9580e-02, -4.0451e-02,  2.5053e-01,  7.8018e-01,  1.3538e-01,\n",
      "          -4.9042e-01,  6.7105e-01,  7.8672e-01, -8.6713e-01, -7.9045e-01,\n",
      "           4.4325e-01,  5.2245e-01, -7.0996e-01, -9.6046e-01,  3.6674e-01,\n",
      "           3.8127e-01, -1.8542e-02,  5.9417e-01, -1.0532e-01,  6.9090e-02,\n",
      "          -3.1467e-01,  5.6182e-02,  7.4196e-01, -1.0941e-01,  3.1031e-01,\n",
      "          -1.6955e-01, -1.8575e-01, -7.9849e-02, -2.2492e-01,  1.5275e-01,\n",
      "           1.0299e+00,  1.7205e-01, -1.6041e-01,  2.8327e-01, -2.0330e-01,\n",
      "           3.0482e-01,  9.1042e-01,  1.1173e-01, -1.7749e-01,  5.6587e-01,\n",
      "          -1.3552e-01, -5.7362e-01, -5.3292e-01,  4.0338e-01,  4.1507e-01,\n",
      "           1.3053e-01,  1.2720e-01, -4.2974e-01, -7.2264e-01,  8.7854e-01,\n",
      "          -1.1966e-01,  2.4945e-01, -6.3522e-02,  6.9011e-01, -5.4689e-01,\n",
      "          -8.0142e-01,  1.0509e-01,  8.4604e-02,  1.7528e-01,  2.8705e-01,\n",
      "          -7.0199e-01,  4.3872e-01,  3.1989e-01,  2.7669e-01,  1.4340e-01,\n",
      "          -2.1895e-01, -5.8014e-01,  3.9510e-01,  1.5405e-01, -1.7530e-02,\n",
      "          -3.7567e-01,  1.7443e-01,  6.1885e-02,  1.8850e-01, -5.4031e-01,\n",
      "           2.7766e-01, -2.2406e-01,  7.1565e-01, -3.3471e-03, -1.8293e-01,\n",
      "           5.5658e-01, -4.0974e-02,  7.3199e-01,  1.9069e-02,  4.1075e-01,\n",
      "          -1.1134e+00, -1.8910e-01, -2.4550e-01,  3.7262e-01, -1.3528e-01,\n",
      "          -1.5049e-01, -1.4232e+00,  4.6977e-01, -1.8106e-02,  5.1254e-01,\n",
      "           3.1793e-01, -2.4615e-01, -6.3040e-01]],\n",
      "\n",
      "        [[ 9.0284e-02,  4.4382e-01,  6.6005e-02,  2.3123e-01, -2.9620e-01,\n",
      "           1.2283e-01,  2.0117e-01,  1.0059e-01, -3.4599e-01,  2.2273e-01,\n",
      "          -4.5863e-01,  4.0404e-02,  4.6677e-02, -3.3017e-01, -8.0341e-02,\n",
      "           1.1267e-01,  2.0451e-01, -1.3281e-02,  1.7807e-01, -6.3942e-01,\n",
      "           3.1263e-01,  2.2662e-01, -1.1500e-01, -1.5307e-01,  6.8811e-01,\n",
      "          -4.6659e-01,  3.4120e-03, -1.5673e-01, -3.1834e-01, -3.2022e-01,\n",
      "          -1.4155e-01, -8.0942e-02, -2.1672e-01,  2.6433e-01,  2.4635e-01,\n",
      "           1.0845e-01,  6.1117e-01,  4.1761e-01, -2.5281e-01, -4.3345e-01,\n",
      "          -8.1460e-02,  4.1230e-01, -7.0181e-01, -9.8376e-01,  1.0501e-01,\n",
      "           6.6774e-02, -2.5906e-01,  1.8751e-01,  3.1135e-01,  2.8005e-01,\n",
      "          -2.1223e-01,  1.4463e-01,  5.5574e-01, -2.2625e-01,  6.3912e-01,\n",
      "          -6.2435e-01,  1.9267e-01, -4.3053e-01, -1.4136e-01,  3.2985e-01,\n",
      "           2.7724e-01,  3.9828e-01, -9.2796e-02,  1.4728e-02,  4.0047e-01,\n",
      "           1.9341e-01,  1.0613e+00, -8.3823e-02, -2.7608e-01,  5.6366e-01,\n",
      "           2.7872e-01, -3.7452e-01, -3.3367e-01,  1.6095e-01, -1.4521e-02,\n",
      "          -3.9203e-01,  2.1860e-03, -3.7300e-01, -4.0474e-01,  5.6339e-01,\n",
      "          -1.1135e-01,  6.5424e-01, -3.1739e-01,  5.0160e-01,  4.4126e-02,\n",
      "          -4.8833e-01,  5.6407e-02, -1.7515e-01, -1.7806e-03,  3.1886e-01,\n",
      "          -8.9889e-02,  3.3831e-01,  1.8208e-01,  1.0119e-01, -2.3493e-01,\n",
      "          -3.8165e-01, -3.5588e-01,  6.0671e-01, -2.4704e-02,  3.0834e-01,\n",
      "          -3.3402e-01,  2.0341e-01, -1.1320e-01,  3.8557e-01, -2.9711e-01,\n",
      "           2.7358e-01,  8.2012e-03, -1.3857e-01,  9.2697e-02,  8.9933e-02,\n",
      "           2.7363e-01,  1.0838e-02,  5.1796e-01, -5.3322e-01,  4.0097e-01,\n",
      "          -5.1152e-01, -3.0171e-01, -1.1644e-01,  2.3169e-01, -2.4415e-01,\n",
      "          -5.8742e-01, -6.3836e-01,  9.9485e-01,  1.8093e-01,  7.5265e-02,\n",
      "           3.4292e-01, -1.2435e-01, -3.7263e-01]],\n",
      "\n",
      "        [[ 4.7304e-01, -2.9821e-01,  7.2142e-02,  4.4643e-01,  1.0761e-01,\n",
      "          -9.9236e-02,  2.7193e-01,  2.4571e-01, -5.6149e-01,  2.5861e-01,\n",
      "          -3.1400e-01,  3.1499e-01,  2.9786e-01, -1.4133e-01, -4.5123e-01,\n",
      "          -1.9896e-01,  2.4899e-01,  4.3167e-02,  5.4438e-01, -5.4866e-01,\n",
      "           8.5813e-02,  2.9571e-01, -1.7896e-01, -6.9964e-01,  6.5636e-01,\n",
      "          -5.2064e-01, -2.5108e-01,  1.2825e-01, -2.6975e-01, -2.8685e-01,\n",
      "          -2.1031e-01, -2.0995e-01,  3.1657e-02,  3.4539e-01,  7.6140e-02,\n",
      "          -4.0932e-01,  6.2548e-01,  2.0528e-01, -2.6703e-01, -9.4908e-01,\n",
      "           2.1736e-01,  2.6729e-01, -4.4053e-01, -1.1962e+00,  3.9792e-01,\n",
      "           4.2349e-01,  2.8856e-02,  8.7878e-01,  6.6569e-02,  3.5085e-01,\n",
      "          -4.1384e-01,  2.6155e-01,  5.5789e-01, -3.0553e-01,  6.3790e-01,\n",
      "          -1.8125e-02,  2.4490e-01,  1.9517e-01, -2.4526e-01,  2.9932e-03,\n",
      "           6.5875e-01,  2.8281e-01,  1.3619e-01,  1.1233e-01, -8.3954e-02,\n",
      "           4.2601e-01,  8.3922e-01,  5.2383e-02, -1.2997e-01,  1.0311e+00,\n",
      "          -1.2174e-01, -4.6421e-01, -8.6354e-01,  8.1467e-01,  3.4908e-01,\n",
      "          -1.8741e-01,  2.0662e-01, -5.8144e-01, -8.0955e-01,  3.9555e-01,\n",
      "          -3.3114e-01,  3.3677e-01, -2.2353e-01,  7.0310e-01, -3.6505e-01,\n",
      "          -4.8964e-01,  2.1926e-01,  7.6488e-02,  1.5035e-01,  4.3894e-01,\n",
      "          -5.3716e-01,  8.2315e-02,  2.4050e-01,  1.0455e-01, -4.3122e-01,\n",
      "          -2.8648e-01, -2.8030e-01,  2.3704e-01, -4.4069e-01,  2.1832e-01,\n",
      "          -3.1563e-01,  1.5635e-02,  1.2230e-01,  2.8538e-01, -1.3984e-01,\n",
      "           3.5306e-01,  5.4719e-02,  5.4374e-01,  1.7570e-01,  1.6729e-01,\n",
      "           3.4461e-01, -3.7319e-01,  8.9689e-01, -4.0673e-01,  1.9846e-01,\n",
      "          -5.2594e-01, -1.7897e-01,  3.1120e-02,  8.6805e-01,  2.3641e-01,\n",
      "          -2.6560e-01, -1.0421e+00,  6.1012e-01, -2.6766e-01,  1.7952e-01,\n",
      "           4.0341e-01, -4.0948e-01, -6.1817e-01]],\n",
      "\n",
      "        [[-3.5666e-01, -5.2844e-01, -1.3266e-01,  3.7541e-01,  2.0246e-02,\n",
      "           2.3184e-01,  4.8048e-01,  6.0972e-01, -2.1244e-01,  2.0814e-01,\n",
      "          -3.6351e-01,  8.5950e-01,  1.6644e-01, -3.2858e-01, -5.0650e-01,\n",
      "          -3.4898e-01,  1.8276e-02, -7.7261e-02,  7.1451e-01, -1.5003e-01,\n",
      "          -1.6945e-01,  2.6978e-01, -1.1948e-01, -3.8274e-01,  7.2097e-01,\n",
      "          -2.9581e-01, -4.5471e-01,  2.0897e-01, -5.6787e-02, -1.6696e-01,\n",
      "           1.5412e-01,  8.2494e-03, -1.6521e-01,  6.1757e-01,  2.5251e-01,\n",
      "           1.6677e-01,  8.7835e-01,  1.0506e-01, -9.2848e-01, -2.2295e-01,\n",
      "           3.8303e-01,  2.5683e-01, -3.3511e-01, -1.1128e+00,  1.1392e-01,\n",
      "           1.8991e-01, -5.2047e-01,  5.6495e-01, -5.7792e-04,  3.5727e-01,\n",
      "          -2.7951e-01,  1.9272e-01,  5.6442e-01,  3.4630e-01,  6.0245e-01,\n",
      "          -4.3493e-02,  9.6655e-02, -4.2170e-01,  1.9866e-01,  9.4952e-02,\n",
      "           5.2353e-01,  3.5239e-01,  4.6360e-01,  6.3745e-02,  1.6779e-01,\n",
      "           2.0414e-01,  7.4276e-01,  2.4273e-01, -1.3203e-02,  5.1070e-01,\n",
      "          -9.6053e-02, -4.4275e-01, -1.6114e-01,  3.7643e-01,  6.4139e-01,\n",
      "          -2.9312e-01,  3.8494e-01, -4.3204e-01, -7.7222e-01,  8.2446e-01,\n",
      "          -7.2547e-01,  5.0099e-01, -2.4068e-01, -8.1483e-02, -6.9088e-01,\n",
      "          -9.0165e-01, -2.1697e-01, -3.5380e-02,  5.1951e-01,  3.3871e-01,\n",
      "          -7.8011e-01,  1.9039e-01,  1.0499e-01, -2.0346e-01, -4.7057e-01,\n",
      "          -1.9782e-01, -5.8346e-01,  1.1397e-01, -2.7704e-02,  3.0421e-01,\n",
      "           8.8339e-03,  6.0605e-02,  1.8734e-01,  2.8492e-01, -3.2186e-01,\n",
      "           1.1354e-01,  1.4381e-01, -8.7258e-02, -2.6349e-01,  2.4684e-01,\n",
      "           6.2506e-01, -2.1817e-01,  5.6629e-01,  1.0711e-01,  5.1345e-01,\n",
      "          -1.3210e+00, -3.6233e-02, -4.8597e-01,  7.4421e-01, -3.3420e-01,\n",
      "          -2.4993e-01, -8.5108e-01,  1.2152e+00,  1.2019e-01,  2.5790e-01,\n",
      "           2.5394e-01, -1.4113e-01, -5.5536e-01]]], grad_fn=<SplitBackward>)\n",
      "here2\n",
      "here!\n",
      "q tensor([[[-8.2404e-03, -2.7987e-02,  2.0123e-02,  5.5333e-03, -1.4972e-02,\n",
      "           8.5797e-03,  2.3315e-02,  1.5792e-02, -5.6204e-02, -1.3204e-02,\n",
      "          -4.0577e-02,  5.2366e-02,  1.0977e-02, -4.2080e-02, -3.0079e-02,\n",
      "           8.8536e-03,  1.5019e-02, -6.0622e-03,  4.9998e-02, -3.9020e-02,\n",
      "           2.9048e-02, -2.5643e-03, -2.1851e-02, -4.9206e-02,  2.4624e-03,\n",
      "          -2.2334e-02,  1.6628e-03, -5.8311e-03, -4.2024e-02, -1.0813e-02,\n",
      "          -1.8989e-02, -3.4736e-03, -6.4613e-03,  2.0411e-02,  3.0064e-02,\n",
      "           8.6264e-03,  7.7099e-02,  3.4264e-02, -6.3562e-02, -2.6698e-02,\n",
      "          -1.1943e-02, -1.0159e-02, -2.1202e-02, -6.9884e-02,  6.0827e-02,\n",
      "           1.6915e-03, -2.4694e-02,  6.5999e-02,  3.6965e-02,  1.5908e-02,\n",
      "          -1.9005e-02,  1.7156e-03,  5.6111e-02,  3.1213e-03,  2.3868e-02,\n",
      "          -1.0518e-02,  9.4827e-03, -5.6096e-03,  2.2918e-02, -1.6004e-02,\n",
      "           5.2535e-02,  2.3985e-03, -7.7272e-03,  3.7232e-02, -6.6227e-03,\n",
      "           4.1007e-02,  7.1258e-02,  2.5526e-02, -3.2631e-02,  8.6166e-02,\n",
      "          -2.0610e-02, -4.4993e-02, -4.3524e-02,  3.0283e-02, -2.6058e-02,\n",
      "          -6.5992e-03,  1.1177e-02, -6.1733e-02, -3.8868e-02,  4.3388e-02,\n",
      "          -4.3428e-02,  2.7224e-02, -3.3419e-02,  2.4986e-02, -1.8366e-03,\n",
      "          -4.4421e-02, -2.2112e-02, -2.3874e-02,  2.2223e-02,  2.9192e-02,\n",
      "          -5.3886e-02,  3.8050e-02,  2.6011e-02, -5.4736e-03, -5.3728e-02,\n",
      "          -5.9826e-03, -5.0386e-02,  3.7736e-02,  2.2667e-03,  3.6071e-02,\n",
      "          -2.1952e-02, -1.0770e-02,  2.6437e-02,  1.8236e-02, -5.9441e-03,\n",
      "           8.2466e-02, -3.0830e-02,  6.5460e-02, -2.3614e-02, -3.9254e-02,\n",
      "           4.5693e-02,  2.0891e-02,  4.8293e-02, -3.0467e-02,  2.9591e-02,\n",
      "          -6.9241e-02,  2.1435e-03, -1.1711e-02,  4.5824e-02, -1.0000e-02,\n",
      "          -2.2555e-02, -7.8094e-02,  4.9598e-02, -3.0672e-02,  1.0803e-02,\n",
      "           1.1635e-02,  6.1600e-03, -2.1614e-02]],\n",
      "\n",
      "        [[ 2.2492e-02, -1.4412e-02,  1.9839e-02,  1.1411e-02,  1.4406e-02,\n",
      "          -8.6037e-03,  3.8491e-02,  1.3638e-02, -3.6295e-02,  5.3889e-02,\n",
      "          -3.1483e-02,  4.5357e-02, -4.3958e-03,  4.1503e-02, -1.9952e-02,\n",
      "          -1.1695e-02,  2.0298e-02, -1.8860e-02,  1.8953e-02, -3.2782e-02,\n",
      "          -1.1502e-02,  1.3761e-02,  3.3894e-05, -4.5652e-02,  5.5808e-02,\n",
      "          -1.7513e-02, -3.0879e-02, -6.2068e-04, -3.3928e-02, -2.7520e-02,\n",
      "           3.4984e-03, -3.5754e-03,  2.2144e-02,  6.8959e-02,  1.1966e-02,\n",
      "          -4.3347e-02,  5.9313e-02,  6.9537e-02, -7.6644e-02, -6.9866e-02,\n",
      "           3.9178e-02,  4.6179e-02, -6.2752e-02, -8.4894e-02,  3.2415e-02,\n",
      "           3.3700e-02, -1.6389e-03,  5.2518e-02, -9.3087e-03,  6.1068e-03,\n",
      "          -2.7813e-02,  4.9658e-03,  6.5580e-02, -9.6705e-03,  2.7428e-02,\n",
      "          -1.4986e-02, -1.6418e-02, -7.0577e-03, -1.9881e-02,  1.3502e-02,\n",
      "           9.1029e-02,  1.5207e-02, -1.4178e-02,  2.5037e-02, -1.7969e-02,\n",
      "           2.6943e-02,  8.0470e-02,  9.8753e-03, -1.5688e-02,  5.0017e-02,\n",
      "          -1.1979e-02, -5.0701e-02, -4.7104e-02,  3.5654e-02,  3.6687e-02,\n",
      "           1.1538e-02,  1.1243e-02, -3.7984e-02, -6.3873e-02,  7.7653e-02,\n",
      "          -1.0577e-02,  2.2048e-02, -5.6146e-03,  6.0998e-02, -4.8339e-02,\n",
      "          -7.0836e-02,  9.2883e-03,  7.4780e-03,  1.5493e-02,  2.5372e-02,\n",
      "          -6.2047e-02,  3.8778e-02,  2.8274e-02,  2.4456e-02,  1.2675e-02,\n",
      "          -1.9352e-02, -5.1278e-02,  3.4922e-02,  1.3616e-02, -1.5495e-03,\n",
      "          -3.3205e-02,  1.5418e-02,  5.4699e-03,  1.6661e-02, -4.7757e-02,\n",
      "           2.4542e-02, -1.9804e-02,  6.3255e-02, -2.9585e-04, -1.6169e-02,\n",
      "           4.9195e-02, -3.6216e-03,  6.4700e-02,  1.6855e-03,  3.6306e-02,\n",
      "          -9.8416e-02, -1.6714e-02, -2.1699e-02,  3.2935e-02, -1.1957e-02,\n",
      "          -1.3301e-02, -1.2580e-01,  4.1522e-02, -1.6003e-03,  4.5303e-02,\n",
      "           2.8101e-02, -2.1757e-02, -5.5720e-02]],\n",
      "\n",
      "        [[ 7.9800e-03,  3.9229e-02,  5.8340e-03,  2.0438e-02, -2.6180e-02,\n",
      "           1.0857e-02,  1.7781e-02,  8.8914e-03, -3.0581e-02,  1.9687e-02,\n",
      "          -4.0538e-02,  3.5713e-03,  4.1257e-03, -2.9183e-02, -7.1012e-03,\n",
      "           9.9590e-03,  1.8077e-02, -1.1739e-03,  1.5739e-02, -5.6517e-02,\n",
      "           2.7633e-02,  2.0030e-02, -1.0165e-02, -1.3529e-02,  6.0821e-02,\n",
      "          -4.1241e-02,  3.0158e-04, -1.3854e-02, -2.8138e-02, -2.8304e-02,\n",
      "          -1.2511e-02, -7.1543e-03, -1.9155e-02,  2.3363e-02,  2.1774e-02,\n",
      "           9.5854e-03,  5.4021e-02,  3.6912e-02, -2.2346e-02, -3.8312e-02,\n",
      "          -7.2001e-03,  3.6443e-02, -6.2032e-02, -8.6953e-02,  9.2815e-03,\n",
      "           5.9021e-03, -2.2898e-02,  1.6574e-02,  2.7519e-02,  2.4753e-02,\n",
      "          -1.8758e-02,  1.2783e-02,  4.9121e-02, -1.9998e-02,  5.6491e-02,\n",
      "          -5.5185e-02,  1.7029e-02, -3.8054e-02, -1.2495e-02,  2.9155e-02,\n",
      "           2.4505e-02,  3.5203e-02, -8.2021e-03,  1.3018e-03,  3.5397e-02,\n",
      "           1.7095e-02,  9.3805e-02, -7.4090e-03, -2.4402e-02,  4.9821e-02,\n",
      "           2.4635e-02, -3.3103e-02, -2.9493e-02,  1.4226e-02, -1.2835e-03,\n",
      "          -3.4651e-02,  1.9322e-04, -3.2969e-02, -3.5774e-02,  4.9797e-02,\n",
      "          -9.8425e-03,  5.7827e-02, -2.8054e-02,  4.4335e-02,  3.9002e-03,\n",
      "          -4.3163e-02,  4.9858e-03, -1.5481e-02, -1.5739e-04,  2.8184e-02,\n",
      "          -7.9451e-03,  2.9902e-02,  1.6093e-02,  8.9444e-03, -2.0765e-02,\n",
      "          -3.3733e-02, -3.1456e-02,  5.3626e-02, -2.1836e-03,  2.7254e-02,\n",
      "          -2.9523e-02,  1.7979e-02, -1.0006e-02,  3.4080e-02, -2.6261e-02,\n",
      "           2.4181e-02,  7.2489e-04, -1.2248e-02,  8.1934e-03,  7.9491e-03,\n",
      "           2.4186e-02,  9.5798e-04,  4.5782e-02, -4.7131e-02,  3.5441e-02,\n",
      "          -4.5213e-02, -2.6668e-02, -1.0292e-02,  2.0479e-02, -2.1580e-02,\n",
      "          -5.1921e-02, -5.6423e-02,  8.7933e-02,  1.5992e-02,  6.6525e-03,\n",
      "           3.0310e-02, -1.0991e-02, -3.2936e-02]],\n",
      "\n",
      "        [[ 4.1811e-02, -2.6359e-02,  6.3765e-03,  3.9459e-02,  9.5111e-03,\n",
      "          -8.7713e-03,  2.4036e-02,  2.1718e-02, -4.9629e-02,  2.2858e-02,\n",
      "          -2.7754e-02,  2.7842e-02,  2.6327e-02, -1.2492e-02, -3.9883e-02,\n",
      "          -1.7586e-02,  2.2008e-02,  3.8154e-03,  4.8117e-02, -4.8495e-02,\n",
      "           7.5849e-03,  2.6137e-02, -1.5818e-02, -6.1840e-02,  5.8015e-02,\n",
      "          -4.6019e-02, -2.2192e-02,  1.1335e-02, -2.3843e-02, -2.5355e-02,\n",
      "          -1.8589e-02, -1.8558e-02,  2.7981e-03,  3.0529e-02,  6.7299e-03,\n",
      "          -3.6179e-02,  5.5286e-02,  1.8145e-02, -2.3602e-02, -8.3888e-02,\n",
      "           1.9212e-02,  2.3626e-02, -3.8938e-02, -1.0573e-01,  3.5172e-02,\n",
      "           3.7431e-02,  2.5505e-03,  7.7674e-02,  5.8840e-03,  3.1011e-02,\n",
      "          -3.6579e-02,  2.3118e-02,  4.9311e-02, -2.7005e-02,  5.6383e-02,\n",
      "          -1.6021e-03,  2.1646e-02,  1.7251e-02, -2.1678e-02,  2.6456e-04,\n",
      "           5.8226e-02,  2.4997e-02,  1.2037e-02,  9.9287e-03, -7.4205e-03,\n",
      "           3.7654e-02,  7.4177e-02,  4.6300e-03, -1.1488e-02,  9.1136e-02,\n",
      "          -1.0760e-02, -4.1031e-02, -7.6327e-02,  7.2007e-02,  3.0855e-02,\n",
      "          -1.6565e-02,  1.8262e-02, -5.1393e-02, -7.1555e-02,  3.4962e-02,\n",
      "          -2.9269e-02,  2.9766e-02, -1.9757e-02,  6.2146e-02, -3.2267e-02,\n",
      "          -4.3278e-02,  1.9380e-02,  6.7606e-03,  1.3289e-02,  3.8797e-02,\n",
      "          -4.7479e-02,  7.2757e-03,  2.1258e-02,  9.2412e-03, -3.8115e-02,\n",
      "          -2.5321e-02, -2.4775e-02,  2.0951e-02, -3.8952e-02,  1.9297e-02,\n",
      "          -2.7898e-02,  1.3820e-03,  1.0810e-02,  2.5224e-02, -1.2360e-02,\n",
      "           3.1207e-02,  4.8365e-03,  4.8060e-02,  1.5530e-02,  1.4786e-02,\n",
      "           3.0459e-02, -3.2986e-02,  7.9274e-02, -3.5950e-02,  1.7542e-02,\n",
      "          -4.6487e-02, -1.5819e-02,  2.7507e-03,  7.6726e-02,  2.0896e-02,\n",
      "          -2.3476e-02, -9.2110e-02,  5.3928e-02, -2.3658e-02,  1.5867e-02,\n",
      "           3.5656e-02, -3.6193e-02, -5.4639e-02]],\n",
      "\n",
      "        [[-3.1525e-02, -4.6708e-02, -1.1725e-02,  3.3182e-02,  1.7895e-03,\n",
      "           2.0492e-02,  4.2469e-02,  5.3892e-02, -1.8777e-02,  1.8397e-02,\n",
      "          -3.2130e-02,  7.5970e-02,  1.4712e-02, -2.9042e-02, -4.4769e-02,\n",
      "          -3.0846e-02,  1.6154e-03, -6.8290e-03,  6.3154e-02, -1.3261e-02,\n",
      "          -1.4978e-02,  2.3845e-02, -1.0561e-02, -3.3830e-02,  6.3725e-02,\n",
      "          -2.6146e-02, -4.0191e-02,  1.8471e-02, -5.0193e-03, -1.4757e-02,\n",
      "           1.3622e-02,  7.2915e-04, -1.4602e-02,  5.4586e-02,  2.2319e-02,\n",
      "           1.4741e-02,  7.7636e-02,  9.2864e-03, -8.2067e-02, -1.9706e-02,\n",
      "           3.3856e-02,  2.2701e-02, -2.9619e-02, -9.8358e-02,  1.0070e-02,\n",
      "           1.6785e-02, -4.6004e-02,  4.9935e-02, -5.1081e-05,  3.1579e-02,\n",
      "          -2.4706e-02,  1.7034e-02,  4.9888e-02,  3.0609e-02,  5.3250e-02,\n",
      "          -3.8443e-03,  8.5432e-03, -3.7273e-02,  1.7560e-02,  8.3926e-03,\n",
      "           4.6274e-02,  3.1147e-02,  4.0977e-02,  5.6344e-03,  1.4831e-02,\n",
      "           1.8044e-02,  6.5651e-02,  2.1454e-02, -1.1670e-03,  4.5140e-02,\n",
      "          -8.4900e-03, -3.9134e-02, -1.4243e-02,  3.3272e-02,  5.6691e-02,\n",
      "          -2.5908e-02,  3.4024e-02, -3.8187e-02, -6.8255e-02,  7.2873e-02,\n",
      "          -6.4123e-02,  4.4282e-02, -2.1273e-02, -7.2022e-03, -6.1066e-02,\n",
      "          -7.9695e-02, -1.9178e-02, -3.1272e-03,  4.5919e-02,  2.9938e-02,\n",
      "          -6.8953e-02,  1.6828e-02,  9.2802e-03, -1.7983e-02, -4.1593e-02,\n",
      "          -1.7485e-02, -5.1571e-02,  1.0073e-02, -2.4487e-03,  2.6888e-02,\n",
      "           7.8081e-04,  5.3568e-03,  1.6559e-02,  2.5183e-02, -2.8449e-02,\n",
      "           1.0036e-02,  1.2711e-02, -7.7126e-03, -2.3289e-02,  2.1817e-02,\n",
      "           5.5248e-02, -1.9284e-02,  5.0053e-02,  9.4672e-03,  4.5383e-02,\n",
      "          -1.1676e-01, -3.2026e-03, -4.2954e-02,  6.5779e-02, -2.9540e-02,\n",
      "          -2.2091e-02, -7.5225e-02,  1.0741e-01,  1.0623e-02,  2.2795e-02,\n",
      "           2.2445e-02, -1.2474e-02, -4.9087e-02]]], grad_fn=<MulBackward0>)\n",
      "k tensor([[[-7.3593e-02,  9.4897e-01, -5.3269e-01, -2.6371e-01,  2.7067e-01,\n",
      "           3.9201e-01, -3.4223e-01,  1.6791e-01, -5.8129e-01,  5.4348e-01,\n",
      "          -5.9097e-01, -2.4579e-01,  3.8324e-01, -2.3148e-01,  2.0345e-02,\n",
      "           5.1226e-02,  8.2617e-01,  2.9816e-01, -1.2391e-01, -2.2641e-01,\n",
      "          -1.0790e-02, -8.5838e-02,  2.1071e-01, -5.8849e-01, -5.1696e-01,\n",
      "           1.3232e-01,  5.2038e-01, -1.9229e-01, -8.9872e-01,  1.3239e-01,\n",
      "          -3.6571e-01,  1.2276e-01, -4.2340e-01, -1.2735e-01, -3.4364e-01,\n",
      "          -3.5604e-01, -6.4204e-01, -1.3341e-01, -6.8438e-02, -4.6667e-02,\n",
      "           1.5899e-01, -2.8077e-01,  4.7403e-01,  1.7666e-02, -6.6386e-01,\n",
      "          -3.8731e-01,  1.3547e-01,  6.8675e-01,  5.5895e-01, -2.0888e-02,\n",
      "          -4.9916e-02,  2.9085e-01, -2.3666e-01,  4.3537e-01,  7.3652e-02,\n",
      "          -3.1271e-01, -4.5457e-02, -4.0575e-01,  2.2919e-01,  5.7682e-01,\n",
      "          -4.8864e-01,  2.8240e-01,  5.1380e-01,  3.4074e-01, -1.1962e-01,\n",
      "           2.4729e-01,  4.1487e-01, -5.1139e-01,  2.1235e-01, -6.5116e-02,\n",
      "           1.0620e-01,  7.1541e-02, -3.0326e-01,  1.5117e-02, -2.8570e-01,\n",
      "          -2.2281e-01,  6.3385e-01,  3.8335e-01, -3.4503e-01, -4.8337e-01,\n",
      "          -4.0113e-01, -2.1868e-02,  9.5806e-01,  7.3584e-01,  5.5559e-01,\n",
      "           3.4769e-01, -2.2669e-01, -6.1151e-01, -1.3418e-02,  3.7603e-01,\n",
      "          -7.0554e-02,  5.1684e-01,  9.8954e-02,  2.2878e-01,  6.4211e-01,\n",
      "          -5.6873e-02, -5.2855e-01, -7.7026e-01, -1.7261e-01, -1.6139e-01,\n",
      "          -2.7660e-02,  5.8714e-01,  2.4183e-01, -1.0052e-01,  3.2042e-01,\n",
      "          -7.2294e-01,  8.8412e-01, -4.6320e-01, -7.9298e-01, -4.8466e-01,\n",
      "          -3.8320e-01,  9.3845e-02,  5.1228e-01, -1.3382e-01, -2.2215e-01,\n",
      "           1.4972e-01,  4.6214e-01, -9.0694e-01,  8.8510e-01, -2.6878e-01,\n",
      "          -4.5408e-02, -1.9584e-01, -6.5285e-01,  3.3121e-01,  8.4326e-02,\n",
      "          -1.8869e-01, -2.2867e-01,  6.6750e-02]],\n",
      "\n",
      "        [[-3.0145e-01,  1.2128e+00, -5.8783e-01, -7.3400e-02,  1.5639e-01,\n",
      "          -3.6946e-01, -3.7387e-01,  3.1109e-01, -5.8418e-01,  3.6350e-01,\n",
      "          -3.1158e-01, -8.1066e-01,  4.0748e-01, -7.4941e-01, -4.4277e-01,\n",
      "          -2.4464e-01,  7.4701e-01,  2.5406e-01, -3.6294e-01, -2.5771e-01,\n",
      "          -1.5722e-02,  4.0401e-01,  3.3361e-01, -5.5725e-01, -5.0010e-01,\n",
      "          -5.5749e-01,  4.8528e-01, -7.2863e-01, -1.0759e+00,  6.2714e-02,\n",
      "          -4.8022e-01,  1.1941e-01, -2.7291e-01,  1.8143e-01, -4.1216e-01,\n",
      "          -4.7654e-01, -5.2107e-01,  1.3131e-01, -9.9212e-02,  8.6114e-02,\n",
      "           4.6236e-01, -6.1744e-01,  2.4033e-01,  2.7128e-01, -6.9379e-01,\n",
      "          -2.2218e-01,  3.4389e-01,  3.0542e-01,  7.9934e-01, -3.2803e-01,\n",
      "          -1.8245e-01,  8.9687e-02,  4.4754e-02,  3.0260e-01, -9.7384e-02,\n",
      "           1.0580e-02, -2.2663e-01, -7.2564e-01,  2.5633e-01,  6.5158e-01,\n",
      "          -3.5036e-01, -1.1577e-01,  4.9593e-01,  5.4405e-01,  1.0492e-01,\n",
      "          -1.0173e-01,  4.7844e-02, -5.2570e-01,  2.8713e-01,  2.2643e-01,\n",
      "          -5.0500e-02, -2.6005e-01, -1.9331e-01,  8.3688e-01, -4.1930e-01,\n",
      "          -2.1945e-01,  3.5080e-01,  1.6228e-01, -3.9505e-02, -9.2350e-01,\n",
      "          -3.0042e-01, -2.5251e-01,  3.7842e-01,  8.0131e-01,  4.8102e-02,\n",
      "           1.0593e-03,  1.8110e-01, -3.9994e-01, -4.8170e-01,  4.2888e-01,\n",
      "          -1.2687e-01,  4.4821e-01,  1.7062e-01,  2.6582e-01,  1.0370e+00,\n",
      "          -2.0930e-01, -1.7391e-01, -1.1008e+00,  5.1088e-01, -9.4633e-02,\n",
      "          -3.4845e-01,  9.2454e-01, -2.4278e-02, -2.9059e-01,  4.0214e-01,\n",
      "          -3.6025e-01,  6.8308e-01, -3.9282e-01, -3.7315e-01, -8.2828e-01,\n",
      "          -6.9597e-01, -1.9885e-01,  4.5259e-01, -4.9184e-01, -1.5460e-01,\n",
      "           2.1226e-01,  6.4191e-01, -8.3055e-01,  4.7724e-01, -1.3977e-01,\n",
      "           1.5167e-01, -2.5021e-01, -7.8765e-01,  3.5873e-01,  3.5200e-01,\n",
      "          -6.1631e-01, -3.6139e-02,  1.6068e-01]],\n",
      "\n",
      "        [[ 2.9214e-01,  6.9829e-01, -3.9502e-02, -4.5924e-01, -1.1694e-01,\n",
      "          -2.2908e-01, -3.1818e-01,  3.6360e-01, -5.5510e-01,  5.1850e-01,\n",
      "          -4.1399e-01, -4.7733e-01, -2.4645e-01, -8.5140e-01, -3.9658e-01,\n",
      "          -2.3723e-02,  5.3815e-01,  8.5461e-02,  3.6991e-01, -3.5693e-01,\n",
      "           1.5301e-02,  2.9616e-01,  3.9425e-01, -4.2500e-01, -8.2947e-01,\n",
      "          -4.9657e-01,  5.8491e-01,  6.0296e-02, -5.3757e-01,  1.4397e-01,\n",
      "          -3.8375e-03,  4.7550e-01, -2.2886e-01, -3.9899e-01, -6.0143e-01,\n",
      "          -6.9338e-01, -3.7744e-01,  6.7887e-02,  1.5403e-01, -1.2319e-01,\n",
      "           4.5611e-01, -3.2047e-01,  3.2835e-01,  3.1037e-03, -4.2978e-01,\n",
      "           5.1272e-01,  1.5791e-01,  4.1624e-01,  6.9111e-01, -1.6871e-01,\n",
      "           3.1259e-01,  4.0824e-01,  1.4592e-01,  5.1898e-01,  3.6012e-01,\n",
      "          -1.1821e-01, -2.8253e-01, -4.1967e-01, -8.6267e-02,  8.9157e-01,\n",
      "          -4.4110e-01,  1.8100e-01,  5.7523e-01,  4.7006e-01,  1.4097e-01,\n",
      "           2.7810e-02,  4.1712e-01, -7.9277e-01, -1.9996e-02,  1.0621e-01,\n",
      "           5.5762e-01, -1.5820e-01, -2.8166e-01, -8.1018e-02, -1.1343e-01,\n",
      "           1.3709e-01,  5.0072e-01,  1.4722e-01, -1.1248e-01, -4.0215e-01,\n",
      "          -4.3050e-01,  1.0613e-01,  9.9247e-01,  3.9762e-01, -1.6543e-01,\n",
      "           1.2324e-01,  4.1365e-02, -7.9959e-01,  1.0824e-02,  1.8035e-02,\n",
      "           1.3135e-02, -3.7569e-01,  3.7620e-01,  6.1441e-01,  6.9011e-01,\n",
      "          -1.3780e-01, -8.1755e-03, -5.6743e-01,  7.9155e-02, -1.2038e-01,\n",
      "          -9.4458e-02,  6.0656e-01, -2.8366e-01, -5.3719e-01,  5.2714e-01,\n",
      "          -5.8861e-01,  1.0103e+00, -5.5070e-01, -5.3335e-01, -8.7460e-01,\n",
      "          -2.7819e-01,  3.3993e-01,  4.2290e-01, -2.3397e-01, -4.7859e-01,\n",
      "          -1.1073e-01,  6.2135e-01, -6.4506e-01,  1.9368e-01,  1.1637e-01,\n",
      "           1.7082e-01, -3.1880e-01, -5.2202e-01,  2.1024e-01, -2.0492e-02,\n",
      "          -4.3694e-01,  2.7604e-02,  3.2730e-01]],\n",
      "\n",
      "        [[ 1.4327e-01,  1.0168e+00, -2.5612e-01, -4.4827e-01,  2.7904e-01,\n",
      "           1.1868e-01, -8.2590e-01,  1.4630e-01, -4.7056e-01,  4.0383e-01,\n",
      "          -4.1254e-01, -2.6883e-01,  3.8294e-01, -7.6721e-01, -2.0387e-01,\n",
      "          -3.2107e-01,  6.4070e-01,  4.4151e-02, -1.2352e-01, -2.5804e-01,\n",
      "          -2.5115e-01,  3.2810e-01,  7.2882e-01, -7.3883e-01, -8.0491e-01,\n",
      "          -3.0556e-01,  9.2907e-01, -2.5946e-01, -1.0815e+00,  2.9930e-01,\n",
      "          -3.3887e-02,  3.5676e-01, -9.9156e-02, -1.9423e-01, -2.3287e-01,\n",
      "          -5.9146e-01, -4.1226e-01,  3.4700e-01, -2.7897e-01, -1.3247e-01,\n",
      "           5.7446e-01,  9.7352e-02,  5.7002e-01,  3.4983e-02, -4.6604e-01,\n",
      "           2.7785e-01,  3.5570e-01,  6.1325e-01,  1.1705e+00, -2.7350e-01,\n",
      "          -2.1791e-01, -5.0733e-02,  7.0436e-02,  5.8359e-01,  2.5548e-01,\n",
      "          -9.9703e-02, -2.2856e-01, -5.6838e-01,  5.5619e-02,  4.5013e-01,\n",
      "          -5.6197e-01,  3.4749e-01,  9.0820e-01,  5.4494e-01, -7.2412e-02,\n",
      "          -1.7239e-01,  2.7455e-01, -5.6902e-01,  6.6913e-01, -2.4513e-02,\n",
      "           1.6343e-01, -1.2934e-02, -3.6257e-01,  5.2896e-01, -6.7991e-01,\n",
      "          -3.1106e-02, -2.8071e-02,  2.9713e-01, -1.3805e-01, -5.7554e-01,\n",
      "          -4.0915e-01,  2.6344e-02,  9.2399e-01,  1.0083e+00, -2.5501e-01,\n",
      "           2.6017e-01, -3.9008e-01, -3.0544e-01,  1.0410e-01, -1.6375e-01,\n",
      "           2.3052e-01,  4.7950e-02,  1.9310e-01,  3.5481e-01,  5.8417e-01,\n",
      "          -4.6893e-01, -2.8990e-01, -6.1392e-01,  2.2286e-01, -2.7188e-01,\n",
      "          -6.0513e-01,  5.2571e-01,  3.4704e-01, -5.2966e-01,  6.5359e-01,\n",
      "          -4.0517e-01,  8.7942e-01, -5.6397e-01, -5.1174e-01, -7.0693e-01,\n",
      "          -6.9208e-01,  5.8267e-02,  2.1237e-01, -1.3311e-01, -1.3853e-01,\n",
      "           2.3597e-01,  3.9700e-01, -7.7815e-01,  3.4856e-01, -3.1218e-01,\n",
      "          -1.2100e-01, -4.0386e-01, -4.7610e-01,  7.1174e-01,  2.3539e-02,\n",
      "          -6.8776e-01, -3.4631e-01, -4.2879e-02]],\n",
      "\n",
      "        [[-1.7997e-01,  5.7638e-01, -3.5878e-01,  1.2841e-01,  2.7226e-02,\n",
      "          -1.5796e-01, -4.1955e-01,  3.1232e-01, -6.0751e-01,  6.4826e-01,\n",
      "          -8.6914e-01, -4.6300e-01,  3.4485e-01, -6.0750e-01, -2.6779e-01,\n",
      "          -3.6538e-01,  5.6193e-01, -1.7143e-01, -4.0846e-02, -6.2143e-01,\n",
      "          -8.2046e-03,  3.2210e-01,  2.4994e-01, -8.1227e-01, -7.8266e-01,\n",
      "          -4.3288e-01,  5.7604e-01, -2.4733e-01, -9.6491e-01,  2.5435e-01,\n",
      "          -4.4459e-01,  5.9454e-01, -3.4372e-01,  2.5952e-01, -7.3503e-01,\n",
      "          -2.0693e-01, -7.6123e-02,  1.9594e-01, -2.2986e-01,  9.2771e-02,\n",
      "           1.8907e-01, -2.8996e-01,  3.1492e-01,  9.5687e-02, -8.7412e-01,\n",
      "           2.5542e-02,  2.6744e-02,  5.1092e-01,  6.5325e-01, -1.8602e-01,\n",
      "           2.5653e-02, -2.9348e-01,  2.7888e-02,  5.4325e-02,  1.7669e-01,\n",
      "           1.9945e-01, -1.4932e-01, -3.9536e-01,  1.5379e-01,  3.8427e-01,\n",
      "          -3.9887e-01, -6.8105e-02,  4.4833e-01,  4.2444e-01,  1.0768e-01,\n",
      "           3.1200e-02,  8.3629e-01, -7.2620e-01,  1.8122e-01, -2.8369e-01,\n",
      "          -1.0093e-01,  9.5242e-02, -4.9073e-01,  2.5633e-01, -6.7569e-01,\n",
      "           1.4116e-01,  3.0635e-01, -3.9296e-01, -3.6601e-01, -1.1438e+00,\n",
      "          -3.8004e-01, -8.3817e-02,  7.5658e-01,  6.8269e-01,  3.6478e-02,\n",
      "           1.3292e-01, -1.7661e-01, -6.6548e-01, -5.4489e-01,  1.7011e-01,\n",
      "           1.6186e-01, -1.7915e-02,  4.5981e-01,  1.4086e-01,  6.0833e-01,\n",
      "           1.2480e-01, -1.8536e-01, -3.0660e-01,  8.0514e-02,  4.9358e-01,\n",
      "          -3.3161e-01,  8.4660e-01, -1.3138e-01, -1.8895e-01,  4.9391e-01,\n",
      "          -3.1156e-01,  9.1343e-01, -6.9707e-01, -4.7173e-01, -6.0590e-01,\n",
      "          -4.3842e-01,  2.4540e-01,  8.4324e-01,  2.1966e-01, -3.4404e-01,\n",
      "           9.8159e-02, -1.3783e-02, -1.0916e+00,  1.9539e-01, -1.1472e-02,\n",
      "           2.6615e-01, -1.6365e-01, -5.5992e-01,  3.2574e-01,  2.6395e-01,\n",
      "          -4.4238e-01, -3.6845e-01,  3.0315e-01]]], grad_fn=<SplitBackward>)\n",
      "v tensor([[[-8.7059e-01, -1.0160e+00, -7.3724e-01,  7.7488e-01,  3.5506e-01,\n",
      "           8.0809e-01,  3.8183e-02, -1.0932e-01, -2.1246e-01,  1.5134e-01,\n",
      "           9.5316e-02,  3.6980e-01, -4.9751e-01,  6.8986e-01,  2.6683e-01,\n",
      "          -5.7480e-01, -3.9345e-01, -1.2374e-01,  3.2646e-02,  9.5201e-01,\n",
      "          -2.4155e-01, -6.0346e-02, -2.1484e-01,  1.1084e-01,  4.4317e-01,\n",
      "          -4.4065e-01, -4.9080e-01, -1.4517e-02,  3.5586e-01,  7.7865e-01,\n",
      "           8.7274e-01,  3.3029e-01,  4.7654e-01,  5.0809e-01,  2.4131e-01,\n",
      "          -2.6514e-01,  4.8834e-02, -5.5936e-01,  1.1877e-01, -6.3218e-01,\n",
      "           5.4297e-01, -5.4360e-02,  5.5888e-01, -2.1048e-01,  7.8955e-02,\n",
      "          -1.6416e-01, -3.9945e-01, -3.1326e-01, -2.4208e-01,  1.5662e-01,\n",
      "          -1.8506e-01, -4.3614e-01, -2.4162e-01,  7.2759e-01,  5.4137e-02,\n",
      "           1.9103e-01,  4.1894e-01,  2.5387e-01,  3.9757e-01, -3.0240e-01,\n",
      "           1.4135e-01,  3.9673e-02, -1.4137e-01,  5.9165e-01,  7.9108e-02,\n",
      "           8.5844e-01,  6.3664e-01,  1.3530e-02, -1.7677e-02,  3.9549e-01,\n",
      "          -4.5267e-01,  1.6376e-01,  7.3792e-01,  5.4683e-01, -3.3487e-01,\n",
      "           1.6295e-01,  5.9553e-01,  9.1111e-01, -3.5474e-01, -7.7161e-02,\n",
      "          -2.3994e-01,  1.6126e-01,  4.6244e-01, -6.8232e-01,  1.5772e-01,\n",
      "           4.9153e-01, -2.6196e-01,  2.7325e-01, -6.3374e-02,  2.1672e-01,\n",
      "           3.2800e-01, -5.7455e-01,  7.3468e-01,  5.4771e-01,  5.6844e-01,\n",
      "          -1.1574e-01, -3.7357e-01, -6.2035e-03, -3.0113e-01, -8.0218e-03,\n",
      "           2.5693e-01, -1.4847e-01, -2.6783e-01,  4.1016e-01, -5.4267e-01,\n",
      "          -1.2697e-01, -4.2426e-01, -2.9627e-01, -4.6503e-02, -1.8863e-01,\n",
      "          -2.3156e-01, -5.5908e-02,  1.2598e+00,  6.8039e-02,  1.5760e-02,\n",
      "           8.4618e-01, -1.5711e-01,  1.2120e-01, -4.0849e-03,  1.6139e-01,\n",
      "           2.6382e-02,  1.5356e-01, -4.9075e-01,  1.6617e-01, -7.6623e-01,\n",
      "           2.4331e-01,  5.4800e-01, -4.0513e-02]],\n",
      "\n",
      "        [[-4.2956e-01, -1.1168e+00, -4.8237e-01,  5.2262e-01,  1.2663e-01,\n",
      "           5.7892e-01, -1.5123e-01, -5.8892e-01,  2.4537e-01, -3.2963e-01,\n",
      "           5.0836e-01,  1.1311e-01, -2.4015e-01,  1.4660e-01,  3.3866e-01,\n",
      "          -6.1277e-02, -3.7007e-01,  3.4899e-02, -2.4850e-01,  7.2601e-01,\n",
      "          -5.4130e-01,  1.2152e-01, -2.2756e-01, -2.0033e-02,  3.6613e-01,\n",
      "          -7.8797e-01, -5.4340e-01, -8.8701e-02,  6.1327e-01,  3.2872e-01,\n",
      "           9.1958e-01,  1.6894e-01,  1.6182e-01, -1.5546e-01,  5.2792e-02,\n",
      "           5.8134e-01, -1.6127e-01, -5.7288e-01,  5.1398e-03, -5.3156e-01,\n",
      "           3.0333e-01,  3.3774e-01,  5.7436e-01, -4.6841e-01,  2.8648e-01,\n",
      "          -2.4381e-01, -2.3620e-01, -5.4474e-01,  6.4160e-02,  2.5963e-01,\n",
      "          -4.0497e-01, -6.3987e-01, -2.9772e-01,  6.9569e-01, -7.6499e-01,\n",
      "          -1.1838e-01,  3.6837e-01,  4.0462e-02,  6.9691e-03, -3.5082e-01,\n",
      "           7.4109e-01, -2.3911e-01, -4.9019e-02,  5.0939e-01, -2.4748e-02,\n",
      "           6.4326e-01,  3.0790e-01,  3.6629e-01,  1.3543e-01,  7.8001e-01,\n",
      "          -4.0083e-01, -4.0414e-02,  5.5790e-01,  7.4376e-01,  4.2095e-02,\n",
      "           3.5988e-01,  1.2050e-01,  8.2004e-01,  1.0303e-01, -4.1435e-01,\n",
      "          -2.7140e-01,  7.4456e-01,  8.6264e-01, -2.7161e-01, -2.1854e-01,\n",
      "           6.9019e-01, -1.7433e-01,  1.0034e-01, -1.4106e-01,  6.5784e-01,\n",
      "           8.7320e-02, -1.0656e+00,  6.5808e-01,  7.2952e-01,  1.2888e+00,\n",
      "           6.6583e-02, -4.6827e-02, -1.0372e-02,  3.6185e-03, -9.9027e-02,\n",
      "           1.6603e-01,  1.7536e-02, -2.5902e-01,  2.9446e-01, -4.8154e-01,\n",
      "          -1.6831e-01, -6.0672e-01, -1.2640e-01, -5.2853e-01, -2.8482e-01,\n",
      "          -2.3447e-01,  1.2856e-01,  8.3487e-01,  1.2968e-01, -6.5526e-01,\n",
      "           7.3048e-01,  1.4524e-01, -7.3268e-02,  1.5831e-01,  3.2931e-01,\n",
      "          -2.6212e-01,  1.8039e-01, -3.4285e-01,  3.4436e-01, -8.3025e-01,\n",
      "           3.7717e-01,  1.7382e-01, -2.1360e-01]],\n",
      "\n",
      "        [[-2.8173e-01, -1.1791e+00, -9.9175e-01,  2.6337e-01,  7.1455e-01,\n",
      "           2.6462e-01,  9.5148e-03, -7.0551e-02, -1.8417e-02,  1.8571e-01,\n",
      "          -2.7351e-02,  1.0497e-01, -1.8989e-01,  2.8558e-01, -2.1034e-01,\n",
      "           1.4406e-01, -4.6574e-01, -1.4598e-01, -2.3927e-01,  4.9654e-01,\n",
      "          -2.8945e-01, -3.8241e-01, -1.4902e-01, -1.8264e-02,  8.7106e-02,\n",
      "          -5.9283e-01, -4.0189e-01, -1.6698e-01,  5.8058e-01,  8.3460e-01,\n",
      "           4.3059e-01,  7.1298e-02,  5.6421e-01, -5.9626e-01,  3.0992e-01,\n",
      "          -2.9464e-01,  4.3552e-01, -6.5084e-01, -1.9521e-01, -3.9222e-01,\n",
      "           7.9973e-01,  1.2732e-01,  5.6946e-01, -4.5306e-01,  3.2281e-02,\n",
      "           1.4058e-01, -6.0313e-02, -4.8653e-01, -1.7478e-01,  6.6041e-01,\n",
      "          -5.9725e-01, -7.0087e-01, -1.0651e-01,  5.9545e-01,  1.7070e-01,\n",
      "           9.5503e-02,  1.5292e-02,  1.6976e-01, -6.5946e-02, -1.2406e-01,\n",
      "           3.3568e-01,  3.3663e-01, -1.8582e-01, -7.8715e-02,  1.6178e-01,\n",
      "           5.5033e-01,  6.5401e-01,  3.6176e-02, -2.8771e-02,  5.3284e-01,\n",
      "          -8.7450e-01,  1.8008e-01,  8.0371e-01,  4.9263e-01, -3.4525e-01,\n",
      "           3.9276e-02,  3.8581e-01,  5.1098e-01, -3.2230e-01,  1.6471e-01,\n",
      "           1.1128e-01,  2.3488e-01,  4.0235e-01, -3.6809e-01,  2.0139e-01,\n",
      "           5.1834e-01, -2.2665e-01,  2.7545e-01, -6.1650e-02,  1.9554e-01,\n",
      "          -1.0771e-01, -3.0553e-01,  4.2552e-01,  3.7405e-01,  3.6759e-01,\n",
      "          -1.5933e-01, -2.5837e-01, -7.3323e-03,  2.0108e-01, -5.1682e-01,\n",
      "           1.1854e-01, -1.7817e-01, -2.1386e-02,  1.2649e-01, -4.3364e-02,\n",
      "          -2.7213e-01, -2.2854e-01, -1.1238e-01, -4.2283e-01,  4.5439e-01,\n",
      "          -4.3620e-01, -4.0767e-02,  8.1738e-01,  4.6026e-01, -2.5017e-01,\n",
      "          -7.9839e-02, -7.8683e-02,  6.0736e-02, -4.4950e-01,  7.1773e-02,\n",
      "           8.1109e-03,  2.0458e-01,  3.7903e-02,  4.2771e-01, -6.1493e-01,\n",
      "           2.8662e-01,  4.5861e-01,  4.9208e-03]],\n",
      "\n",
      "        [[-2.7247e-01, -1.0580e+00, -5.6648e-01,  5.4954e-01,  6.6335e-02,\n",
      "           7.1621e-02, -5.4880e-02, -2.4503e-01,  1.5975e-01, -5.6350e-02,\n",
      "          -1.3801e-02,  1.3590e-01, -1.7588e-01,  6.3692e-01,  9.0252e-02,\n",
      "          -2.0501e-01, -4.2475e-01, -2.2634e-01, -9.8518e-02,  7.3999e-01,\n",
      "          -5.1463e-01, -9.6316e-02, -4.9500e-02,  3.7695e-02, -1.2207e-01,\n",
      "          -2.6357e-01, -5.8523e-01,  7.3787e-02,  4.4808e-01,  8.4157e-01,\n",
      "           7.6547e-01, -1.4603e-01,  5.9547e-01,  3.4836e-01,  5.8797e-01,\n",
      "           9.4837e-03,  8.4124e-02, -1.0353e+00,  1.5179e-01, -2.5131e-01,\n",
      "           3.8661e-01, -2.3449e-01,  7.0083e-01, -5.9670e-03, -3.1736e-01,\n",
      "          -4.3661e-03, -7.4697e-01, -5.0402e-01, -2.2593e-01,  2.5351e-01,\n",
      "          -4.3846e-01, -3.3590e-01, -1.4280e-01,  2.1424e-01,  5.8005e-02,\n",
      "          -4.5936e-01,  3.8427e-01, -1.8777e-01,  1.1587e-01, -6.5518e-02,\n",
      "           8.3211e-01,  6.4545e-02, -7.7551e-02,  2.2588e-01, -9.3450e-02,\n",
      "           6.2568e-01,  6.0826e-01,  2.0464e-01,  3.3445e-01,  4.0047e-01,\n",
      "          -4.1957e-01,  2.6998e-01,  5.1018e-01,  1.8591e-01, -7.8603e-02,\n",
      "           1.2218e-01,  1.9029e-01,  7.0806e-01, -4.5371e-01, -2.7466e-01,\n",
      "           1.6571e-01,  2.9835e-01,  7.8046e-01,  1.7270e-02,  9.8210e-02,\n",
      "           8.6127e-01, -3.6576e-01,  3.5109e-01, -5.4801e-01,  3.3411e-01,\n",
      "          -1.4460e-01, -3.7302e-01,  1.8493e-01,  1.1985e-01,  3.3503e-01,\n",
      "          -1.6101e-01, -1.4442e-01,  4.1345e-02,  1.6648e-01, -1.8605e-01,\n",
      "          -2.0539e-01,  3.7135e-01, -9.6634e-02,  1.7282e-01, -2.2912e-01,\n",
      "          -1.6026e-01, -1.8331e-01, -3.4427e-01, -3.5960e-01,  1.6112e-02,\n",
      "          -4.4511e-01, -2.8517e-01,  6.6230e-01,  5.3126e-01, -4.6574e-01,\n",
      "           1.4735e-01,  1.3301e-01,  5.8454e-01, -3.0288e-01, -6.1875e-04,\n",
      "           7.9440e-02, -1.0111e-01, -1.8477e-01,  5.9700e-01, -8.8476e-01,\n",
      "           2.2192e-01,  5.1405e-01, -2.4621e-01]],\n",
      "\n",
      "        [[-1.5562e-01, -9.9246e-01, -9.5456e-01,  8.3153e-01,  9.7681e-02,\n",
      "           2.0751e-01, -1.3957e-02, -1.9082e-01, -1.3767e-01, -1.2819e-01,\n",
      "           1.8970e-01, -1.1686e-02, -4.8071e-01,  2.0833e-01,  9.2116e-02,\n",
      "          -2.0117e-01, -1.9867e-01,  4.1834e-01, -2.4805e-01,  4.4472e-01,\n",
      "          -3.1561e-01,  5.6083e-01, -5.0152e-01,  2.3208e-01,  3.3377e-01,\n",
      "          -6.1821e-01, -4.9448e-01, -2.3954e-01,  3.3142e-01,  4.7622e-01,\n",
      "           4.3579e-01, -2.3075e-02,  2.7664e-01,  3.2090e-02,  1.9975e-02,\n",
      "          -4.3550e-02,  8.2443e-02, -5.7884e-01, -1.1146e-01, -7.4404e-01,\n",
      "           4.3092e-01, -1.4600e-01,  4.5289e-01, -5.8187e-01, -1.5891e-01,\n",
      "           3.9764e-02, -6.5492e-01, -2.6411e-01, -1.5778e-01,  7.5663e-03,\n",
      "          -5.2907e-01, -3.7719e-01, -1.3771e-01,  3.8900e-01, -1.3094e-01,\n",
      "          -3.9425e-01,  2.3226e-01, -2.4763e-01, -1.4455e-02, -1.5935e-01,\n",
      "           5.2204e-01,  8.0465e-02,  1.4072e-01,  3.4509e-01,  1.7145e-02,\n",
      "           9.4994e-01,  5.0208e-01,  3.8092e-01, -2.2077e-01,  3.6037e-01,\n",
      "          -3.4787e-01,  2.4663e-01,  7.6053e-01,  6.8141e-01, -6.1234e-01,\n",
      "          -1.5905e-01,  2.1909e-01,  6.8610e-01, -9.9593e-02,  1.5786e-03,\n",
      "           4.1151e-01,  1.7765e-02,  3.2267e-01,  2.6146e-03, -1.5934e-01,\n",
      "           5.1331e-01, -9.5543e-01,  5.8566e-01, -1.1884e-01,  1.4946e-01,\n",
      "           3.8358e-01, -7.3217e-01,  5.9568e-01,  3.7866e-01,  5.7463e-01,\n",
      "          -6.6279e-02, -1.2034e-01,  2.2761e-01,  2.5233e-01, -1.6402e-02,\n",
      "          -2.1819e-01, -1.2705e-01,  6.5389e-03, -9.0403e-02, -1.9412e-01,\n",
      "          -1.5113e-01, -3.6454e-01, -2.4487e-01, -8.7430e-02, -2.4205e-01,\n",
      "          -4.8108e-01, -2.0907e-01,  1.1112e+00,  5.1151e-01, -2.9741e-01,\n",
      "           4.3473e-01,  1.2834e-02,  2.2486e-01, -5.3859e-01,  2.6406e-01,\n",
      "          -4.6391e-01,  8.9658e-02, -4.1882e-01,  7.5907e-01, -6.8012e-01,\n",
      "           4.3004e-01,  3.5246e-01, -4.3657e-02]]], grad_fn=<SplitBackward>)\n",
      "presoftmax tensor([[[-0.0450, -0.0939, -0.0818, -0.0534,  0.0893],\n",
      "         [-0.0941, -0.1006, -0.0708, -0.0328,  0.0161],\n",
      "         [ 0.0408, -0.0042,  0.0339,  0.0372,  0.1332],\n",
      "         [ 0.0053, -0.0308,  0.0144,  0.0188,  0.1109],\n",
      "         [-0.0875, -0.2734, -0.1332, -0.1486, -0.0562]]],\n",
      "       grad_fn=<BmmBackward0>)\n",
      "post softmax tensor([[[0.1980, 0.1885, 0.1908, 0.1963, 0.2264],\n",
      "         [0.1924, 0.1912, 0.1970, 0.2046, 0.2148],\n",
      "         [0.1983, 0.1896, 0.1970, 0.1976, 0.2175],\n",
      "         [0.1961, 0.1892, 0.1979, 0.1988, 0.2180],\n",
      "         [0.2102, 0.1745, 0.2008, 0.1977, 0.2168]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[-3.9580e-01, -1.0690e+00, -7.5344e-01,  5.9832e-01,  2.6564e-01,\n",
      "           3.8064e-01, -3.3068e-02, -2.3743e-01,  8.7301e-04, -3.6832e-02,\n",
      "           1.4973e-01,  1.3859e-01, -3.2336e-01,  3.9089e-01,  1.1510e-01,\n",
      "          -1.8364e-01, -3.6488e-01,  4.5151e-03, -1.6154e-01,  6.6602e-01,\n",
      "          -3.7758e-01,  4.6067e-02, -2.3713e-01,  7.4624e-02,  2.2498e-01,\n",
      "          -5.4061e-01, -5.0312e-01, -9.1207e-02,  4.5983e-01,  6.4838e-01,\n",
      "           6.7721e-01,  7.6944e-02,  4.1202e-01,  3.3147e-02,  2.3680e-01,\n",
      "          -7.1151e-03,  9.7548e-02, -6.7721e-01, -8.2068e-03, -5.1798e-01,\n",
      "           4.9073e-01, -1.8850e-03,  5.6768e-01, -3.4933e-01, -2.2482e-02,\n",
      "          -4.3487e-02, -4.3002e-01, -4.1628e-01, -1.4925e-01,  2.5744e-01,\n",
      "          -4.3280e-01, -4.9204e-01, -1.8349e-01,  5.1893e-01, -1.1918e-01,\n",
      "          -1.4572e-01,  2.8331e-01, -2.6538e-03,  8.6903e-02, -1.9861e-01,\n",
      "           5.1328e-01,  5.7899e-02, -5.6045e-02,  3.2060e-01,  2.7401e-02,\n",
      "           7.3411e-01,  5.4194e-01,  2.0505e-01,  3.2209e-02,  4.8721e-01,\n",
      "          -4.9316e-01,  1.6800e-01,  6.7695e-01,  5.3324e-01, -2.7830e-01,\n",
      "           9.5567e-02,  3.0118e-01,  7.2679e-01, -2.2391e-01, -1.1552e-01,\n",
      "           4.8275e-02,  2.7969e-01,  5.5720e-01, -2.5253e-01,  1.1653e-02,\n",
      "           6.1161e-01, -4.1609e-01,  3.2709e-01, -1.8539e-01,  3.0365e-01,\n",
      "           1.1930e-01, -6.1192e-01,  5.2186e-01,  4.2658e-01,  6.2149e-01,\n",
      "          -8.7374e-02, -1.8767e-01,  5.5069e-02,  6.9252e-02, -1.5911e-01,\n",
      "           1.5057e-02, -1.5948e-02, -1.2342e-01,  1.7429e-01, -2.9540e-01,\n",
      "          -1.7447e-01, -3.6049e-01, -2.2695e-01, -2.7991e-01, -5.5971e-02,\n",
      "          -3.6957e-01, -9.7926e-02,  9.4434e-01,  3.4584e-01, -3.2691e-01,\n",
      "           4.1733e-01,  1.0281e-02,  1.8743e-01, -2.3814e-01,  1.6739e-01,\n",
      "          -1.3209e-01,  1.0389e-01, -2.8564e-01,  4.6848e-01, -7.5320e-01,\n",
      "           3.1489e-01,  4.0947e-01, -1.0556e-01],\n",
      "         [-3.9431e-01, -1.0709e+00, -7.5040e-01,  5.9197e-01,  2.6783e-01,\n",
      "           3.7752e-01, -3.3917e-02, -2.3865e-01,  5.5060e-03, -3.6389e-02,\n",
      "           1.4807e-01,  1.3875e-01, -3.1831e-01,  3.9208e-01,  1.1291e-01,\n",
      "          -1.7911e-01, -3.6778e-01, -2.3207e-03, -1.6180e-01,  6.6673e-01,\n",
      "          -3.8008e-01,  3.7087e-02, -2.3207e-01,  7.1473e-02,  2.1916e-01,\n",
      "          -5.3894e-01, -5.0346e-01, -8.9008e-02,  4.6295e-01,  6.5155e-01,\n",
      "           6.7878e-01,  7.5059e-02,  4.1502e-01,  2.8770e-02,  2.4216e-01,\n",
      "          -5.3303e-03,  9.9271e-02, -6.8153e-01, -7.5035e-03, -5.1179e-01,\n",
      "           4.9167e-01, -1.5767e-04,  5.7019e-01, -3.4552e-01, -2.2752e-02,\n",
      "          -4.2859e-02, -4.2743e-01, -4.2011e-01, -1.4886e-01,  2.6334e-01,\n",
      "          -4.3404e-01, -4.9406e-01, -1.8319e-01,  5.1769e-01, -1.1847e-01,\n",
      "          -1.4575e-01,  2.8257e-01, -1.5999e-03,  8.5442e-02, -1.9733e-01,\n",
      "           5.1740e-01,  5.8718e-02, -5.8806e-02,  3.1608e-01,  2.6920e-02,\n",
      "           7.2865e-01,  5.4250e-01,  2.0346e-01,  3.7817e-02,  4.8953e-01,\n",
      "          -4.9656e-01,  1.6748e-01,  6.7472e-01,  5.2888e-01, -2.7203e-01,\n",
      "           9.8716e-02,  2.9962e-01,  7.2500e-01, -2.2627e-01, -1.1747e-01,\n",
      "           4.6180e-02,  2.8449e-01,  5.6215e-01, -2.5162e-01,  1.4094e-02,\n",
      "           6.1512e-01, -4.0848e-01,  3.2367e-01, -1.8896e-01,  3.0645e-01,\n",
      "           1.1142e-01, -6.0808e-01,  5.1680e-01,  4.2440e-01,  6.2017e-01,\n",
      "          -8.8105e-02, -1.8712e-01,  5.2740e-02,  7.0627e-02, -1.6386e-01,\n",
      "           1.5627e-02, -1.1627e-02, -1.2363e-01,  1.7606e-01, -2.9360e-01,\n",
      "          -1.7547e-01, -3.5846e-01, -2.2636e-01, -2.8563e-01, -4.9956e-02,\n",
      "          -3.6972e-01, -9.7472e-02,  9.3725e-01,  3.4713e-01, -3.3070e-01,\n",
      "           4.1029e-01,  1.2008e-02,  1.8918e-01, -2.3674e-01,  1.6475e-01,\n",
      "          -1.2685e-01,  1.0290e-01, -2.8029e-01,  4.6728e-01, -7.5442e-01,\n",
      "           3.1317e-01,  4.0990e-01, -1.0741e-01],\n",
      "         [-3.9728e-01, -1.0704e+00, -7.5257e-01,  5.9411e-01,  2.6951e-01,\n",
      "           3.8143e-01, -3.3107e-02, -2.3716e-01,  2.3801e-03, -3.4928e-02,\n",
      "           1.4844e-01,  1.3977e-01, -3.2092e-01,  3.9203e-01,  1.1357e-01,\n",
      "          -1.8151e-01, -3.6707e-01, -4.0074e-04, -1.6119e-01,  6.6721e-01,\n",
      "          -3.7789e-01,  3.8720e-02, -2.3397e-01,  7.2518e-02,  2.2294e-01,\n",
      "          -5.4011e-01, -5.0272e-01, -9.0109e-02,  4.6183e-01,  6.5101e-01,\n",
      "           6.7829e-01,  7.7698e-02,  4.1415e-01,  2.9672e-02,  2.3944e-01,\n",
      "          -7.9923e-03,  9.9442e-02, -6.7824e-01, -8.1680e-03, -5.1491e-01,\n",
      "           4.9283e-01,  2.3291e-04,  5.6889e-01, -3.4753e-01, -2.0949e-02,\n",
      "          -4.3306e-02, -4.2595e-01, -4.1828e-01, -1.4923e-01,  2.6209e-01,\n",
      "          -4.3284e-01, -4.9427e-01, -1.8351e-01,  5.2042e-01, -1.1770e-01,\n",
      "          -1.4229e-01,  2.8239e-01,  4.7893e-04,  8.6929e-02, -1.9853e-01,\n",
      "           5.1264e-01,  5.9091e-02, -5.8641e-02,  3.1811e-01,  2.8122e-02,\n",
      "           7.3087e-01,  5.4285e-01,  2.0255e-01,  3.4570e-02,  4.8879e-01,\n",
      "          -4.9658e-01,  1.6728e-01,  6.7666e-01,  5.3145e-01, -2.7516e-01,\n",
      "           9.7828e-02,  3.0220e-01,  7.2597e-01, -2.2562e-01, -1.1535e-01,\n",
      "           4.5138e-02,  2.8223e-01,  5.5892e-01, -2.5533e-01,  1.4254e-02,\n",
      "           6.1228e-01, -4.0975e-01,  3.2424e-01, -1.8560e-01,  3.0475e-01,\n",
      "           1.1525e-01, -6.0914e-01,  5.2039e-01,  4.2665e-01,  6.2068e-01,\n",
      "          -8.7943e-02, -1.8856e-01,  5.3042e-02,  6.8356e-02, -1.6249e-01,\n",
      "           1.7728e-02, -1.5463e-02, -1.2411e-01,  1.7657e-01, -2.9496e-01,\n",
      "          -1.7523e-01, -3.5970e-01, -2.2615e-01, -2.8279e-01, -5.1386e-02,\n",
      "          -3.6890e-01, -9.6573e-02,  9.4170e-01,  3.4498e-01, -3.2711e-01,\n",
      "           4.1427e-01,  9.9576e-03,  1.8653e-01, -2.3634e-01,  1.6590e-01,\n",
      "          -1.2808e-01,  1.0447e-01, -2.8248e-01,  4.6558e-01, -7.5326e-01,\n",
      "           3.1361e-01,  4.1021e-01, -1.0571e-01],\n",
      "         [-3.9586e-01, -1.0706e+00, -7.5282e-01,  5.9347e-01,  2.6950e-01,\n",
      "           3.7986e-01, -3.3190e-02, -2.3712e-01,  2.8528e-03, -3.5064e-02,\n",
      "           1.4806e-01,  1.3917e-01, -3.2034e-01,  3.9158e-01,  1.1279e-01,\n",
      "          -1.8042e-01, -3.6710e-01, -3.6983e-04, -1.6162e-01,  6.6638e-01,\n",
      "          -3.7817e-01,  3.8563e-02, -2.3384e-01,  7.2413e-02,  2.2191e-01,\n",
      "          -5.3998e-01, -5.0273e-01, -9.0222e-02,  4.6204e-01,  6.5119e-01,\n",
      "           6.7752e-01,  7.6793e-02,  4.1442e-01,  2.8472e-02,  2.3989e-01,\n",
      "          -7.9485e-03,  9.9962e-02, -6.7889e-01, -8.4896e-03, -5.1432e-01,\n",
      "           4.9295e-01, -5.4900e-06,  5.6902e-01, -3.4758e-01, -2.1653e-02,\n",
      "          -4.2698e-02, -4.2621e-01, -4.1856e-01, -1.4924e-01,  2.6259e-01,\n",
      "          -4.3360e-01, -4.9430e-01, -1.8320e-01,  5.1955e-01, -1.1733e-01,\n",
      "          -1.4328e-01,  2.8190e-01, -2.5817e-04,  8.6124e-02, -1.9799e-01,\n",
      "           5.1357e-01,  5.9543e-02, -5.8523e-02,  3.1695e-01,  2.8014e-02,\n",
      "           7.3042e-01,  5.4291e-01,  2.0282e-01,  3.4823e-02,  4.8876e-01,\n",
      "          -4.9693e-01,  1.6754e-01,  6.7654e-01,  5.3095e-01, -2.7514e-01,\n",
      "           9.7436e-02,  3.0154e-01,  7.2528e-01, -2.2578e-01, -1.1518e-01,\n",
      "           4.6261e-02,  2.8216e-01,  5.5901e-01, -2.5406e-01,  1.4242e-02,\n",
      "           6.1267e-01, -4.1018e-01,  3.2454e-01, -1.8616e-01,  3.0466e-01,\n",
      "           1.1439e-01, -6.0850e-01,  5.1941e-01,  4.2583e-01,  6.1991e-01,\n",
      "          -8.8093e-02, -1.8820e-01,  5.3202e-02,  6.9517e-02, -1.6316e-01,\n",
      "           1.6875e-02, -1.4936e-02, -1.2355e-01,  1.7584e-01, -2.9398e-01,\n",
      "          -1.7541e-01, -3.5912e-01, -2.2608e-01, -2.8334e-01, -5.0501e-02,\n",
      "          -3.6946e-01, -9.6973e-02,  9.4068e-01,  3.4608e-01, -3.2780e-01,\n",
      "           4.1240e-01,  1.0327e-02,  1.8715e-01, -2.3743e-01,  1.6559e-01,\n",
      "          -1.2814e-01,  1.0418e-01, -2.8163e-01,  4.6653e-01, -7.5319e-01,\n",
      "           3.1365e-01,  4.1015e-01, -1.0584e-01],\n",
      "         [-4.0211e-01, -1.0695e+00, -7.5723e-01,  5.9590e-01,  2.7448e-01,\n",
      "           3.8315e-01, -3.0332e-02, -2.2973e-01, -3.8013e-03, -2.7369e-02,\n",
      "           1.4166e-01,  1.4287e-01, -3.2361e-01,  3.9900e-01,  1.1077e-01,\n",
      "          -1.8673e-01, -3.6784e-01, -3.2598e-03, -1.5781e-01,  6.6921e-01,\n",
      "          -3.7353e-01,  3.4318e-02, -2.3332e-01,  7.3909e-02,  2.2276e-01,\n",
      "          -5.3531e-01, -5.0160e-01, -8.9409e-02,  4.5883e-01,  6.5823e-01,\n",
      "           6.7618e-01,  7.9337e-02,  4.1939e-01,  3.5776e-02,  2.4273e-01,\n",
      "          -2.1002e-02,  1.0407e-01, -6.7842e-01, -7.4922e-03, -5.1540e-01,\n",
      "           4.9749e-01, -4.9453e-03,  5.6878e-01, -3.4429e-01, -2.4135e-02,\n",
      "          -4.1064e-02, -4.2698e-01, -4.1550e-01, -1.5365e-01,  2.6258e-01,\n",
      "          -4.3089e-01, -4.9224e-01, -1.8221e-01,  5.2058e-01, -1.0477e-01,\n",
      "          -1.3765e-01,  2.8174e-01,  3.6762e-03,  9.1305e-02, -1.9719e-01,\n",
      "           5.0414e-01,  6.4406e-02, -6.0391e-02,  3.1692e-01,  3.0030e-02,\n",
      "           7.3285e-01,  5.4797e-01,  1.9709e-01,  3.2391e-02,  4.8354e-01,\n",
      "          -4.9905e-01,  1.7038e-01,  6.7959e-01,  5.2814e-01, -2.8067e-01,\n",
      "           9.4600e-02,  3.0878e-01,  7.2595e-01, -2.3258e-01, -1.0941e-01,\n",
      "           4.6550e-02,  2.7382e-01,  5.5278e-01, -2.6072e-01,  2.0309e-02,\n",
      "           6.0941e-01, -4.1048e-01,  3.2665e-01, -1.8443e-01,  2.9807e-01,\n",
      "           1.1714e-01, -6.0057e-01,  5.2041e-01,  4.2332e-01,  6.0901e-01,\n",
      "          -9.0899e-02, -1.9321e-01,  5.2945e-02,  6.5345e-02, -1.6307e-01,\n",
      "           1.8849e-02, -1.8044e-02, -1.2347e-01,  1.7755e-01, -2.9418e-01,\n",
      "          -1.7515e-01, -3.5622e-01, -2.2805e-01, -2.7695e-01, -4.7421e-02,\n",
      "          -3.6948e-01, -9.9217e-02,  9.4647e-01,  3.4529e-01, -3.1783e-01,\n",
      "           4.1269e-01,  5.6077e-03,  1.8921e-01, -2.4015e-01,  1.6293e-01,\n",
      "          -1.2346e-01,  1.0428e-01, -2.8271e-01,  4.6352e-01, -7.5179e-01,\n",
      "           3.1163e-01,  4.1564e-01, -1.0295e-01]]], grad_fn=<BmmBackward0>)\n",
      "postlinear tensor([[[-0.2186, -0.3842,  0.1497, -0.0263,  0.2757,  0.1261,  0.3305,\n",
      "          -0.1523, -0.2219,  0.1608, -0.1299,  0.0411, -0.0869, -0.1442,\n",
      "          -0.1239,  0.0169,  0.2050,  0.2134,  0.3478,  0.1704,  0.0294,\n",
      "           0.3097,  0.1604,  0.0194, -0.1723, -0.4511, -0.1123, -0.3128,\n",
      "           0.1281, -0.0255, -0.0620, -0.0702, -0.0028,  0.1525, -0.1405,\n",
      "          -0.1809, -0.0257, -0.2073, -0.0193,  0.0517,  0.1479,  0.0442,\n",
      "          -0.0223, -0.3561, -0.1487, -0.0483,  0.1664, -0.0244, -0.1497,\n",
      "           0.1999,  0.2401,  0.0254, -0.0240,  0.2015,  0.3916, -0.3342,\n",
      "           0.1651, -0.0227, -0.2236,  0.1835,  0.4124, -0.3942, -0.0274,\n",
      "          -0.1710, -0.2303,  0.0193, -0.0628,  0.1685,  0.2286,  0.1842,\n",
      "          -0.4431,  0.2175,  0.1231, -0.3336, -0.0163, -0.2438,  0.1562,\n",
      "          -0.2061, -0.1103,  0.2375, -0.1800, -0.3421,  0.0936,  0.0660,\n",
      "           0.0680,  0.2059,  0.1330,  0.1728, -0.0286, -0.3282,  0.2460,\n",
      "          -0.0512, -0.6025,  0.0551, -0.1901, -0.0535,  0.0051,  0.2428,\n",
      "          -0.1554, -0.4326, -0.1242, -0.3151, -0.1999, -0.3309, -0.3721,\n",
      "           0.1106, -0.2754,  0.2321,  0.0183, -0.0854, -0.0852, -0.0384,\n",
      "          -0.0311,  0.1796, -0.1434, -0.3068, -0.1373,  0.2470,  0.0550,\n",
      "           0.0781, -0.0398, -0.2515, -0.2239,  0.1369, -0.2193, -0.3131,\n",
      "          -0.0044,  0.0349]],\n",
      "\n",
      "        [[-0.2206, -0.3831,  0.1537, -0.0255,  0.2772,  0.1257,  0.3319,\n",
      "          -0.1527, -0.2217,  0.1606, -0.1294,  0.0423, -0.0874, -0.1498,\n",
      "          -0.1247,  0.0157,  0.2054,  0.2117,  0.3487,  0.1719,  0.0302,\n",
      "           0.3123,  0.1587,  0.0206, -0.1731, -0.4513, -0.1112, -0.3171,\n",
      "           0.1266, -0.0196, -0.0641, -0.0708, -0.0026,  0.1545, -0.1365,\n",
      "          -0.1792, -0.0243, -0.2079, -0.0196,  0.0540,  0.1442,  0.0509,\n",
      "          -0.0202, -0.3546, -0.1491, -0.0469,  0.1656, -0.0269, -0.1479,\n",
      "           0.1997,  0.2390,  0.0259, -0.0226,  0.1993,  0.3925, -0.3361,\n",
      "           0.1642, -0.0243, -0.2206,  0.1856,  0.4106, -0.3949, -0.0262,\n",
      "          -0.1711, -0.2285,  0.0226, -0.0631,  0.1668,  0.2300,  0.1831,\n",
      "          -0.4460,  0.2172,  0.1221, -0.3354, -0.0176, -0.2426,  0.1559,\n",
      "          -0.2075, -0.1109,  0.2368, -0.1795, -0.3435,  0.0962,  0.0647,\n",
      "           0.0666,  0.2033,  0.1379,  0.1702, -0.0321, -0.3303,  0.2456,\n",
      "          -0.0513, -0.6025,  0.0520, -0.1900, -0.0553,  0.0061,  0.2437,\n",
      "          -0.1586, -0.4326, -0.1213, -0.3145, -0.1976, -0.3340, -0.3740,\n",
      "           0.1113, -0.2750,  0.2343,  0.0177, -0.0833, -0.0822, -0.0402,\n",
      "          -0.0335,  0.1793, -0.1427, -0.3059, -0.1379,  0.2489,  0.0543,\n",
      "           0.0800, -0.0395, -0.2483, -0.2246,  0.1370, -0.2197, -0.3140,\n",
      "          -0.0057,  0.0334]],\n",
      "\n",
      "        [[-0.2191, -0.3837,  0.1516, -0.0255,  0.2774,  0.1262,  0.3315,\n",
      "          -0.1523, -0.2226,  0.1603, -0.1300,  0.0413, -0.0860, -0.1476,\n",
      "          -0.1246,  0.0167,  0.2056,  0.2118,  0.3496,  0.1715,  0.0293,\n",
      "           0.3120,  0.1589,  0.0204, -0.1721, -0.4514, -0.1111, -0.3143,\n",
      "           0.1272, -0.0231, -0.0633, -0.0704, -0.0025,  0.1540, -0.1384,\n",
      "          -0.1804, -0.0240, -0.2077, -0.0200,  0.0532,  0.1455,  0.0481,\n",
      "          -0.0216, -0.3558, -0.1496, -0.0468,  0.1657, -0.0253, -0.1484,\n",
      "           0.1997,  0.2395,  0.0257, -0.0236,  0.1992,  0.3925, -0.3351,\n",
      "           0.1644, -0.0232, -0.2232,  0.1841,  0.4119, -0.3950, -0.0267,\n",
      "          -0.1711, -0.2299,  0.0195, -0.0629,  0.1666,  0.2293,  0.1837,\n",
      "          -0.4434,  0.2168,  0.1226, -0.3353, -0.0173, -0.2426,  0.1555,\n",
      "          -0.2071, -0.1115,  0.2381, -0.1799, -0.3430,  0.0953,  0.0658,\n",
      "           0.0667,  0.2036,  0.1356,  0.1712, -0.0302, -0.3297,  0.2449,\n",
      "          -0.0510, -0.6025,  0.0536, -0.1904, -0.0556,  0.0061,  0.2434,\n",
      "          -0.1561, -0.4322, -0.1212, -0.3151, -0.1996, -0.3334, -0.3731,\n",
      "           0.1111, -0.2736,  0.2349,  0.0177, -0.0835, -0.0828, -0.0394,\n",
      "          -0.0322,  0.1787, -0.1440, -0.3065, -0.1382,  0.2489,  0.0558,\n",
      "           0.0788, -0.0403, -0.2505, -0.2261,  0.1382, -0.2198, -0.3124,\n",
      "          -0.0050,  0.0342]],\n",
      "\n",
      "        [[-0.2193, -0.3836,  0.1521, -0.0258,  0.2775,  0.1258,  0.3315,\n",
      "          -0.1528, -0.2228,  0.1602, -0.1297,  0.0414, -0.0862, -0.1483,\n",
      "          -0.1244,  0.0166,  0.2051,  0.2113,  0.3497,  0.1714,  0.0296,\n",
      "           0.3118,  0.1588,  0.0201, -0.1723, -0.4511, -0.1111, -0.3153,\n",
      "           0.1270, -0.0225, -0.0631, -0.0704, -0.0025,  0.1542, -0.1381,\n",
      "          -0.1803, -0.0238, -0.2079, -0.0199,  0.0534,  0.1454,  0.0485,\n",
      "          -0.0214, -0.3552, -0.1493, -0.0468,  0.1653, -0.0257, -0.1482,\n",
      "           0.1997,  0.2393,  0.0260, -0.0235,  0.1995,  0.3925, -0.3349,\n",
      "           0.1644, -0.0235, -0.2225,  0.1843,  0.4116, -0.3950, -0.0270,\n",
      "          -0.1711, -0.2295,  0.0203, -0.0630,  0.1667,  0.2295,  0.1839,\n",
      "          -0.4440,  0.2168,  0.1221, -0.3351, -0.0172, -0.2425,  0.1559,\n",
      "          -0.2071, -0.1115,  0.2375, -0.1794, -0.3427,  0.0951,  0.0654,\n",
      "           0.0667,  0.2033,  0.1356,  0.1710, -0.0305, -0.3296,  0.2450,\n",
      "          -0.0515, -0.6024,  0.0532, -0.1906, -0.0547,  0.0058,  0.2432,\n",
      "          -0.1569, -0.4322, -0.1215, -0.3148, -0.1991, -0.3331, -0.3731,\n",
      "           0.1112, -0.2739,  0.2348,  0.0177, -0.0834, -0.0829, -0.0393,\n",
      "          -0.0323,  0.1789, -0.1434, -0.3063, -0.1377,  0.2490,  0.0551,\n",
      "           0.0790, -0.0403, -0.2502, -0.2257,  0.1376, -0.2200, -0.3126,\n",
      "          -0.0052,  0.0337]],\n",
      "\n",
      "        [[-0.2169, -0.3866,  0.1539, -0.0280,  0.2769,  0.1264,  0.3308,\n",
      "          -0.1550, -0.2282,  0.1550, -0.1349,  0.0418, -0.0829, -0.1490,\n",
      "          -0.1240,  0.0178,  0.2067,  0.2094,  0.3505,  0.1708,  0.0283,\n",
      "           0.3102,  0.1603,  0.0179, -0.1695, -0.4477, -0.1077, -0.3137,\n",
      "           0.1286, -0.0274, -0.0634, -0.0734, -0.0012,  0.1536, -0.1428,\n",
      "          -0.1848, -0.0245, -0.2100, -0.0208,  0.0546,  0.1431,  0.0471,\n",
      "          -0.0209, -0.3567, -0.1507, -0.0468,  0.1650, -0.0254, -0.1468,\n",
      "           0.1989,  0.2369,  0.0279, -0.0257,  0.2001,  0.3937, -0.3274,\n",
      "           0.1613, -0.0207, -0.2224,  0.1798,  0.4130, -0.4003, -0.0281,\n",
      "          -0.1690, -0.2297,  0.0143, -0.0589,  0.1629,  0.2282,  0.1819,\n",
      "          -0.4445,  0.2190,  0.1245, -0.3353, -0.0129, -0.2408,  0.1530,\n",
      "          -0.2088, -0.1123,  0.2382, -0.1754, -0.3417,  0.0966,  0.0654,\n",
      "           0.0657,  0.2009,  0.1324,  0.1696, -0.0299, -0.3284,  0.2403,\n",
      "          -0.0542, -0.5989,  0.0537, -0.1944, -0.0528,  0.0051,  0.2413,\n",
      "          -0.1492, -0.4308, -0.1205, -0.3171, -0.2006, -0.3333, -0.3736,\n",
      "           0.1142, -0.2703,  0.2363,  0.0205, -0.0809, -0.0819, -0.0388,\n",
      "          -0.0355,  0.1814, -0.1475, -0.3086, -0.1381,  0.2510,  0.0606,\n",
      "           0.0792, -0.0437, -0.2520, -0.2254,  0.1393, -0.2200, -0.3125,\n",
      "          -0.0037,  0.0308]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = model.forward(src, src, src)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1, 5)\n",
      "(128, 128)\n",
      "in_proj_bias\n",
      "here!\n",
      "0q_0in_proj_weight 1\n",
      "(128,)\n",
      "in_proj_bias\n",
      "0q_1in_proj_bias 2\n",
      "(128, 128)\n",
      "in_proj_bias\n",
      "here!\n",
      "1k_0in_proj_weight 3\n",
      "(128,)\n",
      "in_proj_bias\n",
      "1k_1in_proj_bias 4\n",
      "(128, 128)\n",
      "in_proj_bias\n",
      "here!\n",
      "2v_0in_proj_weight 5\n",
      "(128,)\n",
      "in_proj_bias\n",
      "2v_1in_proj_bias 6\n",
      "(128, 128)\n",
      "out_proj.weight 7\n",
      "(128,)\n",
      "out_proj.bias 8\n",
      "(128, 1, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = '/private/home/padentomasello/scratch/pytorch_testing/multi_headed_attention.array'\n",
    "af.array.save_array('input', toArrayFire(src), filepath, False)\n",
    "i = 1\n",
    "params = {}\n",
    "for (name, param) in model.named_parameters():\n",
    "        if 'in_proj' in name:\n",
    "            q, k, v = param.chunk(3, dim=0)\n",
    "            hack = '0'\n",
    "            if 'in_proj_bias' in name: hack = '1'\n",
    "            params['0q_' + hack + name] = q\n",
    "            params['1k_' + hack + name] = k\n",
    "            params['2v_' + hack + name] = v\n",
    "            if 'in_proj_bias' in name:\n",
    "                for key in sorted(params.keys()):\n",
    "                    af_array = toArrayFire(params[key])\n",
    "                    print(name)\n",
    "                    if 'weight' in key:\n",
    "                        print('here!')\n",
    "                        af_array = af.array.transpose(af_array)\n",
    "                    print(key, i)\n",
    "                    af.array.save_array(key, af_array, filepath, True)  \n",
    "                    i = i + 1\n",
    "            continue\n",
    "        elif len(param.size()) > 0:\n",
    "            af_array = toArrayFire(param)\n",
    "            if 'fc' in name and 'weight' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            if 'weight' in name and 'proj' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            print(name, i)\n",
    "            af.array.save_array(name, af_array, filepath, True)\n",
    "            i = i + 1\n",
    "af.array.save_array('output', toArrayFire(output), filepath, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2186, -0.3842,  0.1497, -0.0263,  0.2757,  0.1261,  0.3305,\n",
       "          -0.1523, -0.2219,  0.1608, -0.1299,  0.0411, -0.0869, -0.1442,\n",
       "          -0.1239,  0.0169,  0.2050,  0.2134,  0.3478,  0.1704,  0.0294,\n",
       "           0.3097,  0.1604,  0.0194, -0.1723, -0.4511, -0.1123, -0.3128,\n",
       "           0.1281, -0.0255, -0.0620, -0.0702, -0.0028,  0.1525, -0.1405,\n",
       "          -0.1809, -0.0257, -0.2073, -0.0193,  0.0517,  0.1479,  0.0442,\n",
       "          -0.0223, -0.3561, -0.1487, -0.0483,  0.1664, -0.0244, -0.1497,\n",
       "           0.1999,  0.2401,  0.0254, -0.0240,  0.2015,  0.3916, -0.3342,\n",
       "           0.1651, -0.0227, -0.2236,  0.1835,  0.4124, -0.3942, -0.0274,\n",
       "          -0.1710, -0.2303,  0.0193, -0.0628,  0.1685,  0.2286,  0.1842,\n",
       "          -0.4431,  0.2175,  0.1231, -0.3336, -0.0163, -0.2438,  0.1562,\n",
       "          -0.2061, -0.1103,  0.2375, -0.1800, -0.3421,  0.0936,  0.0660,\n",
       "           0.0680,  0.2059,  0.1330,  0.1728, -0.0286, -0.3282,  0.2460,\n",
       "          -0.0512, -0.6025,  0.0551, -0.1901, -0.0535,  0.0051,  0.2428,\n",
       "          -0.1554, -0.4326, -0.1242, -0.3151, -0.1999, -0.3309, -0.3721,\n",
       "           0.1106, -0.2754,  0.2321,  0.0183, -0.0854, -0.0852, -0.0384,\n",
       "          -0.0311,  0.1796, -0.1434, -0.3068, -0.1373,  0.2470,  0.0550,\n",
       "           0.0781, -0.0398, -0.2515, -0.2239,  0.1369, -0.2193, -0.3131,\n",
       "          -0.0044,  0.0349]],\n",
       "\n",
       "        [[-0.2206, -0.3831,  0.1537, -0.0255,  0.2772,  0.1257,  0.3319,\n",
       "          -0.1527, -0.2217,  0.1606, -0.1294,  0.0423, -0.0874, -0.1498,\n",
       "          -0.1247,  0.0157,  0.2054,  0.2117,  0.3487,  0.1719,  0.0302,\n",
       "           0.3123,  0.1587,  0.0206, -0.1731, -0.4513, -0.1112, -0.3171,\n",
       "           0.1266, -0.0196, -0.0641, -0.0708, -0.0026,  0.1545, -0.1365,\n",
       "          -0.1792, -0.0243, -0.2079, -0.0196,  0.0540,  0.1442,  0.0509,\n",
       "          -0.0202, -0.3546, -0.1491, -0.0469,  0.1656, -0.0269, -0.1479,\n",
       "           0.1997,  0.2390,  0.0259, -0.0226,  0.1993,  0.3925, -0.3361,\n",
       "           0.1642, -0.0243, -0.2206,  0.1856,  0.4106, -0.3949, -0.0262,\n",
       "          -0.1711, -0.2285,  0.0226, -0.0631,  0.1668,  0.2300,  0.1831,\n",
       "          -0.4460,  0.2172,  0.1221, -0.3354, -0.0176, -0.2426,  0.1559,\n",
       "          -0.2075, -0.1109,  0.2368, -0.1795, -0.3435,  0.0962,  0.0647,\n",
       "           0.0666,  0.2033,  0.1379,  0.1702, -0.0321, -0.3303,  0.2456,\n",
       "          -0.0513, -0.6025,  0.0520, -0.1900, -0.0553,  0.0061,  0.2437,\n",
       "          -0.1586, -0.4326, -0.1213, -0.3145, -0.1976, -0.3340, -0.3740,\n",
       "           0.1113, -0.2750,  0.2343,  0.0177, -0.0833, -0.0822, -0.0402,\n",
       "          -0.0335,  0.1793, -0.1427, -0.3059, -0.1379,  0.2489,  0.0543,\n",
       "           0.0800, -0.0395, -0.2483, -0.2246,  0.1370, -0.2197, -0.3140,\n",
       "          -0.0057,  0.0334]],\n",
       "\n",
       "        [[-0.2191, -0.3837,  0.1516, -0.0255,  0.2774,  0.1262,  0.3315,\n",
       "          -0.1523, -0.2226,  0.1603, -0.1300,  0.0413, -0.0860, -0.1476,\n",
       "          -0.1246,  0.0167,  0.2056,  0.2118,  0.3496,  0.1715,  0.0293,\n",
       "           0.3120,  0.1589,  0.0204, -0.1721, -0.4514, -0.1111, -0.3143,\n",
       "           0.1272, -0.0231, -0.0633, -0.0704, -0.0025,  0.1540, -0.1384,\n",
       "          -0.1804, -0.0240, -0.2077, -0.0200,  0.0532,  0.1455,  0.0481,\n",
       "          -0.0216, -0.3558, -0.1496, -0.0468,  0.1657, -0.0253, -0.1484,\n",
       "           0.1997,  0.2395,  0.0257, -0.0236,  0.1992,  0.3925, -0.3351,\n",
       "           0.1644, -0.0232, -0.2232,  0.1841,  0.4119, -0.3950, -0.0267,\n",
       "          -0.1711, -0.2299,  0.0195, -0.0629,  0.1666,  0.2293,  0.1837,\n",
       "          -0.4434,  0.2168,  0.1226, -0.3353, -0.0173, -0.2426,  0.1555,\n",
       "          -0.2071, -0.1115,  0.2381, -0.1799, -0.3430,  0.0953,  0.0658,\n",
       "           0.0667,  0.2036,  0.1356,  0.1712, -0.0302, -0.3297,  0.2449,\n",
       "          -0.0510, -0.6025,  0.0536, -0.1904, -0.0556,  0.0061,  0.2434,\n",
       "          -0.1561, -0.4322, -0.1212, -0.3151, -0.1996, -0.3334, -0.3731,\n",
       "           0.1111, -0.2736,  0.2349,  0.0177, -0.0835, -0.0828, -0.0394,\n",
       "          -0.0322,  0.1787, -0.1440, -0.3065, -0.1382,  0.2489,  0.0558,\n",
       "           0.0788, -0.0403, -0.2505, -0.2261,  0.1382, -0.2198, -0.3124,\n",
       "          -0.0050,  0.0342]],\n",
       "\n",
       "        [[-0.2193, -0.3836,  0.1521, -0.0258,  0.2775,  0.1258,  0.3315,\n",
       "          -0.1528, -0.2228,  0.1602, -0.1297,  0.0414, -0.0862, -0.1483,\n",
       "          -0.1244,  0.0166,  0.2051,  0.2113,  0.3497,  0.1714,  0.0296,\n",
       "           0.3118,  0.1588,  0.0201, -0.1723, -0.4511, -0.1111, -0.3153,\n",
       "           0.1270, -0.0225, -0.0631, -0.0704, -0.0025,  0.1542, -0.1381,\n",
       "          -0.1803, -0.0238, -0.2079, -0.0199,  0.0534,  0.1454,  0.0485,\n",
       "          -0.0214, -0.3552, -0.1493, -0.0468,  0.1653, -0.0257, -0.1482,\n",
       "           0.1997,  0.2393,  0.0260, -0.0235,  0.1995,  0.3925, -0.3349,\n",
       "           0.1644, -0.0235, -0.2225,  0.1843,  0.4116, -0.3950, -0.0270,\n",
       "          -0.1711, -0.2295,  0.0203, -0.0630,  0.1667,  0.2295,  0.1839,\n",
       "          -0.4440,  0.2168,  0.1221, -0.3351, -0.0172, -0.2425,  0.1559,\n",
       "          -0.2071, -0.1115,  0.2375, -0.1794, -0.3427,  0.0951,  0.0654,\n",
       "           0.0667,  0.2033,  0.1356,  0.1710, -0.0305, -0.3296,  0.2450,\n",
       "          -0.0515, -0.6024,  0.0532, -0.1906, -0.0547,  0.0058,  0.2432,\n",
       "          -0.1569, -0.4322, -0.1215, -0.3148, -0.1991, -0.3331, -0.3731,\n",
       "           0.1112, -0.2739,  0.2348,  0.0177, -0.0834, -0.0829, -0.0393,\n",
       "          -0.0323,  0.1789, -0.1434, -0.3063, -0.1377,  0.2490,  0.0551,\n",
       "           0.0790, -0.0403, -0.2502, -0.2257,  0.1376, -0.2200, -0.3126,\n",
       "          -0.0052,  0.0337]],\n",
       "\n",
       "        [[-0.2169, -0.3866,  0.1539, -0.0280,  0.2769,  0.1264,  0.3308,\n",
       "          -0.1550, -0.2282,  0.1550, -0.1349,  0.0418, -0.0829, -0.1490,\n",
       "          -0.1240,  0.0178,  0.2067,  0.2094,  0.3505,  0.1708,  0.0283,\n",
       "           0.3102,  0.1603,  0.0179, -0.1695, -0.4477, -0.1077, -0.3137,\n",
       "           0.1286, -0.0274, -0.0634, -0.0734, -0.0012,  0.1536, -0.1428,\n",
       "          -0.1848, -0.0245, -0.2100, -0.0208,  0.0546,  0.1431,  0.0471,\n",
       "          -0.0209, -0.3567, -0.1507, -0.0468,  0.1650, -0.0254, -0.1468,\n",
       "           0.1989,  0.2369,  0.0279, -0.0257,  0.2001,  0.3937, -0.3274,\n",
       "           0.1613, -0.0207, -0.2224,  0.1798,  0.4130, -0.4003, -0.0281,\n",
       "          -0.1690, -0.2297,  0.0143, -0.0589,  0.1629,  0.2282,  0.1819,\n",
       "          -0.4445,  0.2190,  0.1245, -0.3353, -0.0129, -0.2408,  0.1530,\n",
       "          -0.2088, -0.1123,  0.2382, -0.1754, -0.3417,  0.0966,  0.0654,\n",
       "           0.0657,  0.2009,  0.1324,  0.1696, -0.0299, -0.3284,  0.2403,\n",
       "          -0.0542, -0.5989,  0.0537, -0.1944, -0.0528,  0.0051,  0.2413,\n",
       "          -0.1492, -0.4308, -0.1205, -0.3171, -0.2006, -0.3333, -0.3736,\n",
       "           0.1142, -0.2703,  0.2363,  0.0205, -0.0809, -0.0819, -0.0388,\n",
       "          -0.0355,  0.1814, -0.1475, -0.3086, -0.1381,  0.2510,  0.0606,\n",
       "           0.0792, -0.0437, -0.2520, -0.2254,  0.1393, -0.2200, -0.3125,\n",
       "          -0.0037,  0.0308]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
