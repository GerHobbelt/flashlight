{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arrayfire as af\n",
    "import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toArrayFire(x):\n",
    "    x_np = x.cpu().detach().contiguous().numpy()\n",
    "    shape = 1\n",
    "    if len(x_np.shape) == 0:\n",
    "        shape = (1,)\n",
    "    else:\n",
    "        shape = x_np.shape[::-1]\n",
    "    afArray = af.Array(x_np.ctypes.data, shape, x_np.dtype.char)\n",
    "    return afArray\n",
    "\n",
    "def saveStateDict(model, filepath):\n",
    "    params = {}\n",
    "    i = 0\n",
    "    for (name, param) in model.state_dict().items():\n",
    "        if 'running' in name:\n",
    "            continue\n",
    "        if 'in_proj' in name and 'in_proj.weight' != name and 'in_proj.bias' != name:\n",
    "            q, k, v = param.chunk(3, dim=0)\n",
    "            hack = '0'\n",
    "            if 'in_proj_bias' in name: hack = '1'\n",
    "            params['0q_' + hack + name] = q\n",
    "            params['1k_' + hack + name] = k\n",
    "            params['2v_' + hack + name] = v\n",
    "            if 'in_proj_bias' in name:\n",
    "                for key in sorted(params.keys()):\n",
    "                    af_array = toArrayFire(params[key])\n",
    "                    if 'weight' in key:\n",
    "                        af_array = af.array.transpose(af_array)\n",
    "                    #print(key, i, params[key].shape)\n",
    "                    af.array.save_array(key, af_array, filepath, True)\n",
    "                    #print(key, af.array.save_array(key, af_array, filepath, True))\n",
    "                    i = i + 1\n",
    "                params = {}\n",
    "            continue\n",
    "        elif len(param.size()) > 0:\n",
    "            if 'input_proj.bias' in name:\n",
    "                param = param.reshape((1, 1, 256))\n",
    "            af_array = toArrayFire(param)\n",
    "            if 'fc' in name and 'weight' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            elif 'weight' in name and 'proj' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            elif 'weight' in name and 'linear' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            elif 'query_embed' in name:\n",
    "                af_array = af_array\n",
    "            elif 'weight' in name and 'embed' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "\n",
    "            #print(name, i, param.shape)\n",
    "            af.array.save_array(name, af_array, filepath, True)\n",
    "            #print(name, af.array.save_array(name, af_array, filepath, True))\n",
    "            i = i + 1\n",
    "    for name in model.state_dict():\n",
    "        if 'running' in name:\n",
    "            #print(name)\n",
    "            af_array = toArrayFire(model.state_dict()[name])\n",
    "            #af.array.save_array(name, af_array, filepath + 'running', True)\n",
    "            #print(name, af.array.save_array(name, af_array, filepath + 'running', True))\n",
    "            #print(name, model.state_dict()[name].shape,)\n",
    "    \n",
    "def create_parser():\n",
    "    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n",
    "    parser.add_argument('--lr', default=1e-4, type=float)\n",
    "    parser.add_argument('--lr_backbone', default=1e-5, type=float)\n",
    "    parser.add_argument('--batch_size', default=2, type=int)\n",
    "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
    "    parser.add_argument('--epochs', default=300, type=int)\n",
    "    parser.add_argument('--lr_drop', default=200, type=int)\n",
    "    parser.add_argument('--optimizer', default=\"adam\", type=str)\n",
    "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
    "                        help='gradient clipping max norm')\n",
    "    parser.add_argument('--eval_skip', default=1, type=int,\n",
    "                        help='do evaluation every \"eval_skip\" frames')\n",
    "    parser.add_argument('--schedule', default='step', type=str,\n",
    "                        choices=('step', 'multistep'))\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--frozen_weights', type=str, default=None,\n",
    "                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n",
    "    # * Backbone\n",
    "    parser.add_argument('--backbone', default='resnet50', type=str,\n",
    "                        help=\"Name of the convolutional backbone to use\")\n",
    "    parser.add_argument('--dilation', action='store_true',\n",
    "                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
    "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n",
    "                        help=\"Type of positional embedding to use on top of the image features\")\n",
    "\n",
    "    # * Transformer\n",
    "    parser.add_argument('--enc_layers', default=6, type=int,\n",
    "                        help=\"Number of encoding layers in the transformer\")\n",
    "    parser.add_argument('--dec_layers', default=6, type=int,\n",
    "                        help=\"Number of decoding layers in the transformer\")\n",
    "    parser.add_argument('--dim_feedforward', default=2048, type=int,\n",
    "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
    "    parser.add_argument('--hidden_dim', default=256, type=int,\n",
    "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
    "    parser.add_argument('--dropout', default=0.1, type=float,\n",
    "                        help=\"Dropout applied in the transformer\")\n",
    "    parser.add_argument('--nheads', default=8, type=int,\n",
    "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
    "    parser.add_argument('--num_queries', default=100, type=int,\n",
    "                        help=\"Number of query slots\")\n",
    "    parser.add_argument('--pre_norm', action='store_true')\n",
    "    parser.add_argument('--no_pass_pos_and_query', dest='pass_pos_and_query', action='store_false',\n",
    "                        help=\"Disables passing the positional encodings to each attention layers\")\n",
    "\n",
    "    # * Segmentation\n",
    "    parser.add_argument('--mask_model', default='none', type=str, choices=(\"none\", \"smallconv\", \"v2\"),\n",
    "                        help=\"Segmentation head to be used (if None, segmentation will not be trained)\")\n",
    "\n",
    "    # Loss\n",
    "    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',\n",
    "                        help=\"Disables auxiliary decoding losses (loss at each layer)\")\n",
    "    parser.add_argument('--set_loss', default='hungarian', type=str,\n",
    "                        choices=('sequential', 'hungarian', 'lexicographical'),\n",
    "                        help=\"Type of matching to perform in the loss\")\n",
    "    parser.add_argument('--bcl', dest='use_bcl', action='store_true',\n",
    "                        help=\"Use balanced classification loss\")\n",
    "    # * Matcher\n",
    "    parser.add_argument('--set_cost_class', default=1, type=float,\n",
    "                        help=\"Class coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_bbox', default=5, type=float,\n",
    "                        help=\"L1 box coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_giou', default=2, type=float,\n",
    "                        help=\"giou box coefficient in the matching cost\")\n",
    "    # * Loss coefficients\n",
    "    parser.add_argument('--mask_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--dice_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--bbox_loss_coef', default=5, type=float)\n",
    "    parser.add_argument('--giou_loss_coef', default=2, type=float)\n",
    "    parser.add_argument('--eos_coef', default=0.1, type=float,\n",
    "                        help=\"Relative classification weight of the no-object class\")\n",
    "\n",
    "    # dataset parameters\n",
    "    parser.add_argument('--dataset_file', default='coco')\n",
    "    parser.add_argument('--coco_path', type=str, default='/datasets01/COCO/022719')\n",
    "    parser.add_argument('--coco_panoptic_path', type=str, default='/datasets01/COCO/060419')\n",
    "    parser.add_argument('--remove_difficult', action='store_true')\n",
    "    parser.add_argument('--masks', action='store_true')\n",
    "\n",
    "    parser.add_argument('--output-dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=42, type=int)\n",
    "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "    parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--num_workers', default=2, type=int)\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world-size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--dist-url', default='env://', help='url used to set up distributed training')\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=18.69s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import models.detr \n",
    "import datasets.coco\n",
    "parser = create_parser()\n",
    "pretrained_path = '/private/home/padentomasello/scratch/pytorch_testing/detr-r50-e632da11.pth'\n",
    "args = parser.parse_args([\"--eos_coef=0.5\", \"--resume=/private/home/padentomasello/scratch/pytorch_testing/detr-r50-e632da11.pth\"])\n",
    "args = parser.parse_args([])\n",
    "model, criterion, post = models.detr.build(args)   \n",
    "dataset = datasets.coco.build('train', args)\n",
    "if args.resume:\n",
    "    if args.resume.startswith('https'):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            args.resume, map_location='cpu', check_hash=True)\n",
    "    else:\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from torch.utils.data import DataLoader, DistributedSampler\n",
    "    import util.misc as utils\n",
    "    dataset_train = dataset\n",
    "    sampler_train = torch.utils.data.SequentialSampler(dataset)\n",
    "    #batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "        #    sampler_train, args.batch_size, drop_last=True)\n",
    "    batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "            sampler_train, 1, drop_last=True)\n",
    "    data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                                collate_fn=utils.collate_fn, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(args.device)\n",
    "for (sample_tmp, target_tmp) in data_loader_train:\n",
    "    samples = sample_tmp.to(device)\n",
    "    targets  = [{k: v.to(device) for k, v in t.items()} for t in target_tmp]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "429"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.transformer import *\n",
    "\n",
    "\n",
    "from models.backbone import *\n",
    "filepath = '/private/home/padentomasello/scratch/pytorch_testing/detr.array'\n",
    "\n",
    "N = 2\n",
    "C = 3\n",
    "H = 224\n",
    "W = 224\n",
    "\n",
    "\n",
    "image = samples.tensors\n",
    "image.requires_grad = True\n",
    "mask = samples.mask\n",
    "\n",
    "af.array.save_array('image', toArrayFire(image), filepath, False)\n",
    "af.array.save_array('mask', toArrayFire(mask.float()), filepath, True)\n",
    "target_boxes = targets[0]['boxes']\n",
    "target_classes = targets[0]['labels']\n",
    "af.array.save_array('target_boxes', toArrayFire(target_boxes), filepath, True)\n",
    "af.array.save_array('target_labels', toArrayFire(target_classes.float()), filepath, True)\n",
    "#af.array.save_array('pos', toArrayFire(pos), filepath, True)\n",
    "       \n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "output = model(samples)\n",
    "saveStateDict(model, filepath)\n",
    "af.array.save_array('pred_logits', toArrayFire(output['pred_logits']), filepath, True)\n",
    "af.array.save_array('pred_boxes', toArrayFire(output['pred_boxes']), filepath, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6541,  0.5225,  1.5646,  ..., -0.3926, -0.2307, -0.3364],\n",
       "         [-0.6425,  0.5298,  1.5569,  ..., -0.3923, -0.2162, -0.3354],\n",
       "         [-0.6480,  0.5251,  1.5511,  ..., -0.3904, -0.2246, -0.3296],\n",
       "         ...,\n",
       "         [-0.6490,  0.5282,  1.5628,  ..., -0.3981, -0.2268, -0.3437],\n",
       "         [-0.6439,  0.5284,  1.5476,  ..., -0.3958, -0.2203, -0.3420],\n",
       "         [-0.6496,  0.5246,  1.5533,  ..., -0.3917, -0.2156, -0.3437]]],\n",
       "       device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"pred_logits\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "criteiron(output, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([15, 27, 37, 45, 55, 65, 75, 76]), tensor([6, 1, 5, 7, 2, 0, 3, 4]))]\n",
      "here\n",
      "tensor([[91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 55, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 51, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 55, 91, 91, 91, 91, 91, 91, 91, 55, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 56, 91, 91, 91, 91, 91, 91, 91, 91, 91, 51, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 51, 55, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91]], device='cuda:1')\n",
      "tensor([[[-0.6541,  0.5225,  1.5646,  ..., -0.3926, -0.2307, -0.3364],\n",
      "         [-0.6425,  0.5298,  1.5569,  ..., -0.3923, -0.2162, -0.3354],\n",
      "         [-0.6480,  0.5251,  1.5511,  ..., -0.3904, -0.2246, -0.3296],\n",
      "         ...,\n",
      "         [-0.6490,  0.5282,  1.5628,  ..., -0.3981, -0.2268, -0.3437],\n",
      "         [-0.6439,  0.5284,  1.5476,  ..., -0.3958, -0.2203, -0.3420],\n",
      "         [-0.6496,  0.5246,  1.5533,  ..., -0.3917, -0.2156, -0.3437]]],\n",
      "       device='cuda:1', grad_fn=<SelectBackward>)\n",
      "> \u001b[0;32m/private/home/padentomasello/code/detection-transformer/models/detr.py\u001b[0m(116)\u001b[0;36mloss_labels\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    114 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    115 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 116 \u001b[0;31m        \u001b[0mloss_ce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    117 \u001b[0;31m        \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss_ce'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_ce\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    118 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n",
      "{'loss_ce': tensor(4.7498, device='cuda:1', grad_fn=<NllLoss2DBackward>), 'class_error': tensor(100., device='cuda:1'), 'loss_bbox': tensor(0.9223, device='cuda:1', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0065, device='cuda:1', grad_fn=<DivBackward0>), 'cardinality_error': tensor(92., device='cuda:1')}\n",
      "0\n",
      "tensor([[91, 91, 91, 91, 91, 91, 56, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 55, 55, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 51, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 51, 55, 91, 91, 91, 91, 91, 91, 91, 91, 55, 91, 91, 91, 91, 91,\n",
      "         91, 91, 51, 91, 91, 91, 91, 91, 91, 91]], device='cuda:1')\n",
      "tensor([[[ 1.1975,  0.1097, -0.0833,  ...,  0.0198,  0.3334, -0.4715],\n",
      "         [ 1.1986,  0.1060, -0.0876,  ...,  0.0272,  0.3446, -0.4621],\n",
      "         [ 1.1993,  0.1078, -0.0867,  ...,  0.0147,  0.3354, -0.4795],\n",
      "         ...,\n",
      "         [ 1.2050,  0.0973, -0.0846,  ...,  0.0107,  0.3436, -0.4684],\n",
      "         [ 1.2103,  0.1073, -0.0787,  ...,  0.0204,  0.3367, -0.4804],\n",
      "         [ 1.1970,  0.1040, -0.0765,  ...,  0.0142,  0.3387, -0.4674]]],\n",
      "       device='cuda:1', grad_fn=<UnbindBackward>)\n",
      "> \u001b[0;32m/private/home/padentomasello/code/detection-transformer/models/detr.py\u001b[0m(116)\u001b[0;36mloss_labels\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    114 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    115 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 116 \u001b[0;31m        \u001b[0mloss_ce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    117 \u001b[0;31m        \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss_ce'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_ce\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    118 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n",
      "1\n",
      "tensor([[91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 56, 91, 91, 91, 91, 91, 91, 91,\n",
      "         51, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 55, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 55, 91, 91, 91, 91, 91, 51, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 55, 91, 91,\n",
      "         91, 91, 51, 91, 91, 91, 91, 91, 91, 91, 55, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91]], device='cuda:1')\n",
      "tensor([[[ 1.2732,  0.2040, -0.5077,  ...,  0.5756,  1.2888,  0.0987],\n",
      "         [ 1.2756,  0.2025, -0.5155,  ...,  0.5690,  1.2946,  0.1053],\n",
      "         [ 1.2733,  0.1955, -0.5144,  ...,  0.5718,  1.2786,  0.0948],\n",
      "         ...,\n",
      "         [ 1.2805,  0.1953, -0.5120,  ...,  0.5694,  1.2860,  0.1022],\n",
      "         [ 1.2789,  0.1966, -0.5107,  ...,  0.5697,  1.2836,  0.0993],\n",
      "         [ 1.2776,  0.1866, -0.5097,  ...,  0.5643,  1.2900,  0.1010]]],\n",
      "       device='cuda:1', grad_fn=<UnbindBackward>)\n",
      "> \u001b[0;32m/private/home/padentomasello/code/detection-transformer/models/detr.py\u001b[0m(116)\u001b[0;36mloss_labels\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    114 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    115 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 116 \u001b[0;31m        \u001b[0mloss_ce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    117 \u001b[0;31m        \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss_ce'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_ce\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    118 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n",
      "2\n",
      "tensor([[91, 91, 91, 91, 91, 91, 91, 55, 91, 91, 51, 91, 91, 91, 91, 55, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 56, 55, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 55, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 51, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 51, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91]], device='cuda:1')\n",
      "tensor([[[0.5611, 0.0915, 0.7085,  ..., 0.6820, 0.7858, 0.3656],\n",
      "         [0.5713, 0.0942, 0.7020,  ..., 0.6761, 0.7935, 0.3683],\n",
      "         [0.5686, 0.0887, 0.7069,  ..., 0.6774, 0.7831, 0.3621],\n",
      "         ...,\n",
      "         [0.5738, 0.0896, 0.7045,  ..., 0.6845, 0.7926, 0.3678],\n",
      "         [0.5740, 0.0924, 0.7018,  ..., 0.6726, 0.7851, 0.3644],\n",
      "         [0.5705, 0.0876, 0.7038,  ..., 0.6677, 0.7877, 0.3681]]],\n",
      "       device='cuda:1', grad_fn=<UnbindBackward>)\n",
      "> \u001b[0;32m/private/home/padentomasello/code/detection-transformer/models/detr.py\u001b[0m(116)\u001b[0;36mloss_labels\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    114 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    115 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 116 \u001b[0;31m        \u001b[0mloss_ce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    117 \u001b[0;31m        \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss_ce'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_ce\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    118 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n",
      "3\n",
      "tensor([[91, 91, 51, 91, 91, 91, 91, 55, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 56, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 51, 91, 91, 91, 55, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 55, 91, 51, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 55, 91, 91, 91, 91, 91]], device='cuda:1')\n",
      "tensor([[[ 0.1564,  0.1740,  0.1691,  ...,  0.1163,  0.4823, -0.0091],\n",
      "         [ 0.1693,  0.1772,  0.1576,  ...,  0.1164,  0.4897, -0.0025],\n",
      "         [ 0.1577,  0.1755,  0.1647,  ...,  0.1100,  0.4790, -0.0109],\n",
      "         ...,\n",
      "         [ 0.1615,  0.1714,  0.1636,  ...,  0.1185,  0.4868, -0.0007],\n",
      "         [ 0.1636,  0.1779,  0.1671,  ...,  0.1145,  0.4740, -0.0076],\n",
      "         [ 0.1618,  0.1745,  0.1663,  ...,  0.1071,  0.4828, -0.0035]]],\n",
      "       device='cuda:1', grad_fn=<UnbindBackward>)\n",
      "> \u001b[0;32m/private/home/padentomasello/code/detection-transformer/models/detr.py\u001b[0m(116)\u001b[0;36mloss_labels\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    114 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    115 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 116 \u001b[0;31m        \u001b[0mloss_ce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    117 \u001b[0;31m        \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss_ce'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_ce\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    118 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> c\n",
      "4\n",
      "tensor([[55, 91, 91, 91, 91, 91, 91, 55, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 55, 91, 56, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 51, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 51, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 55, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 51, 91, 91, 91, 91]], device='cuda:1')\n",
      "tensor([[[0.3379, 0.7814, 0.6763,  ..., 0.4683, 0.2881, 0.3449],\n",
      "         [0.3456, 0.7802, 0.6691,  ..., 0.4633, 0.2943, 0.3490],\n",
      "         [0.3391, 0.7780, 0.6729,  ..., 0.4664, 0.2857, 0.3448],\n",
      "         ...,\n",
      "         [0.3405, 0.7796, 0.6777,  ..., 0.4635, 0.2900, 0.3434],\n",
      "         [0.3433, 0.7862, 0.6722,  ..., 0.4595, 0.2843, 0.3450],\n",
      "         [0.3409, 0.7739, 0.6772,  ..., 0.4584, 0.2922, 0.3459]]],\n",
      "       device='cuda:1', grad_fn=<UnbindBackward>)\n",
      "> \u001b[0;32m/private/home/padentomasello/code/detection-transformer/models/detr.py\u001b[0m(116)\u001b[0;36mloss_labels\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    114 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    115 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 116 \u001b[0;31m        \u001b[0mloss_ce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    117 \u001b[0;31m        \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss_ce'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_ce\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    118 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n"
     ]
    }
   ],
   "source": [
    "losses = criterion(output, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses['loss_giou'].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6541,  0.5225,  1.5646,  ..., -0.3926, -0.2307, -0.3364],\n",
       "         [-0.6425,  0.5298,  1.5569,  ..., -0.3923, -0.2162, -0.3354],\n",
       "         [-0.6480,  0.5251,  1.5511,  ..., -0.3904, -0.2246, -0.3296],\n",
       "         ...,\n",
       "         [-0.6490,  0.5282,  1.5628,  ..., -0.3981, -0.2268, -0.3437],\n",
       "         [-0.6439,  0.5284,  1.5476,  ..., -0.3958, -0.2203, -0.3420],\n",
       "         [-0.6496,  0.5246,  1.5533,  ..., -0.3917, -0.2156, -0.3437]]],\n",
       "       device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['pred_logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'backbone' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-6b5ba277cffc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbackbone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'backbone' is not defined"
     ]
    }
   ],
   "source": [
    "backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
